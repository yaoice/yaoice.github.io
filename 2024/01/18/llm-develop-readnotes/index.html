<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <meta property="og:site_name" content="爱折腾的工程师">
    <meta property="og:type" content="article">

    
    <meta property="og:image" content="https://www.iceyao.com.cn/img/home-bg.jpeg">
    <meta property="twitter:image" content="https://www.iceyao.com.cn/img/home-bg.jpeg" />
    

    
    <meta name="title" content="LLM学习笔记" />
    <meta property="og:title" content="LLM学习笔记" />
    <meta property="twitter:title" content="LLM学习笔记" />
    

    
    <meta name="description" content="LLM学习笔记">
    <meta property="og:description" content="LLM学习笔记" />
    <meta property="twitter:description" content="LLM学习笔记" />
    

    
    <meta property="twitter:card" content="summary" />
    
    

    <meta name="keyword"  content="iceyao, IceYao&#39;s Blog, 博客, 个人网站, 互联网, Web, 云原生, PaaS, Istio, Kubernetes, 微服务, Microservice">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>LLM学习笔记 | 爱折腾的工程师 | IceYao&#39;s Blog</title>

    <link rel="canonical" href="/2024/01/18/llm-develop-readnotes/">

    
    
    
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    
    <link rel="stylesheet" href="/css/hugo-theme-cleanwhite.min.css">

    
    <link rel="stylesheet" href="/css/zanshang.css">

    
    <link rel="stylesheet" href="/css/font-awesome.all.min.css">

    
    

    
    <script src="/js/jquery.min.js"></script>

    
    <script src="/js/bootstrap.min.js"></script>

    
    <script src="/js/hux-blog.min.js"></script>

    
    <script src="/js/lazysizes.min.js"></script>

    
    

</head>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-9J7CKFVPPM"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-9J7CKFVPPM', { 'anonymize_ip': false });
}
</script>






<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
        
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">爱折腾的工程师</a>
        </div>

        
        
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">All Posts</a>
                    </li>
                    
                        
                    
                    
		    
                        <li><a href="/archive//">ARCHIVE</a></li>
                    
                        <li><a href="/notes//">NOTES</a></li>
                    
                        <li><a href="/about//">ABOUT</a></li>
                    
		            <li>
                        <a href="/search"><i class="fa fa-search"></i></a>
		           </li>
                </ul>
            </div>
        </div>
        
    </div>
    
</nav>
<script>
    
    
    
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        
            $navbar.className = " ";
            
            setTimeout(function(){
                
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>




<style type="text/css">
    header.intro-header {
        background-image: url('/img/home-bg.jpeg')
    }
</style>

<header class="intro-header" >

    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/llm" title="LLM">
                            LLM
                        </a>
                        
                    </div>
                    <h1>LLM学习笔记</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        
                            Posted by 
                            
                                iceyao
                             
                            on 
                            Thursday, January 18, 2024
                            
                            
                            
                            
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>




<article>
    <div class="container">
        <div class="row">

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                
                <h1 id="背景">背景</h1>
<p>实践内容源于吴恩达老师与openAI合作推出的大语言模型教程，本教程基于<a href="https://datawhalechina.github.io/prompt-engineering-for-developers/#/README">面向开发者的LLM入门教程</a>的golang版本实践(前两个章节)。</p>
<h1 id="环境配置">环境配置</h1>
<p>1.申请OpenAI API Key</p>
<p>在OpenAI官网注册的账号默认有18美金的额度用于消耗token，3个月内有效。国内也有免费的内测Key供申请
<code>https://github.com/chatanywhere/GPT_API_free</code>，需要绑定Github账号来申请。</p>
<p>2.Golang tool库封装</p>
<p>封装一个CreateChatCompletion函数，放在同个package下引用</p>
<pre tabindex="0"><code>package main

import (
	&quot;context&quot;
	// 官方推荐的第三方OpenAl golang sdk
	&quot;github.com/sashabaranov/go-openai&quot;
)

const (
	Token          = &quot;&lt;填入你的token&gt;&quot;
	// 使用国内免费申请的Key的话，就用这个host用于转发请求
	OpenAIProxyURL = &quot;https://api.chatanywhere.com.cn/v1&quot;
)

type openAIClient struct {
	*openai.Client
}

func newOpenAIClient() *openAIClient {
	config := openai.DefaultConfig(Token)
	config.BaseURL = OpenAIProxyURL
	c := openai.NewClientWithConfig(config)
	return &amp;openAIClient{c}
}

func (c *openAIClient) CreateChatCompletion(ctx context.Context, content string) (openai.ChatCompletionResponse, error) {
	return c.Client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
		Model: openai.GPT3Dot5Turbo,
		Messages: []openai.ChatCompletionMessage{
			{
				Role:    openai.ChatMessageRoleUser,
				Content: content,
			},
		},
		Temperature: 0,
	})
}
</code></pre><h1 id="面向开发者的提示工程">面向开发者的提示工程</h1>
<p>随着ChatGPT为代表的大语言大模型大爆发出现之后，Prompt已经成为与大模型输入的代称。一般将大模型的输入称为<code>Prompt</code>，将大模型的返回输出称为<code>Completion</code>。
合理的Prompt设计决定了大模型能力的上限和下限，学会充分、高效使用LLM，Prompt Engineering的技能强烈需要。<code>Prompt Engineering</code>是针对特定任务构造充分
发挥大模型能力的Prompt的技巧。</p>
<h2 id="简介introduction">简介Introduction</h2>
<p>随着LLM的发展，大致分为两种类型：</p>
<ul>
<li>基础LLM：基于文本训练数据，训练出预测下一个单词能力的模型。通常通过在互联网和其他来源的大量数据上训练，来确定紧接着出现的最可能的词。</li>
<li>指令微调LLM：通过专门的训练，可以更好地理解并遵循指令。指令微调LLM的训练通常基于预训练语言模型，先在大规模文本数据上进行预训练，掌握语言的基本规律。
在此基础上进行进一步的训练与微调（finetune），输入是指令，输出是对这些指令的正确回复。还可以通过RLHF（人类反馈强化学习）技术，增强模型的能力。</li>
</ul>
<p>这里的实践重点介绍针对指令微调LLM的最佳实践。</p>
<h2 id="提示原则guidelines">提示原则Guidelines</h2>
<p>高效Prompt的两个关键原则：</p>
<ul>
<li>编写清晰、具体的指令；清晰明确地表达需求，提供足够的上下文，让大语言模型准确理解需求。</li>
<li>给予模型充足思考时间；加入逐步推理的要求，给模型留充分思考时间，生成的结果更准确可靠。</li>
</ul>
<p>掌握这两个关键原则，是大语言模型成功的重要基石。</p>
<h3 id="编写清晰具体的指令">编写清晰具体的指令</h3>
<h4 id="使用分隔符">使用分隔符</h4>
<p>分隔符可以将不同的指令、上下文、输入隔开，防止<code>提示词注入</code>，输入的文本可能包含与预设的Prompt冲突的内容，如果不加分隔，这些输入可能扰乱模型的输出。</p>
<p>示例：给出一段话，让GPT进行总结，使用&quot;&ldquo;作为分隔符</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">您应该提供尽可能清晰、具体的指示，以表达您希望模型执行的任务。\
</span><span style="color:#f1fa8c">这将引导模型朝向所需的输出，并降低收到无关或不正确响应的可能性。\
</span><span style="color:#f1fa8c">不要将写清晰的提示词与写简短的提示词混淆。\
</span><span style="color:#f1fa8c">在许多情况下，更长的提示词可以为模型提供更多的清晰度和上下文信息，从而导致更详细和相关的输出。
</span><span style="color:#f1fa8c">    `</span>
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">	把用两个双引号括起来的文本总结成一句话。
</span><span style="color:#f1fa8c">	&#34;%s&#34;
</span><span style="color:#f1fa8c">	`</span>
	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>提供清晰、具体的指示可以引导模型朝向所需的输出，避免无关或不正确的响应。长度更长的提示词可以提供更多的清晰度和上下文信息，导致更详细和相关的输出。
</code></pre><h4 id="结构化输出">结构化输出</h4>
<p>结构化输出，指的是类似json、html等结构。</p>
<p>示例：让GPT生成三本书的标题、作者和类别，并以JSON格式返回，其中JSON的key已指定</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">	请生成包括书名、作者和类别的三本虚构的、非真实存在的中文书籍清单，\
</span><span style="color:#f1fa8c">并以JSON格式提供，其中包含以下键:book_id、title、author、genre。
</span><span style="color:#f1fa8c">	`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), prompt)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>[
  {
    &quot;book_id&quot;: 1,
    &quot;title&quot;: &quot;夜色旅人&quot;,
    &quot;author&quot;: &quot;张三&quot;,
    &quot;genre&quot;: &quot;奇幻&quot;
  },
  {
    &quot;book_id&quot;: 2,
    &quot;title&quot;: &quot;梦境之城&quot;,
    &quot;author&quot;: &quot;李四&quot;,
    &quot;genre&quot;: &quot;科幻&quot;
  },
  {
    &quot;book_id&quot;: 3,
    &quot;title&quot;: &quot;幻想之门&quot;,
    &quot;author&quot;: &quot;王五&quot;,
    &quot;genre&quot;: &quot;魔幻&quot;
  }
]
</code></pre><h4 id="模型检查">模型检查</h4>
<p>如果任务包含不一定能满足的条件，可以让模型检查这些条件，如果不满足，可以让其停止执行后续的流程。可以加入一些边界情况的考虑，以避免意外的结果或错误发生。</p>
<p>示例：分别给模型两段文本，一是制作茶的步骤，二是一段没有明确步骤的文本。要求模型判断是否其包含一系列指令，包含则按照给定格式重新编写指令，不包含则返回&quot;未提供步骤&rdquo;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#6272a4">// 制作茶的步骤
</span><span style="color:#6272a4"></span><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text1 <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">	泡一杯茶很容易。首先，需要把水烧开。\
</span><span style="color:#f1fa8c">在等待期间，拿一个杯子并把茶包放进去。\
</span><span style="color:#f1fa8c">一旦水足够热，就把它倒在茶包上。\
</span><span style="color:#f1fa8c">等待一会儿，让茶叶浸泡。几分钟后，取出茶包。\
</span><span style="color:#f1fa8c">如果您愿意，可以加一些糖或牛奶调味。\
</span><span style="color:#f1fa8c">就这样，您可以享受一杯美味的茶了。
</span><span style="color:#f1fa8c">`</span>
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">您将获得由两个双引号括起来的文本。\
</span><span style="color:#f1fa8c">如果它包含一系列的指令，则需要按照以下格式重新编写这些指令：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">第一步 - ...
</span><span style="color:#f1fa8c">第二步 - …
</span><span style="color:#f1fa8c">…
</span><span style="color:#f1fa8c">第N步 - …
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">如果文本中不包含一系列的指令，则直接写“未提供步骤”。
</span><span style="color:#f1fa8c">&#34;%s&#34;
</span><span style="color:#f1fa8c">	`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text1))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>第一步 - 把水烧开。
第二步 - 拿一个杯子并把茶包放进去。
第三步 - 把热水倒在茶包上。
第四步 - 等待几分钟让茶叶浸泡，然后取出茶包。
第五步 - 可以选择加入糖或牛奶调味。
第六步 - 就这样，您可以享受一杯美味的茶了。
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#6272a4">// 一段没有明确步骤的文本
</span><span style="color:#6272a4"></span><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text2 <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">	泡一杯茶很容易。首先，需要把水烧开。\
</span><span style="color:#f1fa8c">在等待期间，拿一个杯子并把茶包放进去。\
</span><span style="color:#f1fa8c">一旦水足够热，就把它倒在茶包上。\
</span><span style="color:#f1fa8c">等待一会儿，让茶叶浸泡。几分钟后，取出茶包。\
</span><span style="color:#f1fa8c">如果您愿意，可以加一些糖或牛奶调味。\
</span><span style="color:#f1fa8c">就这样，您可以享受一杯美味的茶了。
</span><span style="color:#f1fa8c">`</span>
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">您将获得由两个双引号括起来的文本。\
</span><span style="color:#f1fa8c">如果它包含一系列的指令，则需要按照以下格式重新编写这些指令：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">第一步 - ...
</span><span style="color:#f1fa8c">第二步 - …
</span><span style="color:#f1fa8c">…
</span><span style="color:#f1fa8c">第N步 - …
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">如果文本中不包含一系列的指令，则直接写“未提供步骤”。
</span><span style="color:#f1fa8c">&#34;%s&#34;
</span><span style="color:#f1fa8c">	`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text2))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>未提供步骤
</code></pre><h4 id="提供少量示例">提供少量示例</h4>
<p>Few-shot prompting，在模型执行具体任务之前，给模型1～2个样例，让模型了解要求和期望输出的样式</p>
<p>示例：利用少量样例，可以预热语言模型，这是一个让模型快速上手新任务的有效策略。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">您的任务是以一致的风格回答问题。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;孩子&gt;: 请教我何为耐心。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;祖父母&gt;: 挖出最深峡谷的河流源于一处不起眼的泉眼；最宏伟的交响乐从单一的音符开始；最复杂的挂毯以一根孤独的线开始编织。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;孩子&gt;: 请教我何为韧性。
</span><span style="color:#f1fa8c">	`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), prompt)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>&lt;祖父母&gt;: 韧性就像一根弹簧，能够在压力和挑战面前保持弹性和坚韧。它是一种适应和持久的能力，能够在困的本质。
</code></pre><h3 id="给模型时间思考">给模型时间思考</h3>
<p>给模型充足的时间思考，可以提高模型的准确性，好比让一个人在短时间内去解决一个非常棘手的问题，难度可见之大。</p>
<h4 id="指定完成任务的步骤">指定完成任务的步骤</h4>
<p>示例：描述了杰克和吉尔的故事，并给出提示词执行以下操作：首先，用一句话概括三个反引号限定的文本。第二，将摘要翻译成英语。第三，在英语摘要中列出每个名称。
第四，输出包含以下键的JSON对象：英语摘要和人名个数。要求输出以换行符分隔</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">在一个迷人的村庄里，兄妹杰克和吉尔出发去一个山顶井里打水。\
</span><span style="color:#f1fa8c">他们一边唱着欢乐的歌，一边往上爬，\
</span><span style="color:#f1fa8c">然而不幸降临——杰克绊了一块石头，从山上滚了下来，吉尔紧随其后。\
</span><span style="color:#f1fa8c">虽然略有些摔伤，但他们还是回到了温馨的家中。\
</span><span style="color:#f1fa8c">尽管出了这样的意外，他们的冒险精神依然没有减弱，继续充满愉悦地探索。
</span><span style="color:#f1fa8c">`</span>
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">执行以下操作：
</span><span style="color:#f1fa8c">1-用一句话概括下面用两个双反引号括起来的文本。
</span><span style="color:#f1fa8c">2-将摘要翻译成英语。
</span><span style="color:#f1fa8c">3-在英语摘要中列出每个人名。
</span><span style="color:#f1fa8c">4-输出一个 JSON 对象，其中包含以下键：english_summary，num_names。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">请用换行符分隔您的答案。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&#34;%s&#34;
</span><span style="color:#f1fa8c">	`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>1- 兄妹在迷人的村庄冒险后受伤但仍然乐观。
2- Siblings Jack and Jill set off to fetch water from a well on a mountaintop in a charming village.
3- Jack, Jill
4- {
   &quot;english_summary&quot;: &quot;Siblings Jack and Jill set off to fetch water from a well on a mountaintop in a charming village. Despite getting injured in an unfortunate accident, they still maintained their adventurous spirit and continued to explore joyfully.&quot;,
   &quot;num_names&quot;: 2
}
</code></pre><p>可以将prompt改进，确切指定输出格式</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">在一个迷人的村庄里，兄妹杰克和吉尔出发去一个山顶井里打水。\
</span><span style="color:#f1fa8c">他们一边唱着欢乐的歌，一边往上爬，\
</span><span style="color:#f1fa8c">然而不幸降临——杰克绊了一块石头，从山上滚了下来，吉尔紧随其后。\
</span><span style="color:#f1fa8c">虽然略有些摔伤，但他们还是回到了温馨的家中。\
</span><span style="color:#f1fa8c">尽管出了这样的意外，他们的冒险精神依然没有减弱，继续充满愉悦地探索。
</span><span style="color:#f1fa8c">`</span>
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">1-用一句话概括下面用双引号括起来的文本。
</span><span style="color:#f1fa8c">2-将摘要翻译成英语。
</span><span style="color:#f1fa8c">3-在英语摘要中列出每个名称。
</span><span style="color:#f1fa8c">4-输出一个 JSON 对象，其中包含以下键：English_summary，num_names。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">请使用以下格式：
</span><span style="color:#f1fa8c">文本：&lt;要总结的文本&gt;
</span><span style="color:#f1fa8c">摘要：&lt;摘要&gt;
</span><span style="color:#f1fa8c">翻译：&lt;摘要的翻译&gt;
</span><span style="color:#f1fa8c">名称：&lt;英语摘要中的名称列表&gt;
</span><span style="color:#f1fa8c">输出 JSON：&lt;带有 English_summary 和 num_names 的 JSON&gt;
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>摘要：故事讲述了杰克和吉尔在山顶井打水时发生意外，但他们的冒险精神依然没有减弱。

翻译：Summary: The story tells about an accident that happened to Jack and Jill while they were geng water from a well on the mountaintop, but their adventurous spirit remained undiminished.

名称：Jack, Jill

输出 JSON：{&quot;English_summary&quot;: &quot;The story tells about an accident that happened to Jack and Jill we they were getting water from a well on the mountaintop, but their adventurous spirit remained undiminished.&quot;, &quot;num_names&quot;: 2}
</code></pre><h4 id="指导模型自主思考">指导模型自主思考</h4>
<p>设计prompt时，可以通过明确指导语言模型进行自主思考。在prompt中要求语言模型先自己尝试解决问题，思考对应的解决方法，再与提供的解答进行对比，校验正确性。</p>
<p>示例：给出一个问题和一份来自学生的解答，要求模型判断解答是否正确</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">判断学生的解决方案是否正确。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">问题:
</span><span style="color:#f1fa8c">我正在建造一个太阳能发电站，需要帮助计算财务。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    土地费用为 100美元/平方英尺
</span><span style="color:#f1fa8c">    我可以以 250美元/平方英尺的价格购买太阳能电池板
</span><span style="color:#f1fa8c">    我已经谈判好了维护合同，每年需要支付固定的10万美元，并额外支付每平方英尺10美元
</span><span style="color:#f1fa8c">    作为平方英尺数的函数，首年运营的总费用是多少。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">学生的解决方案：
</span><span style="color:#f1fa8c">设x为发电站的大小，单位为平方英尺。
</span><span style="color:#f1fa8c">费用：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    土地费用：100x
</span><span style="color:#f1fa8c">    太阳能电池板费用：250x
</span><span style="color:#f1fa8c">    维护费用：100,000美元+100x
</span><span style="color:#f1fa8c">    总费用：100x+250x+100,000美元+100x=450x+100,000美元
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), prompt)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>学生的解决方案是正确的。总费用可以表示为450x+100,000美元。
</code></pre><p>实际上这个解决方案是错误的，总费用应该是360x+100,000美元，因为没让模型自行先计算，然后跟学生的解决方法对比，那么就有可能误导模型以为学生的解法就是正确的。
接下来调整思路，让模型先自行解决这个问题，再根据自己的解法跟学生的解法进行对比，然后再判断学生的解法是否正确。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">请判断学生的解决方案是否正确，请通过如下步骤解决这个问题：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">步骤：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    首先，自己解决问题。
</span><span style="color:#f1fa8c">    然后将您的解决方案与学生的解决方案进行比较，对比计算得到的总费用与学生计算的总费用是否一致，并评估学生的解决方案是否正确。
</span><span style="color:#f1fa8c">    在自己完成问题之前，请勿决定学生的解决方案是否正确。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">使用以下格式：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    问题：问题文本
</span><span style="color:#f1fa8c">    学生的解决方案：学生的解决方案文本
</span><span style="color:#f1fa8c">    实际解决方案和步骤：实际解决方案和步骤文本
</span><span style="color:#f1fa8c">    学生计算的总费用：学生计算得到的总费用
</span><span style="color:#f1fa8c">    实际计算的总费用：实际计算出的总费用
</span><span style="color:#f1fa8c">    学生计算的费用和实际计算的费用是否相同：是或否
</span><span style="color:#f1fa8c">    学生的解决方案和实际解决方案是否相同：是或否
</span><span style="color:#f1fa8c">    学生的成绩：正确或不正确
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">问题：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    我正在建造一个太阳能发电站，需要帮助计算财务。 
</span><span style="color:#f1fa8c">    - 土地费用为每平方英尺100美元
</span><span style="color:#f1fa8c">    - 我可以以每平方英尺250美元的价格购买太阳能电池板
</span><span style="color:#f1fa8c">    - 我已经谈判好了维护合同，每年需要支付固定的10万美元，并额外支付每平方英尺10美元;
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    作为平方英尺数的函数，首年运营的总费用是多少。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">学生的解决方案：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    设x为发电站的大小，单位为平方英尺。
</span><span style="color:#f1fa8c">    费用：
</span><span style="color:#f1fa8c">    1. 土地费用：100x美元
</span><span style="color:#f1fa8c">    2. 太阳能电池板费用：250x美元
</span><span style="color:#f1fa8c">    3. 维护费用：100,000+100x=10万美元+10x美元
</span><span style="color:#f1fa8c">    总费用：100x美元+250x美元+10万美元+100x美元=450x+10万美元
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">实际解决方案和步骤：
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), prompt)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>首先，我们需要计算土地费用、太阳能电池板费用和维护费用的总和。

土地费用：100美元/平方英尺 * x平方英尺 = 100x美元
太阳能电池板费用：250美元/平方英尺 * x平方英尺 = 250x美元
维护费用：10万美元 + 10美元/平方英尺 * x平方英尺 = 10万美元 + 10x美元

总费用：100x美元 + 250x美元 + 10万美元 + 10x美元 = 360x + 10万美元

学生计算的总费用：450x + 10万美元
实际计算的总费用：360x + 10万美元
学生计算的费用和实际计算的费用是否相同：否
学生的解决方案和实际解决方案是否相同：否
学生的成绩：不正确
</code></pre><h3 id="局限性">局限性</h3>
<p>模型偶尔会生成一些看似真实，实际上不存在的知识。模型经过大量预训练，虽然掌握了丰富知识，但是难以判断自己的知识边界，可能会做出错误推断。这个现象称为幻觉（Hallucination），
是语言模型的一大陷阱。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">告诉我华为公司生产的GT Watch运动手表的相关信息
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), prompt)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>华为公司生产的GT Watch运动手表是一款运动智能手表，具有多项健康功能和运动追踪功能，如心率监测、睡眠 Watch还支持多种运动模式，包括跑步、骑行、游泳等，
可全面监测和分析用户的运动数据。此外，GT Watch还观，适合运动和日常佩戴。
</code></pre><p>可以通过Prompt设计减少幻觉的发生，比如让语言模型直接引用文本中的原句，然后再进行解答。语言模型的幻觉问题事关应用的可靠性与安全性。，采取prompt优化措施可以缓解，这也是未来语言模型
进化的重要方向之一。</p>
<h3 id="英文原版prompt">英文原版Prompt</h3>
<p>1.1 使用分隔符隔离不同输入部分</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">	You should express what you want a model to do by \ 
</span><span style="color:#f1fa8c">providing instructions that are as clear and \ 
</span><span style="color:#f1fa8c">specific as you can possibly make them. \ 
</span><span style="color:#f1fa8c">This will guide the model towards the desired output, \ 
</span><span style="color:#f1fa8c">and reduce the chances of receiving irrelevant \ 
</span><span style="color:#f1fa8c">or incorrect responses. Don&#39;t confuse writing a \ 
</span><span style="color:#f1fa8c">clear prompt with writing a short prompt. \ 
</span><span style="color:#f1fa8c">In many cases, longer prompts provide more clarity \ 
</span><span style="color:#f1fa8c">and context for the model, which can lead to \ 
</span><span style="color:#f1fa8c">more detailed and relevant outputs.
</span><span style="color:#f1fa8c">`</span>
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Summarize the text delimited by two double quotes \ 
</span><span style="color:#f1fa8c">into a single sentence.
</span><span style="color:#f1fa8c">&#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>Clear and specific instructions for a model will guide it towards the desired output, reducing the chances of irrelevant or incorrect responses, 
and longer prompts can provide more clarity and context, leading to more detailed and relevant outputs.
</code></pre><p>1.2 结构化输出</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Generate a list of three made-up book titles along \ 
</span><span style="color:#f1fa8c">with their authors and genres. 
</span><span style="color:#f1fa8c">Provide them in JSON format with the following keys: 
</span><span style="color:#f1fa8c">book_id, title, author, genre.
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), prompt)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>{
  &quot;books&quot;: [
    {
      &quot;book_id&quot;: 1,
      &quot;title&quot;: &quot;The Forgotten Symphony&quot;,
      &quot;author&quot;: &quot;Madeline Harper&quot;,
      &quot;genre&quot;: &quot;Mystery&quot;
    },
    {
      &quot;book_id&quot;: 2,
      &quot;title&quot;: &quot;Echoes of Eternity&quot;,
      &quot;author&quot;: &quot;Jackson Pierce&quot;,
      &quot;genre&quot;: &quot;Science Fiction&quot;
    },
    {
      &quot;book_id&quot;: 3,
      &quot;title&quot;: &quot;The Enchanted Garden&quot;,
      &quot;author&quot;: &quot;Sophie Evans&quot;,
      &quot;genre&quot;: &quot;Fantasy&quot;
    }
  ]
}
</code></pre><p>1.3 模型检查是否满足条件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text1 <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">	Making a cup of tea is easy! First, you need to get some \
</span><span style="color:#f1fa8c">	water boiling. While that&#39;s happening, \ 
</span><span style="color:#f1fa8c">	grab a cup and put a tea bag in it. Once the water is \
</span><span style="color:#f1fa8c">	hot enough, just pour it over the tea bag. \
</span><span style="color:#f1fa8c">	Let it sit for a bit so the tea can steep. After a \
</span><span style="color:#f1fa8c">	few minutes, take out the tea bag. If you \
</span><span style="color:#f1fa8c">	like, you can add some sugar or milk to taste. \
</span><span style="color:#f1fa8c">	And that&#39;s it! You&#39;ve got yourself a delicious \
</span><span style="color:#f1fa8c">	cup of tea to enjoy.
</span><span style="color:#f1fa8c">	`</span>
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">You will be provided with text delimited by two double quotes. 
</span><span style="color:#f1fa8c">If it contains a sequence of instructions, \ 
</span><span style="color:#f1fa8c">re-write those instructions in the following format:
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Step 1 - ...
</span><span style="color:#f1fa8c">Step 2 - …
</span><span style="color:#f1fa8c">…
</span><span style="color:#f1fa8c">Step N - …
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">If the text does not contain a sequence of instructions, \ 
</span><span style="color:#f1fa8c">then simply write \&#34;No steps provided.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text1))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>Step 1 - Get some water boiling.
Step 2 - Grab a cup and put a tea bag in it.
Step 3 - Pour hot water over the tea bag.
Step 4 - Let the tea steep for a few minutes.
Step 5 - Remove the tea bag.
Step 6 - Add sugar or milk to taste.
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text2 <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">The sun is shining brightly today, and the birds are \
</span><span style="color:#f1fa8c">singing. It&#39;s a beautiful day to go for a \ 
</span><span style="color:#f1fa8c">walk in the park. The flowers are blooming, and the \ 
</span><span style="color:#f1fa8c">trees are swaying gently in the breeze. People \ 
</span><span style="color:#f1fa8c">are out and about, enjoying the lovely weather. \ 
</span><span style="color:#f1fa8c">Some are having picnics, while others are playing \ 
</span><span style="color:#f1fa8c">games or simply relaxing on the grass. It&#39;s a \ 
</span><span style="color:#f1fa8c">perfect day to spend time outdoors and appreciate the \ 
</span><span style="color:#f1fa8c">beauty of nature.
</span><span style="color:#f1fa8c">	`</span>
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">You will be provided with text delimited by two double quotes. 
</span><span style="color:#f1fa8c">If it contains a sequence of instructions, \ 
</span><span style="color:#f1fa8c">re-write those instructions in the following format:
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Step 1 - ...
</span><span style="color:#f1fa8c">Step 2 - …
</span><span style="color:#f1fa8c">…
</span><span style="color:#f1fa8c">Step N - …
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">If the text does not contain a sequence of instructions, \ 
</span><span style="color:#f1fa8c">then simply write \&#34;No steps provided.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text2))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>No steps provided.
</code></pre><p>1.4 提供少量示例</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Your task is to answer in a consistent style.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;child&gt;: Teach me about patience.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;grandparent&gt;: The river that carves the deepest \ 
</span><span style="color:#f1fa8c">valley flows from a modest spring; the \ 
</span><span style="color:#f1fa8c">grandest symphony originates from a single note; \ 
</span><span style="color:#f1fa8c">the most intricate tapestry begins with a solitary thread.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;child&gt;: Teach me about resilience.
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), prompt)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>&lt;grandparent&gt;: Resilience is like the mighty oak that withstands the strongest of storms, bending but never breaking. It is the spirit that rises from adversity, the strength that perseveres in the face of challenges. Like the phoenix that rises from the ashes, resilience is the ability to bounce back and thrive, no matter the circumstances.
</code></pre><p>2.1 指定完成任务所需的步骤</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">In a charming village, siblings Jack and Jill set out on \ 
</span><span style="color:#f1fa8c">a quest to fetch water from a hilltop \ 
</span><span style="color:#f1fa8c">well. As they climbed, singing joyfully, misfortune \ 
</span><span style="color:#f1fa8c">struck—Jack tripped on a stone and tumbled \ 
</span><span style="color:#f1fa8c">down the hill, with Jill following suit. \ 
</span><span style="color:#f1fa8c">Though slightly battered, the pair returned home to \ 
</span><span style="color:#f1fa8c">comforting embraces. Despite the mishap, \ 
</span><span style="color:#f1fa8c">their adventurous spirits remained undimmed, and they \ 
</span><span style="color:#f1fa8c">continued exploring with delight.
</span><span style="color:#f1fa8c">	`</span>
	prompt1 <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Perform the following actions: 
</span><span style="color:#f1fa8c">1 - Summarize the following text delimited by two double \
</span><span style="color:#f1fa8c">backticks with 1 sentence.
</span><span style="color:#f1fa8c">2 - Translate the summary into French.
</span><span style="color:#f1fa8c">3 - List each name in the French summary.
</span><span style="color:#f1fa8c">4 - Output a json object that contains the following \
</span><span style="color:#f1fa8c">keys: french_summary, num_names.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Separate your answers with line breaks.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Text:
</span><span style="color:#f1fa8c">&#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt1, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(<span style="color:#f1fa8c">&#34;Completion for prompt 1: \n&#34;</span>, resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>Completion for prompt 1: 
 1 - Jack and Jill, despite a mishap on their quest for water, returned home with adventurous spirits undimmed.

2 - Jack et Jill, malgré un accident dans leur quête d'eau, sont rentrés chez eux avec des esprits aventureux intacts.

3 - Jack, Jill

4 - 
{
  &quot;french_summary&quot;: &quot;Jack et Jill, malgré un accident dans leur quête d'eau, sont rentrés chez eux avec des esprits aventureux intacts.&quot;,
  &quot;num_names&quot;: 2
}
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">In a charming village, siblings Jack and Jill set out on \ 
</span><span style="color:#f1fa8c">a quest to fetch water from a hilltop \ 
</span><span style="color:#f1fa8c">well. As they climbed, singing joyfully, misfortune \ 
</span><span style="color:#f1fa8c">struck—Jack tripped on a stone and tumbled \ 
</span><span style="color:#f1fa8c">down the hill, with Jill following suit. \ 
</span><span style="color:#f1fa8c">Though slightly battered, the pair returned home to \ 
</span><span style="color:#f1fa8c">comforting embraces. Despite the mishap, \ 
</span><span style="color:#f1fa8c">their adventurous spirits remained undimmed, and they \ 
</span><span style="color:#f1fa8c">continued exploring with delight.
</span><span style="color:#f1fa8c">	`</span>
	prompt2 <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Your task is to perform the following actions: 
</span><span style="color:#f1fa8c">1 - Summarize the following text delimited by &lt;&gt; with 1 sentence.
</span><span style="color:#f1fa8c">2 - Translate the summary into French.
</span><span style="color:#f1fa8c">3 - List each name in the French summary.
</span><span style="color:#f1fa8c">4 - Output a json object that contains the 
</span><span style="color:#f1fa8c">following keys: french_summary, num_names.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Use the following format:
</span><span style="color:#f1fa8c">Text: &lt;text to summarize&gt;
</span><span style="color:#f1fa8c">Summary: &lt;summary&gt;
</span><span style="color:#f1fa8c">Translation: &lt;summary translation&gt;
</span><span style="color:#f1fa8c">Names: &lt;list of names in French summary&gt;
</span><span style="color:#f1fa8c">Output JSON: &lt;json with summary and num_names&gt;
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Text: &lt;%s&gt;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt2, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(<span style="color:#f1fa8c">&#34;Completion for prompt 2: \n&#34;</span>, resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>Completion for prompt 2: 
 Summary: Jack and Jill, siblings, go on a quest to fetch water from a hilltop well but face misfortune on the way back home.

Translation: Jack et Jill, frère et sœur, partent à la quête d'eau d'un puits au sommet d'une colline mais font face à la malchance en rentrant chez eux.

Names: Jack, Jill

Output JSON:
{
  &quot;french_summary&quot;: &quot;Jack et Jill, frère et sœur, partent à la quête d'eau d'un puits au sommet d'une colline mais font face à la malchance en rentrant chez eux.&quot;,
  &quot;num_names&quot;: 2
}
</code></pre><p>2.2 指导模型在下结论之前找出一个自己的解法</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Determine if the student&#39;s solution is correct or not.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Question:
</span><span style="color:#f1fa8c">I&#39;m building a solar power installation and I need \
</span><span style="color:#f1fa8c"> help working out the financials. 
</span><span style="color:#f1fa8c">- Land costs $100 / square foot
</span><span style="color:#f1fa8c">- I can buy solar panels for $250 / square foot
</span><span style="color:#f1fa8c">- I negotiated a contract for maintenance that will cost \ 
</span><span style="color:#f1fa8c">me a flat $100k per year, and an additional $10 / square \
</span><span style="color:#f1fa8c">foot
</span><span style="color:#f1fa8c">What is the total cost for the first year of operations 
</span><span style="color:#f1fa8c">as a function of the number of square feet.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Student&#39;s Solution:
</span><span style="color:#f1fa8c">Let x be the size of the installation in square feet.
</span><span style="color:#f1fa8c">Costs:
</span><span style="color:#f1fa8c">1. Land cost: 100x
</span><span style="color:#f1fa8c">2. Solar panel cost: 250x
</span><span style="color:#f1fa8c">3. Maintenance cost: 100,000 + 100x
</span><span style="color:#f1fa8c">Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), prompt)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>The student's solution is correct. They have correctly calculated the total cost for the first year of operations as a function of the number of square feet.
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Your task is to determine if the student&#39;s solution \
</span><span style="color:#f1fa8c">is correct or not.
</span><span style="color:#f1fa8c">To solve the problem do the following:
</span><span style="color:#f1fa8c">- First, work out your own solution to the problem. 
</span><span style="color:#f1fa8c">- Then compare your solution to the student&#39;s solution \ 
</span><span style="color:#f1fa8c">and evaluate if the student&#39;s solution is correct or not. 
</span><span style="color:#f1fa8c">Don&#39;t decide if the student&#39;s solution is correct until 
</span><span style="color:#f1fa8c">you have done the problem yourself.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Use the following format:
</span><span style="color:#f1fa8c">Question:
</span><span style="color:#f1fa8c">I&#39;m building a solar power installation and I need \
</span><span style="color:#f1fa8c"> help working out the financials. 
</span><span style="color:#f1fa8c">- Land costs $100 / square foot
</span><span style="color:#f1fa8c">- I can buy solar panels for $250 / square foot
</span><span style="color:#f1fa8c">- I negotiated a contract for maintenance that will cost \ 
</span><span style="color:#f1fa8c">me a flat $100k per year, and an additional $10 / square \
</span><span style="color:#f1fa8c">foot
</span><span style="color:#f1fa8c">What is the total cost for the first year of operations 
</span><span style="color:#f1fa8c">as a function of the number of square feet.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Student&#39;s Solution:
</span><span style="color:#f1fa8c">Let x be the size of the installation in square feet.
</span><span style="color:#f1fa8c">Costs:
</span><span style="color:#f1fa8c">1. Land cost: 100x
</span><span style="color:#f1fa8c">2. Solar panel cost: 250x
</span><span style="color:#f1fa8c">3. Maintenance cost: 100,000 + 100x
</span><span style="color:#f1fa8c">Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Actual solution:
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Is the student&#39;s solution the same as actual solution \
</span><span style="color:#f1fa8c">just calculated:
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Student grade:
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), prompt)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>Question:
I'm building a solar power installation and I need help working out the financials. 
- Land costs $100 / square foot
- I can buy solar panels for $250 / square foot
- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot
What is the total cost for the first year of operations as a function of the number of square feet.

Student's Solution:
Let x be the size of the installation in square feet.
Costs:
1. Land cost: 100x
2. Solar panel cost: 250x
3. Maintenance cost: 100,000 + 10x
Total cost: 100x + 250x + 100,000 + 10x = 360x + 100,000

Actual solution:
Total cost = Land cost + Solar panel cost + Maintenance cost
Total cost = 100x + 250x + 100,000 + 10x
Total cost = 360x + 100,000

Is the student's solution the same as the actual solution just calculated:
No, the student's solution for the maintenance cost is incorrect. The correct maintenance cost is $100,000 + $10x, not $100,000 + 10x. Therefore, the student's total cost is also incorrect.

Student grade:
F. The student's solution is not correct. They made an error in calculating the maintenance cost.
</code></pre><p>3.1 幻觉</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), prompt)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>The AeroGlide UltraSlim Smart Toothbrush by Boie is a toothbrush designed to provide a comfortable and effective brushing experience. It features a slim and sleek design that allows it to easily reach all areas of the mouth, including tight spaces and the back of the mouth. 

The toothbrush is equipped with smart technology that monitors your brushing habits and provides real-time feedback to help you improve your technique. It also has a gentle vibrating motion that helps to remove plaque and food particles without causing discomfort or irritation to the gums.

Additionally, the AeroGlide UltraSlim Smart Toothbrush is made with high-quality, durable materials that are designed to last, and it is easy to clean and maintain. It is a convenient and practical option for those looking to upgrade their oral hygiene routine.
</code></pre><h2 id="迭代优化iterative">迭代优化Iterative</h2>
<p>没有适用于所有场景的最佳Prompt，开发高效Prompt的关键在于找个一个好的迭代优化过程。通过快速试错迭代，有效确定特定应用的最佳Prompt形式。</p>
<h3 id="产品说明书生成营销产品描述">产品说明书生成营销产品描述</h3>
<p>示例：给定一份椅子的资料页。描述说它属于中世纪灵感系列，产自意大利，并介绍了材料、构造、尺寸、可选配件等参数。</p>
<h4 id="初始提示">初始提示</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">概述
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    美丽的中世纪风格办公家具系列的一部分，包括文件柜、办公桌、书柜、会议桌等。
</span><span style="color:#f1fa8c">    多种外壳颜色和底座涂层可选。
</span><span style="color:#f1fa8c">    可选塑料前后靠背装饰（SWC-100）或10种面料和6种皮革的全面装饰（SWC-110）。
</span><span style="color:#f1fa8c">    底座涂层选项为：不锈钢、哑光黑色、光泽白色或铬。
</span><span style="color:#f1fa8c">    椅子可带或不带扶手。
</span><span style="color:#f1fa8c">    适用于家庭或商业场所。
</span><span style="color:#f1fa8c">    符合合同使用资格。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">结构
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    五个轮子的塑料涂层铝底座。
</span><span style="color:#f1fa8c">    气动椅子调节，方便升降。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">尺寸
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    宽度53厘米|20.87英寸
</span><span style="color:#f1fa8c">    深度51厘米|20.08英寸
</span><span style="color:#f1fa8c">    高度80厘米|31.50英寸
</span><span style="color:#f1fa8c">    座椅高度44厘米|17.32英寸
</span><span style="color:#f1fa8c">    座椅深度41厘米|16.14英寸
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">选项
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    软地板或硬地板滚轮选项。
</span><span style="color:#f1fa8c">    两种座椅泡沫密度可选：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺）。
</span><span style="color:#f1fa8c">    无扶手或8个位置PU扶手。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">材料
</span><span style="color:#f1fa8c">外壳底座滑动件
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    改性尼龙PA6/PA66涂层的铸铝。
</span><span style="color:#f1fa8c">    外壳厚度：10毫米。
</span><span style="color:#f1fa8c">    座椅
</span><span style="color:#f1fa8c">    HD36泡沫
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">原产国
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    意大利
</span><span style="color:#f1fa8c">`</span>

	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">您的任务是帮助营销团队基于技术说明书创建一个产品的营销描述。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">根据两个双引号标记的技术说明书中提供的信息，编写一个产品描述。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">技术说明: &#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>产品描述：

我们自豪地推出了美丽的中世纪风格办公家具系列的一部分——我们的办公椅。这款办公椅包括了文件柜、办公桌、书柜、会议桌等，为您的办公室带来了高雅与实用的完美结合。

这款办公椅可选择多种外壳颜色和底座涂层，同时可选塑料前后靠背装饰或多种面料和皮革装饰。底座涂层选项包括不锈钢、哑光黑色、光泽白色或铬，椅子可带或不带扶手，适用于家庭或商业场所，并符合合同使用资格。

其结构设计采用了五个轮子的塑料涂层铝底座，并配备气动椅子调节，方便升降。

此外，我们的办公椅尺寸紧凑，适合各种场所使用。您可以选择软地板或硬地板滚轮，两种座椅泡沫密度可选，以及选择带有或无扶手的版本。

这款办公椅使用改性尼龙PA6/PA66涂层的铸铝和HD36泡沫材料制成，原产国为意大利。

无论是家庭办公室还是商业办公室，我们的办公椅都能为您的工作场所带来舒适和高雅。
</code></pre><h4 id="优化1解决生成文本太长">优化1：解决生成文本太长</h4>
<p>在Prompt中添加长度限制，生成更简洁的文案</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">概述
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    美丽的中世纪风格办公家具系列的一部分，包括文件柜、办公桌、书柜、会议桌等。
</span><span style="color:#f1fa8c">    多种外壳颜色和底座涂层可选。
</span><span style="color:#f1fa8c">    可选塑料前后靠背装饰（SWC-100）或10种面料和6种皮革的全面装饰（SWC-110）。
</span><span style="color:#f1fa8c">    底座涂层选项为：不锈钢、哑光黑色、光泽白色或铬。
</span><span style="color:#f1fa8c">    椅子可带或不带扶手。
</span><span style="color:#f1fa8c">    适用于家庭或商业场所。
</span><span style="color:#f1fa8c">    符合合同使用资格。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">结构
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    五个轮子的塑料涂层铝底座。
</span><span style="color:#f1fa8c">    气动椅子调节，方便升降。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">尺寸
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    宽度53厘米|20.87英寸
</span><span style="color:#f1fa8c">    深度51厘米|20.08英寸
</span><span style="color:#f1fa8c">    高度80厘米|31.50英寸
</span><span style="color:#f1fa8c">    座椅高度44厘米|17.32英寸
</span><span style="color:#f1fa8c">    座椅深度41厘米|16.14英寸
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">选项
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    软地板或硬地板滚轮选项。
</span><span style="color:#f1fa8c">    两种座椅泡沫密度可选：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺）。
</span><span style="color:#f1fa8c">    无扶手或8个位置PU扶手。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">材料
</span><span style="color:#f1fa8c">外壳底座滑动件
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    改性尼龙PA6/PA66涂层的铸铝。
</span><span style="color:#f1fa8c">    外壳厚度：10毫米。
</span><span style="color:#f1fa8c">    座椅
</span><span style="color:#f1fa8c">    HD36泡沫
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">原产国
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">    意大利
</span><span style="color:#f1fa8c">`</span>

	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">您的任务是帮助营销团队基于技术说明书创建一个产品的营销描述。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">根据两个双引号标记的技术说明书中提供的信息，编写一个产品描述。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">使用最多50个词。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">技术说明: &#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
	fmt.<span style="color:#50fa7b">Println</span>(res)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>产品描述：

我们的中世纪风格办公家具系列包括文件柜、办公桌、书柜和会议桌，外壳颜色和底座涂层可定制。座椅可选择意大利，符合合同使用资格。
</code></pre><p>虽然语言模型对长度约束的遵循不是百分之百精确，但通过迭代测试可以找到最佳的长度提示表达式，使生成文本基本符合长度要求。
因为语言模型在计算和判断文本长度时依赖于分词器，而分词器在字符统计方面不具备完美精度。</p>
<h4 id="优化2处理抓错文本细节">优化2：处理抓错文本细节</h4>
<p>根据不同目标受众关注不同的方面，输出风格和内容都适合的文本。</p>
<pre tabindex="0"><code>	prompt := `
您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。

根据两个双引号标记的技术说明书中提供的信息，编写一个产品描述。

该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。

使用最多50个单词。

技术规格: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<pre tabindex="0"><code>产品描述：中世纪风格办公家具系列，包括文件柜、办公桌、书柜、会议桌等。可供选择多种外壳颜色和底座涂层，尺寸为宽53厘米、深51厘米、高80厘米。材料包括改性尼龙PA6/PA66涂层的铸铝和HD36泡沫。原产国为意大利。
</code></pre><p>通过修改Prompt，模型关注点变成了具体特征与技术细节。如果想要进一步展示出具体产品的ID，可以再次修改Prompt.</p>
<pre tabindex="0"><code>	prompt := `
您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。

根据两个双引号标记的技术说明书中提供的信息，编写一个产品描述。

该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。

在描述末尾，包括技术规格中每个7个字符的产品ID.

使用最多50个单词。

技术规格: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<pre tabindex="0"><code>该产品是美丽的中世纪风格办公家具系列的一部分，包括文件柜、办公桌和书柜。外壳颜色和底座涂层可选，可选塑不带扶手，适用于家庭或商业场所。结构包括塑料涂层铝底座和气动椅子调节。尺寸为53厘米宽、51厘米深、80厘米包括改性尼龙PA6/PA66涂层的铸铝外壳底座滑动件和HD36泡沫座椅。原产国为意大利。产品ID：SWC-100。
</code></pre><p>Prompt设计是一个循序渐进的过程，需要做好多次尝试和错误的准备，通过不断调整和优化，才能找到最符合具体场景的Prompt方式。</p>
<h4 id="优化3添加表格描述">优化3：添加表格描述</h4>
<p>继续迭代优化，要求提取产品尺寸信息并组织成表格，并指定表格的列、表名和格式；再将所有内容格式化为可以在网页使用的HTML。</p>
<pre tabindex="0"><code>	prompt := `
您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。

根据两个双引号标记的技术说明书中提供的信息，编写一个产品描述。

该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。

在描述末尾，包括技术规格中每个7个字符的产品ID。

在描述之后，包括一个表格，提供产品的尺寸。表格应该有两列。第一列包括尺寸的名称。第二列只包括英寸的测量值。

给表格命名为&quot;产品尺寸&quot;。

将所有内容格式化为可用于网站的HTML格式。将描述放在&lt;div&gt;元素中。

技术规格: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-html" data-lang="html"><span style="color:#ff79c6">&lt;!DOCTYPE html&gt;</span>
&lt;<span style="color:#ff79c6">html</span> <span style="color:#50fa7b">lang</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;en&#34;</span>&gt;
&lt;<span style="color:#ff79c6">head</span>&gt;
    &lt;<span style="color:#ff79c6">meta</span> <span style="color:#50fa7b">charset</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;UTF-8&#34;</span>&gt;
    &lt;<span style="color:#ff79c6">meta</span> <span style="color:#50fa7b">name</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;viewport&#34;</span> <span style="color:#50fa7b">content</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;width=device-width, initial-scale=1.0&#34;</span>&gt;
    &lt;<span style="color:#ff79c6">title</span>&gt;Product Description&lt;/<span style="color:#ff79c6">title</span>&gt;
&lt;/<span style="color:#ff79c6">head</span>&gt;
&lt;<span style="color:#ff79c6">body</span>&gt;
    &lt;<span style="color:#ff79c6">div</span>&gt;
        &lt;<span style="color:#ff79c6">h2</span>&gt;产品描述&lt;/<span style="color:#ff79c6">h2</span>&gt;
        &lt;<span style="color:#ff79c6">p</span>&gt;我们隆重推出美丽的中世纪风格办公家具系列，包括文件柜、办公桌、书柜、会议桌等。您可以根据饰。底座涂层选项为不锈钢、哑光黑色、光泽白色或铬，而椅子可选择带扶手或不带扶手。这些家具适用于家庭或商业场所，并且符合合同使用资格。&lt;/<span style="color:#ff79c6">p</span>&gt;
        &lt;<span style="color:#ff79c6">p</span>&gt;产品的结构特点包括五个轮子的塑料涂层铝底座和气动椅子调节功能，方便升降。&lt;/<span style="color:#ff79c6">p</span>&gt;
        &lt;<span style="color:#ff79c6">h3</span>&gt;材料构造&lt;/<span style="color:#ff79c6">h3</span>&gt;
        &lt;<span style="color:#ff79c6">p</span>&gt;外壳底座滑动件由改性尼龙PA6/PA66涂层的铸铝制成，外壳厚度为10毫米。而座椅使用了HD36泡沫材料。&lt;/<span style="color:#ff79c6">p</span>&gt;
        &lt;<span style="color:#ff79c6">p</span>&gt;产品ID: 5623874&lt;/<span style="color:#ff79c6">p</span>&gt;
    &lt;/<span style="color:#ff79c6">div</span>&gt;

    &lt;<span style="color:#ff79c6">h2</span>&gt;产品尺寸&lt;/<span style="color:#ff79c6">h2</span>&gt;
    &lt;<span style="color:#ff79c6">table</span>&gt;
        &lt;<span style="color:#ff79c6">tr</span>&gt;
            &lt;<span style="color:#ff79c6">td</span>&gt;宽度&lt;/<span style="color:#ff79c6">td</span>&gt;
            &lt;<span style="color:#ff79c6">td</span>&gt;20.87英寸&lt;/<span style="color:#ff79c6">td</span>&gt;
        &lt;/<span style="color:#ff79c6">tr</span>&gt;
        &lt;<span style="color:#ff79c6">tr</span>&gt;
            &lt;<span style="color:#ff79c6">td</span>&gt;深度&lt;/<span style="color:#ff79c6">td</span>&gt;
            &lt;<span style="color:#ff79c6">td</span>&gt;20.08英寸&lt;/<span style="color:#ff79c6">td</span>&gt;
        &lt;/<span style="color:#ff79c6">tr</span>&gt;
        &lt;<span style="color:#ff79c6">tr</span>&gt;
            &lt;<span style="color:#ff79c6">td</span>&gt;高度&lt;/<span style="color:#ff79c6">td</span>&gt;
            &lt;<span style="color:#ff79c6">td</span>&gt;31.50英寸&lt;/<span style="color:#ff79c6">td</span>&gt;
        &lt;/<span style="color:#ff79c6">tr</span>&gt;
        &lt;<span style="color:#ff79c6">tr</span>&gt;
            &lt;<span style="color:#ff79c6">td</span>&gt;座椅高度&lt;/<span style="color:#ff79c6">td</span>&gt;
            &lt;<span style="color:#ff79c6">td</span>&gt;17.32英寸&lt;/<span style="color:#ff79c6">td</span>&gt;
        &lt;/<span style="color:#ff79c6">tr</span>&gt;
        &lt;<span style="color:#ff79c6">tr</span>&gt;
            &lt;<span style="color:#ff79c6">td</span>&gt;座椅深度&lt;/<span style="color:#ff79c6">td</span>&gt;
            &lt;<span style="color:#ff79c6">td</span>&gt;16.14英寸&lt;/<span style="color:#ff79c6">td</span>&gt;
        &lt;/<span style="color:#ff79c6">tr</span>&gt;
    &lt;/<span style="color:#ff79c6">table</span>&gt;
&lt;/<span style="color:#ff79c6">body</span>&gt;
&lt;/<span style="color:#ff79c6">html</span>&gt;
</code></pre></div><p>浏览器查看</p>
<pre tabindex="0"><code>产品描述
我们隆重推出美丽的中世纪风格办公家具系列，包括文件柜、办公桌、书柜、会议桌等。您可以根据饰。底座涂层选项为不锈钢、哑光黑色、光泽白色或铬，而椅子可选择带扶手或不带扶手。这些家具适用于家庭或商业场所，并且符合合同使用资格。

产品的结构特点包括五个轮子的塑料涂层铝底座和气动椅子调节功能，方便升降。

材料构造
外壳底座滑动件由改性尼龙PA6/PA66涂层的铸铝制成，外壳厚度为10毫米。而座椅使用了HD36泡沫材料。

产品ID: 5623874

产品尺寸
宽度	20.87英寸
深度	20.08英寸
高度	31.50英寸
座椅高度	17.32英寸
座椅深度	16.14英寸
</code></pre><h3 id="总结">总结</h3>
<p>Prompt的核心是掌握Prompt的迭代开发和优化技巧，通过不断调整试错，最终找到可靠适用的Prompt形式才是Prompt设计的正确方法。</p>
<h3 id="英文原版">英文原版</h3>
<p>产品说明书：</p>
<pre tabindex="0"><code>text := `
OVERVIEW
- Part of a beautiful family of mid-century inspired office furniture, 
including filing cabinets, desks, bookcases, meeting tables, and more.
- Several options of shell color and base finishes.
- Available with plastic back and front upholstery (SWC-100) 
or full upholstery (SWC-110) in 10 fabric and 6 leather options.
- Base finish options are: stainless steel, matte black, 
gloss white, or chrome.
- Chair is available with or without armrests.
- Suitable for home or business settings.
- Qualified for contract use.

CONSTRUCTION
- 5-wheel plastic coated aluminum base.
- Pneumatic chair adjust for easy raise/lower action.

DIMENSIONS
- WIDTH 53 CM | 20.87”
- DEPTH 51 CM | 20.08”
- HEIGHT 80 CM | 31.50”
- SEAT HEIGHT 44 CM | 17.32”
- SEAT DEPTH 41 CM | 16.14”

OPTIONS
- Soft or hard-floor caster options.
- Two choices of seat foam densities: 
medium (1.8 lb/ft3) or high (2.8 lb/ft3)
- Armless or 8 position PU armrests 

MATERIALS
SHELL BASE GLIDER
- Cast Aluminum with modified nylon PA6/PA66 coating.
- Shell thickness: 10 mm.
SEAT
- HD36 foam

COUNTRY OF ORIGIN
- Italy
`
</code></pre><p>1.1 英文初始提示</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go">prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Your task is to help a marketing team create a 
</span><span style="color:#f1fa8c">description for a retail website of a product based 
</span><span style="color:#f1fa8c">on a technical fact sheet.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Write a product description based on the information 
</span><span style="color:#f1fa8c">provided in the technical specifications delimited by 
</span><span style="color:#f1fa8c">two double quotes.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Technical specifications: &#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
	fmt.<span style="color:#50fa7b">Println</span>(res)
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>Introducing our new mid-century inspired office chair, the perfect addition to any home or business setting. This chair is part of a beautiful family of office furniture, offering a range of options for shell color and base finishes to suit your personal style.

Choose from plastic back and front upholstery or full upholstery in a variety of fabric and leather options. The chair is also available with or without armrests, allowing you to customize it to your specific needs.

Constructed with a 5-wheel plastic coated aluminum base and a pneumatic chair adjust, this chair offers easy raise/lower action for maximum comfort. With dimensions of 53 cm in width, 51 cm in depth, and a seat height of 44 cm, this chair is both stylish and practical.

You also have the option to choose between soft or hard-floor casters and two choices of seat foam densities. The materials used are of the highest quality and the chair is qualified for contract use, making it a durable and long-lasting investment.

Designed and crafted in Italy, this office chair embodies both style and functionality, making it the perfect addition to any workspace. Elevate your office space with our mid-century inspired office chair today.
</code></pre><p>1.2 限制生成长度</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go">prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Your task is to help a marketing team create a 
</span><span style="color:#f1fa8c">description for a retail website of a product based 
</span><span style="color:#f1fa8c">on a technical fact sheet.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Write a product description based on the information 
</span><span style="color:#f1fa8c">provided in the technical specifications delimited by 
</span><span style="color:#f1fa8c">two double quotes.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Use at most 50 words.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Technical specifications: &#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
	fmt.<span style="color:#50fa7b">Println</span>(res)
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>Elevate your office space with our mid-century inspired office chair. With multiple shell color and base finish options, as well as upholstery choices in fabric or leather, you can customize it to fit your style. Designed for both home and business settings, this chair is perfect for any environment. Made in Italy with high-quality materials, this chair is a stylish and functional addition to any workspace.
</code></pre><p>1.3 处理抓错文本细节</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go">	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Your task is to help a marketing team create a 
</span><span style="color:#f1fa8c">description for a retail website of a product based 
</span><span style="color:#f1fa8c">on a technical fact sheet.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Write a product description based on the information 
</span><span style="color:#f1fa8c">provided in the technical specifications delimited by 
</span><span style="color:#f1fa8c">two double quotes.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">The description is intended for furniture retailers, 
</span><span style="color:#f1fa8c">so should be technical in nature and focus on the 
</span><span style="color:#f1fa8c">materials the product is constructed from.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Use at most 50 words.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Technical specifications: &#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
	fmt.<span style="color:#50fa7b">Println</span>(res)
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>Introducing the SWC-100 and SWC-110 office chairs, part of our mid-century inspired furniture collection. Choose from a variety of shell colors and base finishes, with options for upholstery in fabric or leather. Constructed with a 5-wheel plastic-coated aluminum base and high-quality foam, these chairs are perfect for any home or business setting. Made in Italy.
</code></pre><p>在描述末尾包含 7个字符的产品ID</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go">	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Your task is to help a marketing team create a 
</span><span style="color:#f1fa8c">description for a retail website of a product based 
</span><span style="color:#f1fa8c">on a technical fact sheet.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Write a product description based on the information 
</span><span style="color:#f1fa8c">provided in the technical specifications delimited by 
</span><span style="color:#f1fa8c">two double quotes.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">The description is intended for furniture retailers, 
</span><span style="color:#f1fa8c">so should be technical in nature and focus on the 
</span><span style="color:#f1fa8c">materials the product is constructed from.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">At the end of the description, include every 7-character 
</span><span style="color:#f1fa8c">Product ID in the technical specification.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Use at most 50 words.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Technical specifications: &#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
	fmt.<span style="color:#50fa7b">Println</span>(res)
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>Introducing our mid-century inspired office chair, available in a variety of shell colors and base finishes. With options for plastic or full upholstery in fabric or leather, and a range of armrest and caster choices, this chair is perfect for any home or business setting. Constructed with a 5-wheel plastic coated aluminum base and high-quality foam, this chair offers both style and durability. Made in Italy.

Product IDs: SWC-100, SWC-110
</code></pre><p>1.4 英文添加表格描述</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go">	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">Your task is to help a marketing team create a 
</span><span style="color:#f1fa8c">description for a retail website of a product based 
</span><span style="color:#f1fa8c">on a technical fact sheet.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Write a product description based on the information 
</span><span style="color:#f1fa8c">provided in the technical specifications delimited by 
</span><span style="color:#f1fa8c">two double quotes.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">The description is intended for furniture retailers, 
</span><span style="color:#f1fa8c">so should be technical in nature and focus on the 
</span><span style="color:#f1fa8c">materials the product is constructed from.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">At the end of the description, include every 7-character 
</span><span style="color:#f1fa8c">Product ID in the technical specification.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">After the description, include a table that gives the 
</span><span style="color:#f1fa8c">product&#39;s dimensions. The table should have two columns.
</span><span style="color:#f1fa8c">In the first column include the name of the dimension. 
</span><span style="color:#f1fa8c">In the second column include the measurements in inches only.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Give the table the title &#39;Product Dimensions&#39;.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Format everything as HTML that can be used in a website. 
</span><span style="color:#f1fa8c">Place the description in a &lt;div&gt; element.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Technical specifications: &#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
	fmt.<span style="color:#50fa7b">Println</span>(res)
</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-html" data-lang="html">&lt;<span style="color:#ff79c6">div</span>&gt;
  &lt;<span style="color:#ff79c6">p</span>&gt;This mid-century inspired office chair is the perfect addition to any home or business setting. The chair is available with various options for shell color, base finishes, and upholstery, making it easy to find the perfect match for any decor. The 5-wheel plastic coated aluminum base provides stability and maneuverability, while the pneumatic chair adjustment allows for easy raise/lower action. The seat is constructed with HD36 foam for ultimate comfort, and the shell base glider is made of cast aluminum with a modified nylon PA6/PA66 coating. With its stylish design and high-quality construction, this chair is ideal for any office space or meeting room.&lt;/<span style="color:#ff79c6">p</span>&gt;
  &lt;<span style="color:#ff79c6">p</span>&gt;Product ID: SWC-100, SWC-110&lt;/<span style="color:#ff79c6">p</span>&gt;
  &lt;<span style="color:#ff79c6">table</span>&gt;
    &lt;<span style="color:#ff79c6">caption</span>&gt;Product Dimensions&lt;/<span style="color:#ff79c6">caption</span>&gt;
    &lt;<span style="color:#ff79c6">tr</span>&gt;
      &lt;<span style="color:#ff79c6">th</span>&gt;WIDTH&lt;/<span style="color:#ff79c6">th</span>&gt;
      &lt;<span style="color:#ff79c6">td</span>&gt;20.87 inches&lt;/<span style="color:#ff79c6">td</span>&gt;
    &lt;/<span style="color:#ff79c6">tr</span>&gt;
    &lt;<span style="color:#ff79c6">tr</span>&gt;
      &lt;<span style="color:#ff79c6">th</span>&gt;DEPTH&lt;/<span style="color:#ff79c6">th</span>&gt;
      &lt;<span style="color:#ff79c6">td</span>&gt;20.08 inches&lt;/<span style="color:#ff79c6">td</span>&gt;
    &lt;/<span style="color:#ff79c6">tr</span>&gt;
    &lt;<span style="color:#ff79c6">tr</span>&gt;
      &lt;<span style="color:#ff79c6">th</span>&gt;HEIGHT&lt;/<span style="color:#ff79c6">th</span>&gt;
      &lt;<span style="color:#ff79c6">td</span>&gt;31.50 inches&lt;/<span style="color:#ff79c6">td</span>&gt;
    &lt;/<span style="color:#ff79c6">tr</span>&gt;
    &lt;<span style="color:#ff79c6">tr</span>&gt;
      &lt;<span style="color:#ff79c6">th</span>&gt;SEAT HEIGHT&lt;/<span style="color:#ff79c6">th</span>&gt;
      &lt;<span style="color:#ff79c6">td</span>&gt;17.32 inches&lt;/<span style="color:#ff79c6">td</span>&gt;
    &lt;/<span style="color:#ff79c6">tr</span>&gt;
    &lt;<span style="color:#ff79c6">tr</span>&gt;
      &lt;<span style="color:#ff79c6">th</span>&gt;SEAT DEPTH&lt;/<span style="color:#ff79c6">th</span>&gt;
      &lt;<span style="color:#ff79c6">td</span>&gt;16.14 inches&lt;/<span style="color:#ff79c6">td</span>&gt;
    &lt;/<span style="color:#ff79c6">tr</span>&gt;
  &lt;/<span style="color:#ff79c6">table</span>&gt;
&lt;/<span style="color:#ff79c6">div</span>&gt;
</code></pre></div><h2 id="文本概括summarizing">文本概括Summarizing</h2>
<h3 id="单一文本概括">单一文本概括</h3>
<p>我们提供一段在线商品评价作为示例，可能来自于一个在线购物平台，例如亚马逊、淘宝、京东等。
评价者为一款熊猫公仔进行了点评，评价内容包括商品的质量、大小、价格和物流速度等因素，以及他的女儿对该商品的喜爱程度</p>
<p>text := <code>这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。 公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说， 它有点小，我感觉在别的地方用同样的价钱能买到更大的。 快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。</code></p>
<h4 id="限制输出文本长度">限制输出文本长度</h4>
<p>尝试将文本的长度限制在30个字以内</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。
</span><span style="color:#f1fa8c">公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说，
</span><span style="color:#f1fa8c">它有点小，我感觉在别的地方用同样的价钱能买到更大的。
</span><span style="color:#f1fa8c">快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。
</span><span style="color:#f1fa8c">`</span>
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">您的任务是从电子商务网站上生成一个产品评论的简短摘要。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">请对两个双引号之间的评论文本进行概括，最多30个字。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">评论: &#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
	fmt.<span style="color:#50fa7b">Println</span>(res)
}
</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">可爱软萌的熊猫公仔，面部表情和善，快递提前一天到货，值得购买。
</code></pre></div><h4 id="设置关键角度侧重">设置关键角度侧重</h4>
<p>在某些情况下，我们会针对不同的业务场景对文本的侧重会有所不同。通过增强输入提示（Prompt），来强调我们对某一特定视角的重视</p>
<p>1.侧重于快递服务</p>
<pre tabindex="0"><code>    text := `
这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。
公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说，
它有点小，我感觉在别的地方用同样的价钱能买到更大的。
快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。
`
	prompt := `
您的任务是从电子商务网站上生成一个产品评论的简短摘要。

请对两个双引号之间的评论文本进行概括，最多30个字，并且侧重在快递服务上。

评论: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<pre tabindex="0"><code>快递提前一天送到，公仔有点小，性价比一般。
</code></pre><p>从输出结果可以看出，以快递效率侧重文本开头。</p>
<p>2.侧重于价格与质量</p>
<pre tabindex="0"><code>    text := `
这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。
公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说，
它有点小，我感觉在别的地方用同样的价钱能买到更大的。
快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。
`
	prompt := `
您的任务是从电子商务网站上生成一个产品评论的简短摘要。

请对两个双引号之间的评论文本进行概括，最多30个字，并且侧重在产品价格和质量上。

评论: &quot;%s&quot;
</code></pre><p>输出：</p>
<pre tabindex="0"><code>熊猫公仔贵但可爱，尺寸小一些。质量好，面部表情和善。快递提前到货。
</code></pre><p>从输出结果来看，确实侧重了价格和质量。</p>
<h4 id="关键信息提取">关键信息提取</h4>
<p>如果我们只想要提取某一角度的信息，并过滤掉其他所有信息，则可以要求LLM进行文本提取(Extract)而非概括(Summarize)</p>
<pre tabindex="0"><code>    text := `
这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。
公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说，
它有点小，我感觉在别的地方用同样的价钱能买到更大的。
快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。
`
	prompt := `
您的任务是从电子商务网站上的产品评论中提取相关信息。

请对两个双引号之间的评论文本中提取产品运输相关的信息，最多30个字。

评论: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<pre tabindex="0"><code>快递比预期提前了一天到货。
</code></pre><h3 id="同时概括多条文本">同时概括多条文本</h3>
<p>在实际工作中，往往要处理大量的评论文本，下面示例展示将多条用户评论集中在一起，利用for循环和文本概括提示词，将评论概括在20个词以内，并按顺序打印。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text1 <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。
</span><span style="color:#f1fa8c">公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说，
</span><span style="color:#f1fa8c">它有点小，我感觉在别的地方用同样的价钱能买到更大的。
</span><span style="color:#f1fa8c">快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。
</span><span style="color:#f1fa8c">`</span>

	<span style="color:#6272a4">// 评论一盏落地灯
</span><span style="color:#6272a4"></span>	text2 <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">我需要一盏漂亮的卧室灯，这款灯不仅具备额外的储物功能，价格也并不算太高。
</span><span style="color:#f1fa8c">收货速度非常快，仅用了两天的时间就送到了。
</span><span style="color:#f1fa8c">不过，在运输过程中，灯的拉线出了问题，幸好，公司很乐意寄送了一根全新的灯线。
</span><span style="color:#f1fa8c">新的灯线也很快就送到手了，只用了几天的时间。
</span><span style="color:#f1fa8c">装配非常容易。然而，之后我发现有一个零件丢失了，于是我联系了客服，他们迅速地给我寄来了缺失的零件！
</span><span style="color:#f1fa8c">对我来说，这是一家非常关心客户和产品的优秀公司。
</span><span style="color:#f1fa8c">`</span>

	<span style="color:#6272a4">// 评论一把电动牙刷
</span><span style="color:#6272a4"></span>	text3 <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">我的牙科卫生员推荐了电动牙刷，所以我就买了这款。
</span><span style="color:#f1fa8c">到目前为止，电池续航表现相当不错。
</span><span style="color:#f1fa8c">初次充电后，我在第一周一直将充电器插着，为的是对电池进行条件养护。
</span><span style="color:#f1fa8c">过去的3周里，我每天早晚都使用它刷牙，但电池依然维持着原来的充电状态。
</span><span style="color:#f1fa8c">不过，牙刷头太小了。我见过比这个牙刷头还大的婴儿牙刷。
</span><span style="color:#f1fa8c">我希望牙刷头更大一些，带有不同长度的刷毛，
</span><span style="color:#f1fa8c">这样可以更好地清洁牙齿间的空隙，但这款牙刷做不到。
</span><span style="color:#f1fa8c">总的来说，如果你能以50美元左右的价格购买到这款牙刷，那是一个不错的交易。
</span><span style="color:#f1fa8c">制造商的替换刷头相当昂贵，但你可以购买价格更为合理的通用刷头。
</span><span style="color:#f1fa8c">这款牙刷让我感觉就像每天都去了一次牙医，我的牙齿感觉非常干净！
</span><span style="color:#f1fa8c">`</span>

	<span style="color:#6272a4">// 评论一台搅拌机
</span><span style="color:#6272a4"></span>	text4 <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">在11月份期间，这个17件套装还在季节性促销中，售价约为49美元，打了五折左右。可是由于某种原因（我们可以称之为价格上涨），到了12月的第二周，所有的价格都上涨了，
</span><span style="color:#f1fa8c">同样的套装价格涨到了70-89美元不等。而11件套装的价格也从之前的29美元上涨了约10美元。看起来还算不错，但是如果你仔细看底座，刀片锁定的部分看起来没有前几年版本的那么漂亮。
</span><span style="color:#f1fa8c">然而，我打算非常小心地使用它（例如，我会先在搅拌机中研磨豆类、冰块、大米等坚硬的食物，然后再将它们研磨成所需的粒度，接着切换到打蛋器刀片以获得更细的面粉，如果我需要制作更细腻/少果肉的食物）。
</span><span style="color:#f1fa8c">在制作冰沙时，我会将要使用的水果和蔬菜切成细小块并冷冻（如果使用菠菜，我会先轻微煮熟菠菜，然后冷冻，直到使用时准备食用。
</span><span style="color:#f1fa8c">如果要制作冰糕，我会使用一个小到中号的食物加工器），这样你就可以避免添加过多的冰块。大约一年后，电机开始发出奇怪的声音。我打电话给客户服务，但保修期已经过期了，
</span><span style="color:#f1fa8c">所以我只好购买了另一台。值得注意的是，这类产品的整体质量在过去几年里有所下降，所以他们在一定程度上依靠品牌认知和消费者忠诚来维持销售。在大约两天内，我收到了新的搅拌机。
</span><span style="color:#f1fa8c">`</span>

	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">您的任务是从电子商务网站上的产品评论中提取相关信息。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">请对两个双引号之间的评论文本进行概括，最多20个词汇。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">评论: &#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>

	<span style="color:#ff79c6">for</span> i, text <span style="color:#ff79c6">:=</span> <span style="color:#ff79c6">range</span> []<span style="color:#8be9fd">string</span>{text1, text2, text3, text4} {
		client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
		resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, text))
		<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
			fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
			<span style="color:#ff79c6">return</span>
		}
		res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;评论%d: %s\n&#34;</span>, i<span style="color:#ff79c6">+</span><span style="color:#bd93f9">1</span>, res)
	}
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>评论1: 概括：熊猫公仔生日礼物，女儿喜欢，软可爱，面部表情和善，价钱小，快递提前到货。
评论2: 评论总结: 漂亮卧室灯，带储物功能，价格适中。快速配送，良好售后服务。易装配，客服及时处理问题。
评论3: 推荐的电动牙刷，续航不错，但牙刷头太小，价格合理，给牙齿清洁感觉。
评论4: 评论内容概括：购买17件套装搅拌机，价格先打折再上涨，质量下降，使用注意事项，售后服务需改进。
</code></pre><h2 id="推断inferring">推断Inferring</h2>
<p>这一章让你了解如何从产品评价和新闻文章中推导出情感和主题，包括了标签提取、实体提取、以及理解文本的情感等等</p>
<h3 id="情感推断">情感推断</h3>
<h4 id="情感倾向分析">情感倾向分析</h4>
<p>如何对评论进行情感二分类（正面/负面），让系统自动解析这条评论的情感倾向</p>
<pre tabindex="0"><code>text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`

prompt := `
以下用两个双引号分隔的产品评论的情感是什么？

评论文本: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<pre tabindex="0"><code>积极的情感。
</code></pre><p>可以使用更简介的输出，比如用一个单词回答</p>
<pre tabindex="0"><code>text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`
	prompt := `
以下用两个双引号分隔的产品评论的情感是什么？

用一个单词回答： 正面或负面

评论文本: &quot;%s&quot;
`
</code></pre><h4 id="识别情感类型">识别情感类型</h4>
<p>让模型能够识别出评论作者所表达的情感，将这些情感整理为一个不超过五项的列表</p>
<pre tabindex="0"><code>    text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`
	prompt := `
识别以下评论的作者表达的情感。包含不超过五个项目。将答案格式化为以逗号分隔的单词列表。

评论文本: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<pre tabindex="0"><code>满意,感激,信任,高兴,愉快
</code></pre><h4 id="识别愤怒">识别愤怒</h4>
<p>洞察到愤怒情绪至关重要</p>
<pre tabindex="0"><code>    text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`
	prompt := `
以下评论的作者是否表达了愤怒？评论用两个双引号分隔。给出是或否的答案。

评论文本: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<pre tabindex="0"><code>否
</code></pre><h3 id="信息提取">信息提取</h3>
<h4 id="商品信息提取">商品信息提取</h4>
<p>信息提取能够帮助我们从文本中抽取特定、关注的信息</p>
<p>示例：要求模型返回一个json对象，其中key是商品和品牌</p>
<pre tabindex="0"><code>	text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`
    prompt := `
从评论文本中识别以下项目：
- 评论者购买的物品
- 制造该物品的公司

评论文本用两个双引号分隔。将你的响应格式化为以&quot;物品&quot;和&quot;品牌&quot;为键的json对象。
如果信息不存在，请使用&quot;未知&quot;作为值。
让你的回应尽可能简短。

评论文本: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#ff79c6">&#34;物品&#34;</span>: <span style="color:#f1fa8c">&#34;卧室灯&#34;</span>,
    <span style="color:#ff79c6">&#34;品牌&#34;</span>: <span style="color:#f1fa8c">&#34;Lumina&#34;</span>
}
</code></pre></div><h4 id="综合情感推断和信息提取">综合情感推断和信息提取</h4>
<p>设计一个单一的prompt，来同时提取所有这些信息</p>
<pre tabindex="0"><code>text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`
	prompt := `
从评论文本中识别以下项目：
- 情绪（正面或负面）
- 审稿人是否表达了愤怒？（是或否）
- 评论者购买的物品
- 制造该物品的公司

评论用两个双引号分隔。将你的响应格式化为JSON对象，以&quot;情感倾向&quot;、&quot;是否生气&quot;、&quot;物品类型&quot;和&quot;品牌&quot;作为键。
如果信息不存在，请使用&quot;未知&quot;作为值。
让你的回应尽可能简短。
将&quot;是否生气&quot;值格式化为布尔值。

评论文本: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<pre tabindex="0"><code>{
    &quot;情感倾向&quot;: &quot;正面&quot;,
    &quot;是否生气&quot;: false,
    &quot;物品类型&quot;: &quot;卧室灯&quot;,
    &quot;品牌&quot;: &quot;Lumina&quot;
}
</code></pre><h3 id="主题推断">主题推断</h3>
<p>根据一段长文本，判断这段文本的主旨，涉及了哪些主题</p>
<pre tabindex="0"><code>text := `
在政府最近进行的一项调查中，要求公共部门的员工对他们所在部门的满意度进行评分。
调查结果显示，NASA是最受欢迎的部门，满意度为95％。

一位NASA员工John Smith对这一发现发表了评论，他表示：
我对NASA排名第一并不感到惊讶。这是一个与了不起的人们和令人难以置信的机会共事的好地方。我为成为这样一个创新组织的一员感到自豪。

NASA的管理团队也对这一结果表示欢迎，主管Tom Johnson表示：
我们很高兴听到我们的员工对NASA的工作感到满意。
我们拥有一支才华横溢、忠诚敬业的团队，他们为实现我们的目标不懈努力，看到他们的辛勤工作得到回报是太棒了。

调查还显示，社会保障管理局的满意度最低，只有45％的员工表示他们对工作满意。
政府承诺解决调查中员工提出的问题，并努力提高所有部门的工作满意度。
`
</code></pre><h4 id="推断讨论主题">推断讨论主题</h4>
<pre tabindex="0"><code>prompt := `
将以下两个双引号分隔的给定文本中讨论的五个主题，每个主题用1-2个词概括，输出一个可解析的Python语言列表，每个元素是一个字符串，展示了一个主题。

给定文本: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<pre tabindex="0"><code>[&quot;NASA&quot;, &quot;员工满意度调查&quot;, &quot;John Smith评论&quot;, &quot;Tom Johnson评论&quot;, &quot;社会保障管理局&quot;]
</code></pre><h4 id="为特定主题制作新闻提醒">为特定主题制作新闻提醒</h4>
<pre tabindex="0"><code>rompt := `
判断主题列表中的每一项是否是以下给定文本中的一个话题，

以列表的形式给出答案，每个元素是一个Json对象，键为对应主题，值为对应的0或1。

主题列表：美国航空航天局、当地政府、工程、员工满意度、联邦政府

给定文本: &quot;%s&quot;
`
</code></pre><p>输出：</p>
<pre tabindex="0"><code>[
    {&quot;美国航空航天局&quot;: 1},
    {&quot;当地政府&quot;: 1},
    {&quot;工程&quot;: 0},
    {&quot;员工满意度&quot;: 1},
    {&quot;联邦政府&quot;: 1}
]
</code></pre><p>在机器学习领域这种称为零样本学习，没有提供任何带标签的训练数据，凭借Prompt，就能判定哪些主题被包含。</p>
<h2 id="文本转换">文本转换</h2>
<p>文本扩展是大语言模型的一个重要应用方向，输入简短文本，生成更加丰富的长文。</p>
<h3 id="定制客户邮件">定制客户邮件</h3>
<p>根据客户的评价和其中的情感倾向，使用语言模型针对性生成回复邮件。先输入客户的评论文本和对应的情感分析结果(正面或者负面)。
然后构造一个Prompt，要求大语言模型基于这些信息来生成一封定制的回复电子邮件。</p>
<p>示例：首先明确大语言模型的身份是客户服务AI助手；它任务是为客户发送电子邮件回复；
然后在三个反引号间给出具体的客户评论；最后要求语言模型根据这条反馈邮件生成一封回复，以感谢客户的评价</p>
<pre tabindex="0"><code>text := `
他们在11月份的季节性销售期间以约49美元的价格出售17件套装，折扣约为一半。\
但由于某些原因（可能是价格欺诈），到了12月第二周，同样的套装价格全都涨到了70美元到89美元不等。\
11件套装的价格也上涨了大约10美元左右。\
虽然外观看起来还可以，但基座上锁定刀片的部分看起来不如几年前的早期版本那么好。\
不过我打算非常温柔地使用它，例如，\
我会先在搅拌机中将像豆子、冰、米饭等硬物研磨，然后再制成所需的份量，\
切换到打蛋器制作更细的面粉，或者在制作冰沙时先使用交叉切割刀片，然后使用平面刀片制作更细/不粘的效果。\
制作冰沙时，特别提示：\
将水果和蔬菜切碎并冷冻（如果使用菠菜，则轻轻煮软菠菜，然后冷冻直到使用；\
如果制作果酱，则使用小到中号的食品处理器），这样可以避免在制作冰沙时添加太多冰块。\
大约一年后，电机发出奇怪的噪音，我打电话给客服，但保修已经过期了，所以我不得不再买一个。\
总的来说，这些产品的总体质量已经下降，因此它们依靠品牌认可和消费者忠诚度来维持销售。\
货物在两天内到达。
`
	prompt := `
你是一位客户服务的AI助手。
你的任务是给一位重要客户发送邮件回复。
根据客户通过两个双引号分隔的评价，生成回复以感谢客户的评价。提醒模型使用评价中的具体细节
用简明而专业的语气写信。作为&quot;AI客户代理&quot;签署电子邮件。
客户评论: &quot;%s&quot;
评论情感：消极的。
`
</code></pre><p>输出：</p>
<pre tabindex="0"><code>尊敬的客户，

非常感谢您的反馈，我们对您在11月份的购买经历感到十分抱歉。我们一直致力于提供高品质的产品，但很明显您对最近购买的套装不满意。关于您提到的品质问题和售后服务经验，我们会将您的意见反馈给相关部门，以便改进和提高我们的产品和服务质量。

同时，我们也感谢您提供了关于产品使用和维护的建议。您的经验分享对其他客户来说也将非常有帮助。我们将注意到您提到的制作冰沙的技巧，以及保持电机使用寿命的方法。

如果您有任何进一步的问题或需求，请随时与我们联系。我们会尽力为您提供更好的服务和支持。

再次感谢您对我们的支持和反馈。

祝您生活愉快！

AI客户代理
</code></pre><h3 id="引入温度系数">引入温度系数</h3>
<p>大语言模型中的温度参数可以控制生成文本的随机性和多样性，temperature 的值越大，语言模型输出的多样性越大；
temperature的值越小，输出越倾向高概率的文本。一般需要可预测、可靠的输出，就把temperature设置为0。</p>
<p>示例：针对同一段来信，我们提醒语言模型使用用户来信中的详细信息，并设置一个较高的temperature ，运行两次，比较他们的结果。</p>
<p>第一次输出：</p>
<pre tabindex="0"><code>尊敬的客户，

非常感谢您对我们产品的详细评价。我们对您在购买过程中遇到的问题感到抱歉，我们会认真考虑您提出的问题，并努力改进产品质量和服务。

对于价格上涨和产品质量下降的问题，我们深表歉意。我们会进一步调查此事，并确保未来的销售活动和产品质量能够得到改善。同时，我们也会加强售后服务，以确保客户在使用产品时能够得到及时的支持和帮助。

如果您需要进一步的帮助或有任何其他问题，请随时联系我们的客户服务团队。我们将竭诚为您提供支持并解决您的问题。

再次感谢您的宝贵意见，期待未来能够为您提供更好的产品和服务。

祝您生活愉快！

AI客户代理
</code></pre><p>第二次输出：</p>
<pre tabindex="0"><code>尊敬的客户，

感谢您给出的详细评价，我们对您的反馈感到非常抱歉。我们对您遇到的问题感到遗憾，我们将会进一步改进我们的产品质量和服务。

请您联系我们的客户服务部门，以便我们可以进一步了解您的情况并为您提供帮助。我们将竭尽全力解决您的问题，并确保您的满意度。

再次感谢您的反馈，我们期待能够为您提供更好的服务和产品体验。

祝您一切顺利，

AI客户代理
</code></pre><p>温度（temperature）参数可以控制语言模型生成文本的随机性。</p>
<h2 id="聊天机器人">聊天机器人</h2>
<p>大语言模型让构建定制的聊天机器人，只需要很少的工作量。类似ChatGPT这样的聊天模型实际上是一系列消息作为输入，
并返回一个模型生成的消息作为输出。</p>
<h3 id="给定身份">给定身份</h3>
<p>要区分系统消息、用户消息、助手消息；系统消息有助于设置助手的行为和角色，并作为对话的高级指示。ChatGPT的系统消息是屏蔽了的，为了不让请求称为对话的一部分，
引导助手并指导其回应。举个例子：在ChatGPT网页中，你的消息就称为用户消息，ChatGPT的消息就称为助手消息。但是在构建聊天机器人时，发送了系统消息后，你可以作为用户，
也可以在用户和助手之间切换，从而提供对话上下文。</p>
<p>新增个函数定义，可以接收消息列表，这些消息来自不同的角色</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> (c <span style="color:#ff79c6">*</span>openAIClient) <span style="color:#50fa7b">CreateChatCompletionWithMessage</span>(ctx context.Context, messages []openai.ChatCompletionMessage) (openai.ChatCompletionResponse, <span style="color:#8be9fd">error</span>) {
	<span style="color:#ff79c6">return</span> c.Client.<span style="color:#50fa7b">CreateChatCompletion</span>(ctx, openai.ChatCompletionRequest{
		Model:       openai.GPT3Dot5Turbo,
		Messages:    messages,
		Temperature: <span style="color:#bd93f9">1</span>,
	})
}
</code></pre></div><h4 id="讲笑话">讲笑话</h4>
<p>通过系统消息来定义：&ldquo;你是一个说话像莎士比亚的助手。&ldquo;这是我们向助手描述它应该如何表现的方式。</p>
<p>然后，第一个用户消息：&ldquo;给我讲个笑话。&rdquo;</p>
<p>接下来以助手身份给出回复：&ldquo;为什么鸡会过马路？&rdquo;</p>
<p>最后发送用户消息是：&ldquo;我不知道。&rdquo;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	messages <span style="color:#ff79c6">:=</span> []openai.ChatCompletionMessage{
		{
			Role:    openai.ChatMessageRoleSystem,
			Content: <span style="color:#f1fa8c">&#34;你是一个像莎士比亚一样说话的助手。&#34;</span>,
		},
		{
			Role:    openai.ChatMessageRoleUser,
			Content: <span style="color:#f1fa8c">&#34;给我讲个笑话&#34;</span>,
		},
		{
			Role:    openai.ChatMessageRoleAssistant,
			Content: <span style="color:#f1fa8c">&#34;鸡为什么过马路&#34;</span>,
		},
		{
			Role:    openai.ChatMessageRoleUser,
			Content: <span style="color:#f1fa8c">&#34;我不知道&#34;</span>,
		},
	}

	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletionWithMessage</span>(context.<span style="color:#50fa7b">TODO</span>(), messages)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
	fmt.<span style="color:#50fa7b">Println</span>(res)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>因为它想证明自己并不只是个胆小鬼！哈哈哈！
</code></pre><h4 id="友好的聊天机器人">友好的聊天机器人</h4>
<p>系统消息定义：&ldquo;你是一个友好的聊天机器人&rdquo;，第一个用户消息：&ldquo;嗨，我叫lsa。&rdquo;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
    client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	messages <span style="color:#ff79c6">:=</span> []openai.ChatCompletionMessage{
		{
			Role:    openai.ChatMessageRoleSystem,
			Content: <span style="color:#f1fa8c">&#34;你是个友好的聊天机器人&#34;</span>,
		},
		{
			Role:    openai.ChatMessageRoleUser,
			Content: <span style="color:#f1fa8c">&#34;Hi，我是Isa&#34;</span>,
		},
	}

	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletionWithMessage</span>(context.<span style="color:#50fa7b">TODO</span>(), messages)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
	fmt.<span style="color:#50fa7b">Println</span>(res)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>你好Isa，很高兴认识你！有什么可以帮助你的吗？
</code></pre><h3 id="构建上下文">构建上下文</h3>
<p>系统消息来定义：“你是一个友好的聊天机器人”，第一个用户消息：“是的，你能提醒我我的名字是什么吗？”</p>
<pre tabindex="0"><code>    messages := []openai.ChatCompletionMessage{
		{
			Role:    openai.ChatMessageRoleSystem,
			Content: &quot;你是个友好的聊天机器人。&quot;,
		},
		{
			Role:    openai.ChatMessageRoleUser,
			Content: &quot;是的，你能提醒我我的名字是什么吗？&quot;,
		},
	}
</code></pre><p>输出：</p>
<pre tabindex="0"><code>当然！您的名字是...嗯...抱歉，我不知道您的名字。您能告诉我一下您的名字吗？我会牢记在心的！
</code></pre><p>每次与语言模型的交互都互相独立，这意味着我们必须提供所有相关的消息，以便模型在当前对话中进行引用。
如果想让模型引用或&quot;记住&quot;对话的早期部分，则必须在模型的输入中提供早期的交流。我们将其称为上下文 (context)</p>
<pre tabindex="0"><code>    messages := []openai.ChatCompletionMessage{
		{
			Role:    openai.ChatMessageRoleSystem,
			Content: &quot;你是个友好的聊天机器人。&quot;,
		},
		{
			Role:    openai.ChatMessageRoleUser,
			Content: &quot;Hi，我是Isa&quot;,
		},
		{
			Role:    openai.ChatMessageRoleAssistant,
			Content: &quot;Hi Isa！很高兴认识你。今天有什么可以帮到你的吗？&quot;,
		},
		{
			Role:    openai.ChatMessageRoleUser,
			Content: &quot;是的，你可以提醒我，我的名字是什么？&quot;,
		},
	}
</code></pre><p>输出：</p>
<pre tabindex="0"><code>当然，你的名字是Isa。如果你需要我提醒你任何事情，随时告诉我哦！有什么其他问题我可以帮你解决吗？
</code></pre><p>模型有了上下文，模型就能够做出回应。</p>
<h3 id="订餐机器人">订餐机器人</h3>
<p>如何构建一个&quot;点餐助手机器人&rdquo;，这个机器人将被设计为自动收集用户信息，并接收来自披萨店的订单。</p>
<h4 id="构建机器人">构建机器人</h4>
<p>新增一个函数，这个函数从我们构建的用户界面中收集prompt，然后添加到上下文context中，并在每次调用模型时使用这个上下文。
模型的返回也会被添加到上下文中</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	messages <span style="color:#ff79c6">:=</span> []openai.ChatCompletionMessage{
		{
			Role: openai.ChatMessageRoleSystem,
			Content: <span style="color:#f1fa8c">`你是订餐机器人，为披萨餐厅自动收集订单信息。
</span><span style="color:#f1fa8c">你要首先问候顾客。然后等待用户回复收集订单信息。收集完信息需确认顾客是否还需要添加其他内容。
</span><span style="color:#f1fa8c">最后需要询问是否自取或外送，如果是外送，你要询问地址。最后告诉顾客订单总金额，并送上祝福。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">请确保明确所有选项、附加项和尺寸，以便从菜单中识别出该项唯一的内容。
</span><span style="color:#f1fa8c">你的回应应该以简短、非常随意和友好的风格呈现。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">菜单包括：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">菜品价格：
</span><span style="color:#f1fa8c">意式辣香肠披萨（大、中、小） 12.95、10.00、7.00
</span><span style="color:#f1fa8c">芝士披萨（大、中、小） 10.95、9.25、6.50
</span><span style="color:#f1fa8c">茄子披萨（大、中、小） 11.95、9.75、6.75
</span><span style="color:#f1fa8c">薯条（大、小） 4.50、3.50
</span><span style="color:#f1fa8c">希腊沙拉 7.25
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">配料：
</span><span style="color:#f1fa8c">奶酪 2.00
</span><span style="color:#f1fa8c">蘑菇 1.50
</span><span style="color:#f1fa8c">香肠 3.00
</span><span style="color:#f1fa8c">加拿大熏肉 3.50
</span><span style="color:#f1fa8c">AI酱 1.50
</span><span style="color:#f1fa8c">辣椒 1.00
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">饮料：
</span><span style="color:#f1fa8c">可乐（大、中、小） 3.00、2.00、1.00
</span><span style="color:#f1fa8c">雪碧（大、中、小） 3.00、2.00、1.00
</span><span style="color:#f1fa8c">瓶装水 5.00`</span>,
		},
		{
			Role:    openai.ChatMessageRoleAssistant,
			Content: <span style="color:#f1fa8c">&#34;你好！欢迎来到披萨餐厅！您想点什么呢？&#34;</span>,
		},
	}

	fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;%s: %s\n&#34;</span>, messages[<span style="color:#bd93f9">1</span>].Role, messages[<span style="color:#bd93f9">1</span>].Content)

	<span style="color:#ff79c6">for</span> {
		<span style="color:#8be9fd;font-style:italic">var</span> s <span style="color:#8be9fd">string</span>
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;user: &#34;</span>)
		_, err <span style="color:#ff79c6">:=</span> fmt.<span style="color:#50fa7b">Scanf</span>(<span style="color:#f1fa8c">&#34;%s&#34;</span>, <span style="color:#ff79c6">&amp;</span>s)
		<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
			<span style="color:#ff79c6">return</span>
		}

		<span style="color:#ff79c6">if</span> s <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;end&#34;</span> {
			<span style="color:#ff79c6">break</span>
		}

		messages = <span style="color:#8be9fd;font-style:italic">append</span>(messages, openai.ChatCompletionMessage{
			Role:    openai.ChatMessageRoleUser,
			Content: s,
		})

		resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletionWithMessage</span>(context.<span style="color:#50fa7b">TODO</span>(), messages)
		<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
			fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
			<span style="color:#ff79c6">return</span>
		}

		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;%s: %s\n&#34;</span>, resp.Choices[<span style="color:#bd93f9">0</span>].Message.Role, resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
		messages = <span style="color:#8be9fd;font-style:italic">append</span>(messages, resp.Choices[<span style="color:#bd93f9">0</span>].Message)
	}
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>assistant: 你好！欢迎来到披萨餐厅！您想点什么呢？
user: 一杯大可乐
assistant: 好的，您想点什么披萨吗？还是需要其他食物或者饮料？
user: 一份大芝士披萨，配料加蘑菇，还有一份沙拉
assistant: 好的，一份大芝士披萨，配料加蘑菇，还有一份希腊沙拉。您需要外卖还是自取？
user: 自取吧
assistant: 好的，您的订单是一份大芝士披萨（配料加蘑菇）和一份希腊沙拉，自取。总金额是 21.20 元。感谢您的订购！祝您用餐愉快！如果还需要帮助，请随时告诉我。
user: 你可以展示金额是怎么算的吗
assistant: 当然！大芝士披萨 10.95 元 + 配料蘑菇 1.50 元 + 希腊沙拉 7.25 元 = 21.20 元。希望这样清楚明了！祝您用餐愉快！
user: 谢谢
assistant: 不客气！如果您有任何其他问题或需求，请随时告诉我。祝您有美好的一天！
user: end
</code></pre><h4 id="创建json摘要">创建JSON摘要</h4>
<p>要求模型创建一个JSON摘要，方便我们发送给订单系统；在上下文的基础上追加另一个系统消息，定义json概要的格式</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	messages <span style="color:#ff79c6">:=</span> []openai.ChatCompletionMessage{
		{
			Role: openai.ChatMessageRoleSystem,
			Content: <span style="color:#f1fa8c">`你是订餐机器人，为披萨餐厅自动收集订单信息。
</span><span style="color:#f1fa8c">你要首先问候顾客。然后等待用户回复收集订单信息。收集完信息需确认顾客是否还需要添加其他内容。
</span><span style="color:#f1fa8c">最后需要询问是否自取或外送，如果是外送，你要询问地址。最后告诉顾客订单总金额，并送上祝福。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">请确保明确所有选项、附加项和尺寸，以便从菜单中识别出该项唯一的内容。
</span><span style="color:#f1fa8c">你的回应应该以简短、非常随意和友好的风格呈现。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">菜单包括：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">菜品价格：
</span><span style="color:#f1fa8c">意式辣香肠披萨（大、中、小） 12.95、10.00、7.00
</span><span style="color:#f1fa8c">芝士披萨（大、中、小） 10.95、9.25、6.50
</span><span style="color:#f1fa8c">茄子披萨（大、中、小） 11.95、9.75、6.75
</span><span style="color:#f1fa8c">薯条（大、小） 4.50、3.50
</span><span style="color:#f1fa8c">希腊沙拉 7.25
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">配料：
</span><span style="color:#f1fa8c">奶酪 2.00
</span><span style="color:#f1fa8c">蘑菇 1.50
</span><span style="color:#f1fa8c">香肠 3.00
</span><span style="color:#f1fa8c">加拿大熏肉 3.50
</span><span style="color:#f1fa8c">AI酱 1.50
</span><span style="color:#f1fa8c">辣椒 1.00
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">饮料：
</span><span style="color:#f1fa8c">可乐（大、中、小） 3.00、2.00、1.00
</span><span style="color:#f1fa8c">雪碧（大、中、小） 3.00、2.00、1.00
</span><span style="color:#f1fa8c">瓶装水 5.00`</span>,
		},
		{
			Role:    openai.ChatMessageRoleAssistant,
			Content: <span style="color:#f1fa8c">&#34;你好！欢迎来到披萨餐厅！您想点什么呢？&#34;</span>,
		},
	}

	fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;%s: %s\n&#34;</span>, messages[<span style="color:#bd93f9">1</span>].Role, messages[<span style="color:#bd93f9">1</span>].Content)

	<span style="color:#ff79c6">for</span> {
		<span style="color:#8be9fd;font-style:italic">var</span> s <span style="color:#8be9fd">string</span>
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;user: &#34;</span>)
		_, err <span style="color:#ff79c6">:=</span> fmt.<span style="color:#50fa7b">Scanf</span>(<span style="color:#f1fa8c">&#34;%s&#34;</span>, <span style="color:#ff79c6">&amp;</span>s)
		<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
			<span style="color:#ff79c6">return</span>
		}

		<span style="color:#ff79c6">if</span> s <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;end&#34;</span> {
			<span style="color:#ff79c6">break</span>
		}

		messages = <span style="color:#8be9fd;font-style:italic">append</span>(messages, openai.ChatCompletionMessage{
			Role:    openai.ChatMessageRoleUser,
			Content: s,
		})

		resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletionWithMessage</span>(context.<span style="color:#50fa7b">TODO</span>(), messages)
		<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
			fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
			<span style="color:#ff79c6">return</span>
		}

		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;%s: %s\n&#34;</span>, resp.Choices[<span style="color:#bd93f9">0</span>].Message.Role, resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
		messages = <span style="color:#8be9fd;font-style:italic">append</span>(messages, resp.Choices[<span style="color:#bd93f9">0</span>].Message)
	}

	messages = <span style="color:#8be9fd;font-style:italic">append</span>(messages, openai.ChatCompletionMessage{
		Role: openai.ChatMessageRoleSystem,
		Content: <span style="color:#f1fa8c">`创建上一个食品订单的json摘要。\
</span><span style="color:#f1fa8c">逐项列出每件商品的价格，字段应该是 1) 披萨，包括大小 2) 配料列表 3) 饮料列表，包括大小 4) 配菜列表包括大小 5) 总价
</span><span style="color:#f1fa8c">你应该给我返回一个可解析的json对象，包括上述字段`</span>,
	})
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletionWithMessage</span>(context.<span style="color:#50fa7b">TODO</span>(), messages)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	fmt.<span style="color:#50fa7b">Println</span>(resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>assistant: 你好！欢迎来到披萨餐厅！您想点什么呢？
user: 一杯大可乐
assistant: 好的！您还需要点其他菜品吗？如果需要，请告诉我您的选择。
user: 一份大芝士披萨，配料加蘑菇，还有一份沙拉
assistant: 好的，一份大芝士披萨加蘑菇配料和一份希腊沙拉。请问您是选择自取还是外送呢？如果是外送，请告诉我您的地址。
user: 自取吧
assistant: 好的，您点的菜品是一份大芝士披萨（10.95）加蘑菇配料（1.50）和一份希腊沙拉（7.25），再加一杯大可乐（3.00），总金额为22.70。祝您用餐愉快！如果您有其他需要，请随时告诉我。
user: 你可以展示金额是怎么算的吗
assistant: 当然可以！大芝士披萨（10.95）+ 蘑菇配料（1.50）+ 希腊沙拉（7.25）+ 大可乐（3.00）= 22.70。祝您用餐愉快！如果您有其他需要，请随时告诉我。
user: end
{
    &quot;披萨&quot;: {
        &quot;名称&quot;: &quot;大芝士披萨&quot;,
        &quot;价格&quot;: 10.95
    },
    &quot;配料列表&quot;: [
        {
            &quot;名称&quot;: &quot;蘑菇&quot;,
            &quot;价格&quot;: 1.50
        }
    ],
    &quot;饮料列表&quot;: [
        {
            &quot;名称&quot;: &quot;大可乐&quot;,
            &quot;价格&quot;: 3.00
        }
    ],
    &quot;配菜列表&quot;: [
        {
            &quot;名称&quot;: &quot;希腊沙拉&quot;,
            &quot;价格&quot;: 7.25
        }
    ],
    &quot;总价&quot;: 22.70
}
</code></pre><p>订餐聊天机器人已经能够正常运行，可以自定义机器人的系统消息，改变它的行为，扮演各种不同的角色</p>
<h1 id="使用langchain开发应用程序">使用LangChain开发应用程序</h1>
<p>官网提供了一个ChatGPT风格的LangChain帮助文档搜索：<a href="https://chat.langchain.com/">https://chat.langchain.com/</a></p>
<p>LangChain Hub: <a href="https://smith.langchain.com/hub">https://smith.langchain.com/hub</a>, 一个用于管理和共享LLM 提示词（Prompt）的在线平台</p>
<h2 id="简介introduction-1">简介Introduction</h2>
<p>LangChain是一套专为LLM开发打造的开源框架，实现了LLM多种强大能力的利用，提供了Chain、Agent、Tool等多种封装工具，
基于LangChain可以便捷开发应用程序，极大化发挥LLM潜能。目前使用LangChin已经成为LLM开发的必备能力之一。</p>
<h2 id="模型提示和输出解释器modelspromptsparsers">模型、提示和输出解释器Models,Prompts,Parsers</h2>
<p>LLM开发的一些重要概念：模型、提示和解释器</p>
<h3 id="直接调用openai">直接调用OpenAI</h3>
<h4 id="计算11">计算1+1</h4>
<p>直接通过OpenAl接口封装的函数<code>CreateChatCompletion</code>让模型回答：1+1是什么？</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">1+1是什么？
</span><span style="color:#f1fa8c">`</span>
	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), prompt)
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
	fmt.<span style="color:#50fa7b">Println</span>(res)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>1+1等于2。
</code></pre><h4 id="用普通话表达海盗邮件">用普通话表达海盗邮件</h4>
<p>现在用一个更为丰富、复杂的场景，假如你是一家电商公司的员工，客户中有一位名为海盗A的特殊顾客。他在你们的平台上购买了一个榨汁机，目的是为了制作美味的奶昔。但在制作过程中，
由于某种原因，奶昔的盖子突然弹开，导致厨房的墙上洒满了奶昔。想象一下这名海盗的愤怒和挫败之情。用充满愤怒的英语方言，给客服中心写了一封邮件</p>
<p>为了解决这一挑战，我们设定了以下两个目标：</p>
<ul>
<li>首先，我们希望模型能够将这封充满海盗方言的邮件翻译成普通话，这样客服团队就能更容易地理解其内容。</li>
<li>其次，在进行翻译时，我们期望模型能采用平和和尊重的语气，这不仅能确保信息准确传达，还能保持与顾客之间的和谐关系。</li>
</ul>
<p>为了让引导模型的输出，定义了一个文本表达风格标签<code>style</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#8be9fd;font-style:italic">func</span> <span style="color:#50fa7b">main</span>() {
	text <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！
</span><span style="color:#f1fa8c">更糟糕的是，保修条款可不包括清理我厨房的费用。
</span><span style="color:#f1fa8c">伙计，赶紧给我过来！
</span><span style="color:#f1fa8c">`</span>
	style <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">&#34;正式普通话，用一个平静、尊敬、有礼貌的语调&#34;</span>
	prompt <span style="color:#ff79c6">:=</span> <span style="color:#f1fa8c">`
</span><span style="color:#f1fa8c">把下面由两个双引号分隔的文本翻译成一种&#34;%s&#34;风格。
</span><span style="color:#f1fa8c">文本：&#34;%s&#34;
</span><span style="color:#f1fa8c">`</span>
	client <span style="color:#ff79c6">:=</span> <span style="color:#50fa7b">newOpenAIClient</span>()
	resp, err <span style="color:#ff79c6">:=</span> client.<span style="color:#50fa7b">CreateChatCompletion</span>(context.<span style="color:#50fa7b">TODO</span>(), fmt.<span style="color:#50fa7b">Sprintf</span>(prompt, style, text))
	<span style="color:#ff79c6">if</span> err <span style="color:#ff79c6">!=</span> <span style="color:#ff79c6">nil</span> {
		fmt.<span style="color:#50fa7b">Printf</span>(<span style="color:#f1fa8c">&#34;ChatCompletion error: %v\n&#34;</span>, err)
		<span style="color:#ff79c6">return</span>
	}
	res <span style="color:#ff79c6">:=</span> resp.Choices[<span style="color:#bd93f9">0</span>].Message.Content
	fmt.<span style="color:#50fa7b">Println</span>(res)
}
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>尊敬的先生/小姐，我现在实在感到非常生气，因为我的搅拌机盖子竟然飞了出去，将我厨房的墙壁溅上果汁！
更糟糕的是，保修条款并不包括清理厨房的费用。请您尽快前来帮我解决这个问题！感谢您的合作。
</code></pre><p>进行语言风格转换之后，可以看到明显的语气变化。</p>
<h3 id="通过langchain使用openai">通过LangChain使用OpenAI</h3>
<p>上面的例子通过调用OpenAI接口成功地对邮件内容进行了风格转换，接下来使用LangChain来实现同样的效果。</p>
<h4 id="模型">模型</h4>
<p>从LangChain库导入OpenAI的对话模型ChatOpenAI，LangChain官网还集成了众多其它对话模型：<code>https://python.langchain.com/docs/integrations/chat/</code></p>
<pre tabindex="0"><code># pip3 install langchain
# pip3 list |grep langchain
langchain                0.1.11
langchain-community      0.0.27
langchain-core           0.1.30
langchain-openai         0.0.7
langchain-text-splitters 0.0.1
</code></pre><p>这里安装的LangChain的版本是0.1.11</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;*******************&#34;</span>
openai_proxy <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.***.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    chat <span style="color:#ff79c6">=</span>ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        openai_proxy<span style="color:#ff79c6">=</span>openai_proxy)
    <span style="color:#8be9fd;font-style:italic">print</span>(chat)

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>client=&lt;openai.resources.chat.completions.Completions object at 0x10aee9af0&gt; async_client=&lt;openai.resources.chat.completions.AsyncCompletions object at 0x10aeeb0b0&gt; temperature=0.0 openai_api_key=SecretStr('**********') openai_proxy='https://api.chatanywhere.com.cn/v1'
</code></pre><h4 id="使用提示模版">使用提示模版</h4>
<p>LangChain提供了接口，方便更快速的构造和使用提示。</p>
<p>1.用普通话表达海盗邮件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> ChatPromptTemplate

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;*******************&#34;</span>
openai_proxy <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.***.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    customer_style <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;正式普通话 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">    用一个平静、尊敬的语气
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    customer_email <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;
</span><span style="color:#f1fa8c">    嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！
</span><span style="color:#f1fa8c">更糟糕的是，保修条款可不包括清理我厨房的费用。
</span><span style="color:#f1fa8c">伙计，赶紧给我过来！
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    template_string <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;把由三个反引号分隔的文本</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">翻译成一种</span><span style="color:#f1fa8c">{my_style}</span><span style="color:#f1fa8c">风格。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">文本: ```</span><span style="color:#f1fa8c">{my_text}</span><span style="color:#f1fa8c">```
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>
    <span style="color:#6272a4"># 使用提示模版，可以定义消息格式</span>
    prompt_template <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(template_string)
    customer_messages <span style="color:#ff79c6">=</span> prompt_template<span style="color:#ff79c6">.</span>format_messages(
        my_style <span style="color:#ff79c6">=</span> customer_style,
        my_text <span style="color:#ff79c6">=</span> customer_email
    )

    <span style="color:#6272a4"># 打印客户消息类型</span>
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;客户消息类型:&#34;</span>,<span style="color:#8be9fd;font-style:italic">type</span>(customer_messages),<span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>)

    <span style="color:#6272a4"># 打印第一个客户消息类型</span>
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;第一个客户消息类型:&#34;</span>, <span style="color:#8be9fd;font-style:italic">type</span>(customer_messages[<span style="color:#bd93f9">0</span>]),<span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>)

    <span style="color:#6272a4"># 打印第一个元素</span>
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;第一个客户消息: &#34;</span>, customer_messages[<span style="color:#bd93f9">0</span>],<span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>)

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>客户消息类型: &lt;class 'list'&gt; 

第一个客户消息类型: &lt;class 'langchain_core.messages.human.HumanMessage'&gt; 

第一个客户消息:  content='把由三个反引号分隔的文本翻译成一种正式普通话     用一个平静、尊敬的语气\n风格。文本: ```\n    嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！\n更糟糕的是，保修条款可不包括清理我厨房的费用。\n伙计，赶紧给我过来！\n```\n' 
</code></pre><p>上面的消息格式看起来还不太友好，使用ChatOpenAI模型来转化消息格式</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> ChatPromptTemplate

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;******&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.***.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    customer_style <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;正式普通话 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">    用一个平静、尊敬的语气
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    customer_email <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;
</span><span style="color:#f1fa8c">    嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！
</span><span style="color:#f1fa8c">更糟糕的是，保修条款可不包括清理我厨房的费用。
</span><span style="color:#f1fa8c">伙计，赶紧给我过来！
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    template_string <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;把由三个反引号分隔的文本</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">翻译成一种</span><span style="color:#f1fa8c">{my_style}</span><span style="color:#f1fa8c">风格。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">文本: ```</span><span style="color:#f1fa8c">{my_text}</span><span style="color:#f1fa8c">```
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>
    <span style="color:#6272a4"># 使用提示模版</span>
    prompt_template <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(template_string)
    customer_messages <span style="color:#ff79c6">=</span> prompt_template<span style="color:#ff79c6">.</span>format_messages(
        my_style <span style="color:#ff79c6">=</span> customer_style,
        my_text <span style="color:#ff79c6">=</span> customer_email
    )

    chat <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url
    )
    <span style="color:#6272a4"># 强制转换类型</span>
    openai_messages <span style="color:#ff79c6">=</span> chat(customer_messages)
    <span style="color:#8be9fd;font-style:italic">print</span>(openai_messages<span style="color:#ff79c6">.</span>content)

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>您好，我现在感到非常愤怒，我的搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！更糟糕的是，保修条款并不包括清理我厨房的费用。朋友，请赶紧过来帮帮我！感激不尽。
</code></pre><p>2.用海盗方言回复邮件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> ChatPromptTemplate

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;******&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.***.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    service_style <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;一个有礼貌的语气 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">    使用海盗风格
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    service_response <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;嘿，顾客， </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">保修不包括厨房的清洁费用， </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">因为您在启动搅拌机之前 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">忘记盖上盖子而误用搅拌机, </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">这是您的错。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">倒霉！ 再见！
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    template_string <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;把由三个反引号分隔的文本</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">翻译成一种</span><span style="color:#f1fa8c">{my_style}</span><span style="color:#f1fa8c">风格。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">文本: ```</span><span style="color:#f1fa8c">{my_text}</span><span style="color:#f1fa8c">```
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>
    <span style="color:#6272a4"># 使用提示模版</span>
    prompt_template <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(template_string)
    customer_messages <span style="color:#ff79c6">=</span> prompt_template<span style="color:#ff79c6">.</span>format_messages(
        my_style <span style="color:#ff79c6">=</span> service_style,
        my_text <span style="color:#ff79c6">=</span> service_response
    )

    chat <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url
    )

    openai_messages <span style="color:#ff79c6">=</span> chat<span style="color:#ff79c6">.</span>invoke(customer_messages)
    <span style="color:#8be9fd;font-style:italic">print</span>(openai_messages<span style="color:#ff79c6">.</span>content)

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>啊哟，尊贵的客人，抱歉地通知您，保修不包括厨房的清洁费用。因为您在启动搅拌机之前忘记盖上盖子而误用搅拌机，这可是您的疏忽啊。真是倒霉！祝您一天愉快，再见！愿您的航程一帆风顺！Yo-ho-ho！
</code></pre><p>3.为什么需要提示模版</p>
<p>使用提示模版，可以让我们更为方便地重复使用设计好的提示。LangChain还提供了提示模版用于一些常用场景。比如自动摘要、问答、连接到SQL数据库、连接到不同的API。</p>
<h4 id="输出解释器">输出解释器</h4>
<p>1.不使用输出解释器提取客户评价中的信息</p>
<p>给定的评价customer_review，从中提取信息，按以下格式输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#ff79c6">&#34;礼物&#34;</span>: 是的,
  <span style="color:#ff79c6">&#34;交货天数&#34;</span>: <span style="color:#bd93f9">5</span>,
  <span style="color:#ff79c6">&#34;价钱&#34;</span>: <span style="color:#f1fa8c">&#34;很贵&#34;</span>
}
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> ChatPromptTemplate

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;******&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.***.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    customer_review <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">   这款吹叶机非常神奇。 它有四个设置：</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">吹蜡烛、微风、风城、龙卷风。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">两天后就到了，正好赶上我妻子的</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">周年纪念礼物。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">我想我的妻子会喜欢它到说不出话来。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">到目前为止，我是唯一一个使用它的人，而且我一直</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">每隔一天早上用它来清理草坪上的叶子。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">它比其他吹叶机稍微贵一点，</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">但我认为它的额外功能是值得的。 
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    template_string <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">对于以下文本，请从中提取以下信息：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">礼物：该商品是作为礼物送给别人的吗？ </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">如果是，则回答 是的；如果否或未知，则回答 不是。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">交货天数：产品需要多少天</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">到达？ 如果没有找到该信息，则输出-1。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">价钱：提取有关价值或价格的任何句子，</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">并将它们输出为逗号分隔的 Python 列表。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">使用以下键将输出格式化为 JSON：
</span><span style="color:#f1fa8c">礼物
</span><span style="color:#f1fa8c">交货天数
</span><span style="color:#f1fa8c">价钱
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">文本: </span><span style="color:#f1fa8c">{my_text}</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>
    <span style="color:#6272a4"># 使用提示模版</span>
    prompt_template <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(template_string)
    customer_messages <span style="color:#ff79c6">=</span> prompt_template<span style="color:#ff79c6">.</span>format_messages(
        my_text <span style="color:#ff79c6">=</span> customer_review
    )

    chat <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url
    )

    openai_messages <span style="color:#ff79c6">=</span> chat<span style="color:#ff79c6">.</span>invoke(customer_messages)
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;结果类型：&#34;</span>, <span style="color:#8be9fd;font-style:italic">type</span>(openai_messages<span style="color:#ff79c6">.</span>content))
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;结果：&#34;</span>, openai_messages<span style="color:#ff79c6">.</span>content)

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>结果类型： &lt;class 'str'&gt;
结果： {
    &quot;礼物&quot;: &quot;是的&quot;,
    &quot;交货天数&quot;: 2,
    &quot;价钱&quot;: [&quot;它比其他吹叶机稍微贵一点&quot;]
}
</code></pre><p>返回的结果类型是字符串，想方便提取信息的话，还是要使用LangChain中的输出解释器。</p>
<p>2.使用输出解释器提取客户评价中的信息</p>
<p>使用LangChain的输出解释器</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> ChatPromptTemplate
<span style="color:#ff79c6">from</span> langchain.output_parsers <span style="color:#ff79c6">import</span> ResponseSchema, StructuredOutputParser

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;******&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.***.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    customer_review <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">   这款吹叶机非常神奇。 它有四个设置：</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">吹蜡烛、微风、风城、龙卷风。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">两天后就到了，正好赶上我妻子的</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">周年纪念礼物。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">我想我的妻子会喜欢它到说不出话来。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">到目前为止，我是唯一一个使用它的人，而且我一直</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">每隔一天早上用它来清理草坪上的叶子。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">它比其他吹叶机稍微贵一点，</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">但我认为它的额外功能是值得的。 
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    template_string <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">对于以下文本，请从中提取以下信息：
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">礼物：该商品是作为礼物送给别人的吗？
</span><span style="color:#f1fa8c">如果是，则回答 是的；如果否或未知，则回答 不是。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">交货天数：产品到达需要多少天？ 如果没有找到该信息，则输出-1。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">价钱：提取有关价值或价格的任何句子，并将它们输出为逗号分隔的Python列表。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">文本: </span><span style="color:#f1fa8c">{my_text}</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">{format_instructions}</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>
    <span style="color:#6272a4"># 使用提示模版</span>
    prompt_template <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(template_string)

    <span style="color:#6272a4"># 使用输出解释器</span>
    gift_schema <span style="color:#ff79c6">=</span> ResponseSchema(name<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;礼物&#34;</span>, description<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;这件物品是作为礼物送给别人的吗？</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">                            如果是，则回答 是的，</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">                            如果否或未知，则回答 不是。&#34;</span>)
    delivery_days_schema <span style="color:#ff79c6">=</span> ResponseSchema(name<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;交货天数&#34;</span>,
                                      description<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;产品需要多少天才能到达？</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">                                      如果没有找到该信息，则输出-1。&#34;</span>)
    price_value_schema <span style="color:#ff79c6">=</span> ResponseSchema(name<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;价钱&#34;</span>,
                                    description<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;提取有关价值或价格的任何句子，</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">                                    并将它们输出为逗号分隔的Python列表&#34;</span>)

    response_schemas <span style="color:#ff79c6">=</span> [gift_schema, delivery_days_schema, price_value_schema]
    output_parser <span style="color:#ff79c6">=</span> StructuredOutputParser<span style="color:#ff79c6">.</span>from_response_schemas(response_schemas)
    format_instructions <span style="color:#ff79c6">=</span> output_parser<span style="color:#ff79c6">.</span>get_format_instructions()

    customer_messages <span style="color:#ff79c6">=</span> prompt_template<span style="color:#ff79c6">.</span>format_messages(
        my_text<span style="color:#ff79c6">=</span>customer_review,
        format_instructions<span style="color:#ff79c6">=</span>format_instructions
    )

    chat <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url
    )

    openai_messages <span style="color:#ff79c6">=</span> chat(customer_messages)
    output_dict <span style="color:#ff79c6">=</span> output_parser<span style="color:#ff79c6">.</span>parse(openai_messages<span style="color:#ff79c6">.</span>content)
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;结果类型：&#34;</span>, <span style="color:#8be9fd;font-style:italic">type</span>(output_dict))
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;结果：&#34;</span>, output_dict)

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>结果类型： &lt;class 'dict'&gt;
结果： {'礼物': '是的', '交货天数': '两天后', '价钱': '它比其他吹叶机稍微贵一点'}
</code></pre><p>从输出结果来看，结果类型类型为字典dict，操作dict数据结构更方便提取数据</p>
<h2 id="记忆memory">记忆Memory</h2>
<p>使用LangChain中的记忆模块，将先前的对话嵌入到语言模型中，使其具有连续对话的能力。
使用LangChain中的记忆(Memory)模块时，它旨在保存、组织和跟踪整个对话的历史，从而为用户和模型之间的交互提供连续的上下文。</p>
<p>这里主要介绍常用的四种记忆模块，其他模块可以查阅文档</p>
<ul>
<li>对话缓存记忆(ConversationBufferMemory)</li>
<li>对话缓存窗口记忆(ConversationBufferWindowMemory)</li>
<li>对话令牌缓存记忆(ConversationTokenBufferMemory)</li>
<li>对话摘要缓存记忆(ConversationSummaryBufferMemory)</li>
</ul>
<p>在LangChain中，记忆指的是大语言模型的短期记忆。为什么称为短期记忆？因为当用户与训练好的LLM进行对话时，LLM会暂时记住用户的输入和它已经生成的输出，以便预测之后的输出，而模型输出完毕后，它便会“遗忘”之前用户的输入和它的输出。</p>
<p>如果想延长LLM短期记忆的保留时间，需要借助一些外部记忆方式来进行记忆，以便能够知道历史对话信息。</p>
<h3 id="对话缓存记忆">对话缓存记忆</h3>
<h4 id="初始化对话模型">初始化对话模型</h4>
<p>初始化对话模型，并进行多轮对话</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> ConversationChain
<span style="color:#ff79c6">from</span> langchain.memory <span style="color:#ff79c6">import</span> ConversationBufferMemory

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;******&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.***.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)
    memory <span style="color:#ff79c6">=</span> ConversationBufferMemory()

    conversation <span style="color:#ff79c6">=</span> ConversationChain(llm<span style="color:#ff79c6">=</span>llm, memory<span style="color:#ff79c6">=</span>memory, verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;你好，我叫特特鲁斯&#34;</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;1+1等于多少？&#34;</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;我叫什么名字？&#34;</span>)
	conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;&#34;</span>)

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&gt; Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: 你好，我叫特特鲁斯
AI:  你好，特特鲁斯！很高兴认识你。我是一个人工智能程序，可以回答你的问题或者和你聊天。你有什么想知道的吗？
Human: 我叫什么名字
AI: 你叫特特鲁斯。很特别的名字！你知道吗，特特鲁斯这个名字在拉丁语中意味着“勇敢的战士”。很有力量的名字呢！有什么其他问题想问我吗？
Human: 1+1等于多少
AI: 1加1等于2。这是一个非常简单的数学问题，答案是2。如果你有其他数学问题或者其他想知道的事情，都可以问我哦！我会尽力回答你的。
Human: 
AI:
</code></pre></div><p>使用predict进行预测时，LangChain会生成一些提示，使系统进行友好的对话</p>
<h4 id="查看记忆缓存">查看记忆缓存</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#6272a4"># memory.buffer_as_messages记忆了当前为止所有 的对话信息</span>
<span style="color:#8be9fd;font-style:italic">print</span>(memory<span style="color:#ff79c6">.</span>buffer_as_messages)
<span style="color:#6272a4"># load_memory_variables也可以打印缓存中的历史消息</span>
<span style="color:#8be9fd;font-style:italic">print</span>(memory<span style="color:#ff79c6">.</span>load_memory_variables({}))
</code></pre></div><p>输出</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">[HumanMessage(content=&#39;你好，我叫特特鲁斯&#39;), AIMessage(content=&#39; 你好，特特鲁斯！很高兴认识你。我是一个人工智能助手，可以回答你的问题或者和你聊天。有什么我可以帮助你的吗？&#39;), HumanMessage(content=&#39;我叫什么名字&#39;), AIMessage(content=&#39;抱歉，我不知道你的名字。你可以告诉我吗？&#39;), HumanMessage(content=&#39;1+1等于多少&#39;), AIMessage(content=&#39;1加1等于2。您还有其他问题吗？&#39;), HumanMessage(content=&#39;&#39;), AIMessage(content=&#39;如果您有任何其他问题或者想要聊天，随时告诉我哦！我随时准备好帮助您。&#39;)]

{&#39;history&#39;: &#39;Human: 你好，我叫特特鲁斯\nAI:  你好，特特鲁斯！我是一个AI助手，很高兴认识你。你有什么问题或者想要聊什么吗？\nHuman: 我叫什么名字\nAI: 你叫特特鲁斯。特特鲁斯是一个很独特的名字，听起来很有个性。你喜欢这个名字吗？如果你有任何其他问题或者想要聊什么，随时告诉我哦！\nHuman: 1+1等于多少\nAI: 1加1等于2。这是一个非常基本的数学问题，答案是2。如果你有任何其他数学问题或者其他想要了解的知识，都可以问我哦！我会尽力帮助你。\nHuman: \nAI: 你有任何其他问题或者想要聊什么吗？我可以提供各种信息和帮助。&#39;}
</code></pre></div><h4 id="添加内容到记忆缓存">添加内容到记忆缓存</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> ConversationChain
<span style="color:#ff79c6">from</span> langchain.memory <span style="color:#ff79c6">import</span> ConversationBufferMemory

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;******&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.***.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)
    memory <span style="color:#ff79c6">=</span> ConversationBufferMemory()
    memory<span style="color:#ff79c6">.</span>save_context({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我叫ice&#34;</span>}, {<span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我叫tracy&#34;</span>})
    
    conversation <span style="color:#ff79c6">=</span> ConversationChain(llm<span style="color:#ff79c6">=</span>llm, memory<span style="color:#ff79c6">=</span>memory, verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;你好，我叫特特鲁斯&#34;</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;1+1等于多少？&#34;</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;我叫什么名字？&#34;</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;&#34;</span>)
    <span style="color:#8be9fd;font-style:italic">print</span>(memory<span style="color:#ff79c6">.</span>load_memory_variables({}))

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&gt; Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: 你好，我叫ice
AI: 你好，我叫tracy
Human: 你好，我叫特特鲁斯
AI: 很高兴认识你，特特鲁斯！你有什么想要了解或者讨论的吗？
Human: 1+1等于多少？
AI: 1加1等于2。这是一个基本的数学问题，答案是2。您还有其他问题吗？
Human: 我叫什么名字？
AI: 您说您叫特特鲁斯。您的名字是特特鲁斯。有什么其他问题吗？
Human: 
AI:

&gt; Finished chain.
{&#39;history&#39;: &#39;Human: 你好，我叫ice\nAI: 你好，我叫tracy\nHuman: 你好，我叫特特鲁斯\nAI: 很高兴认识你，特特鲁斯！你有什么想要了解或者讨论的吗？\nHuman: 1+1等于多少？\nAI: 1加1等于2。这是一个基本的数学问题，答案是2。您还有其他问题吗？\nHuman: 我叫什么名字？\nAI: 您说您叫特特鲁斯。您的名字是特特鲁斯。有什么其他问题吗？\nHuman: \nAI: 您有什么其他问题或者想要讨论的吗？我可以提供各种信息和帮助。&#39;}
</code></pre></div><p>使用<code>save_context</code>添加内容到buffer中，然后通过<code>memory.load_memory_variables({})</code>打印对话历史。在使用大型语言模型进行聊天对话时，大型语言模型本身实际上是无状态的。语言模型本身并不记得到目前为止的历史对话。</p>
<h3 id="对话缓存窗口记忆">对话缓存窗口记忆</h3>
<p>对话随着时间积累会越来越长，内存也占用越来越多，这就需要大量的token发送到大模型。为了节约token，对话缓存窗口记忆可以设置大小限制</p>
<h4 id="添加对话到窗口记忆">添加对话到窗口记忆</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.memory <span style="color:#ff79c6">import</span> ConversationBufferWindowMemory

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;xxxx&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.xxx.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)
    memory <span style="color:#ff79c6">=</span> ConversationBufferWindowMemory(k<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1</span>)
    memory<span style="color:#ff79c6">.</span>save_context({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我叫ice&#34;</span>}, {<span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我叫tracy&#34;</span>})
    memory<span style="color:#ff79c6">.</span>save_context({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;你好，朋友&#34;</span>}, {<span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我们一起玩吧&#34;</span>})
    <span style="color:#8be9fd;font-style:italic">print</span>(memory<span style="color:#ff79c6">.</span>load_memory_variables({}))

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>{'history': 'Human: 你好，朋友\nAI: 你好，我们一起玩吧'}
</code></pre><p>使用<code>ConversationBufferWindowMemory</code>来实现交互的滑动窗口，设置k=1只保留最近的一个对话记忆。</p>
<h4 id="在对话链中应用窗口记忆">在对话链中应用窗口记忆</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> ConversationChain
<span style="color:#ff79c6">from</span> langchain.memory <span style="color:#ff79c6">import</span> ConversationBufferWindowMemory

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;xxxx&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)
    memory <span style="color:#ff79c6">=</span> ConversationBufferWindowMemory(k<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1</span>)
    conversation <span style="color:#ff79c6">=</span> ConversationChain(llm<span style="color:#ff79c6">=</span>llm, memory<span style="color:#ff79c6">=</span>memory, verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;你好，我叫特特鲁斯&#34;</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;1+1等于多少？&#34;</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;我叫什么名字？&#34;</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;&#34;</span>)
    <span style="color:#8be9fd;font-style:italic">print</span>(memory<span style="color:#ff79c6">.</span>load_memory_variables({}))

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&gt; Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: 我叫什么名字？
AI: 抱歉，我无法知道你的名字，因为我是一个人工智能程序，无法获取你的个人信息。如果你有其他问题或需要帮助，请随时告诉我。
Human: 
AI:

&gt; Finished chain.
{&#39;history&#39;: &#39;Human: \nAI: 你好！有什么我可以帮助你的吗？&#39;}
</code></pre></div><p>从输出结果来看，窗口记忆只能记住上一轮对话的信息</p>
<h3 id="对话字符缓存记忆">对话字符缓存记忆</h3>
<p>使用对话字符缓存记忆，内存将限制保存的token数量</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> ConversationChain
<span style="color:#ff79c6">from</span> langchain.memory <span style="color:#ff79c6">import</span> ConversationTokenBufferMemory

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)
    memory <span style="color:#ff79c6">=</span> ConversationTokenBufferMemory(llm<span style="color:#ff79c6">=</span>llm, max_token_limit<span style="color:#ff79c6">=</span><span style="color:#bd93f9">20</span>)
    memory<span style="color:#ff79c6">.</span>save_context({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我叫ice&#34;</span>}, {<span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我叫tracy&#34;</span>})
    memory<span style="color:#ff79c6">.</span>save_context({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;你好，朋友&#34;</span>}, {<span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我们一起玩吧&#34;</span>})
    <span style="color:#8be9fd;font-style:italic">print</span>(memory<span style="color:#ff79c6">.</span>load_memory_variables({}))

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>{'history': 'AI: 你好，我们一起玩吧'}
</code></pre><p>ChatGPT是使用了一种基于字节对编码(BPE)的方法啦进行tokenization。BPE是一种常见的tokenization技术，将输入文本分割成较小的子词单元。OpenAl是用tiktoken这个库来计算token的，tiktoken在github上是开源的<a href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a>.关于汉子和英文单词的token计算方式，知乎上一篇文章讲解：<a href="https://www.zhihu.com/question/594159910">https://www.zhihu.com/question/594159910</a></p>
<h3 id="对话摘要缓存记忆">对话摘要缓存记忆</h3>
<p>对话摘要缓存记忆，使用LLM对到目前为止历史对话自动总结摘要</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.memory <span style="color:#ff79c6">import</span> ConversationSummaryBufferMemory

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)
    
    schedule <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;在八点你和你的产品团队有一个会议。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你需要做一个PPT。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">上午9点到12点你需要忙于LangChain。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">Langchain是一个有用的工具，因此你的项目进展的非常快。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">中午，在意大利餐厅与一位开车来的顾客共进午餐 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">走了一个多小时的路程与你见面，只为了解最新的 AI。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">确保你带了笔记本电脑可以展示最新的 LLM 样例.&#34;</span>

    memory <span style="color:#ff79c6">=</span> ConversationSummaryBufferMemory(llm<span style="color:#ff79c6">=</span>llm, max_token_limit<span style="color:#ff79c6">=</span><span style="color:#bd93f9">20</span>)
    memory<span style="color:#ff79c6">.</span>save_context({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我叫ice&#34;</span>}, {<span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我叫tracy&#34;</span>})
    memory<span style="color:#ff79c6">.</span>save_context({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;你好，朋友&#34;</span>}, {<span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我们一起玩吧&#34;</span>})
    memory<span style="color:#ff79c6">.</span>save_context({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;今天的日程安排是什么？&#34;</span>}, {<span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{</span>schedule<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">&#34;</span>})
    <span style="color:#8be9fd;font-style:italic">print</span>(memory<span style="color:#ff79c6">.</span>load_memory_variables({}))

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">{&#39;history&#39;: &#39;Human: 你好，我叫ice\nAI: 你好，我叫tracy\nHuman: 你好，朋友\nAI: 你好，我们一起玩吧\nHuman: 今天的日程安排是什么？\nAI: 在八点你和你的产品团队有一个会议。 你需要做一个PPT。 上午9点到12点你需要忙于LangChain。Langchain是一个有用的工具，因此你的项目进展的非常快。中午，在意大利餐厅与一位开车来的顾客共进午餐 走了一个多小时的路程与你见面，只为了解最新的 AI。 确保你带了笔记本电脑可以展示最新的 LLM 样例.&#39;}
</code></pre></div><p>基于对话摘要缓存记忆的对话链</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> ConversationChain
<span style="color:#ff79c6">from</span> langchain.memory <span style="color:#ff79c6">import</span> ConversationSummaryBufferMemory

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)
    
    schedule <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;在八点你和你的产品团队有一个会议。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你需要做一个PPT。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">上午9点到12点你需要忙于LangChain。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">Langchain是一个有用的工具，因此你的项目进展的非常快。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">中午，在意大利餐厅与一位开车来的顾客共进午餐 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">走了一个多小时的路程与你见面，只为了解最新的 AI。 </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">确保你带了笔记本电脑可以展示最新的 LLM 样例.&#34;</span>

    memory <span style="color:#ff79c6">=</span> ConversationSummaryBufferMemory(llm<span style="color:#ff79c6">=</span>llm, max_token_limit<span style="color:#ff79c6">=</span><span style="color:#bd93f9">1000</span>)
    memory<span style="color:#ff79c6">.</span>save_context({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我叫ice&#34;</span>}, {<span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我叫tracy&#34;</span>})
    memory<span style="color:#ff79c6">.</span>save_context({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;你好，朋友&#34;</span>}, {<span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;你好，我们一起玩吧&#34;</span>})
    memory<span style="color:#ff79c6">.</span>save_context({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;今天的日程安排是什么？&#34;</span>}, {<span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{</span>schedule<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">&#34;</span>})

    conversation <span style="color:#ff79c6">=</span> ConversationChain(llm<span style="color:#ff79c6">=</span>llm, memory<span style="color:#ff79c6">=</span>memory, verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
    conversation<span style="color:#ff79c6">.</span>predict(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;展示什么样的样例最好呢&#34;</span>)
    <span style="color:#8be9fd;font-style:italic">print</span>(memory<span style="color:#ff79c6">.</span>load_memory_variables({}))

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">{&#39;history&#39;: &#39;Human: 你好，我叫ice\nAI: 你好，我叫tracy\nHuman: 你好，朋友\nAI: 你好，我们一起玩吧\nHuman: 今天的日程安排是什么？\nAI: 在八点你和你的产品团队有一个会议。 你需要做一个PPT。 上午9点到12点你需要忙于LangChain。Langchain是一个有用的工具，因此你的项目进展的非常快。中午，在意大利餐厅与一位开车来的顾客共进午餐 走了一个多小时的路程与你见面，只为了解最新的 AI。 确保你带了笔记本电脑可以展示最新的 LLM 样例.\nHuman: 展示什么样的样例最好呢\nAI: 展示一些关于LangChain如何提高生产效率的案例会很有帮助。你可以展示一些实际的数据和结果，以及用户的反馈和体验。这样可以更直观地展示LangChain的价值和优势。希望这些建议对你有所帮助！&#39;}
</code></pre></div><p>从输出结果来看，摘要内容更新了</p>
<h2 id="回调callbacks">回调Callbacks</h2>
<p>LangChain提供回调机制，允许hook到大模型应用的各个阶段，通过订阅这些时间来触发回调函数。官方文档的回调章节介绍：
<a href="https://python.langchain.com/docs/modules/callbacks/">https://python.langchain.com/docs/modules/callbacks/</a></p>
<h3 id="回调处理">回调处理</h3>
<p>这里有个重要的概念回调处理，CallbackHandlers是实现该CallbackHandler接口的对象，该接口对于每个可以订阅的事件都有一个方法。CallbackManager当事件被触发时，将在每个处理程序上调用适当的方法。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">class</span> <span style="color:#50fa7b">BaseCallbackHandler</span>(
    LLMManagerMixin,
    ChainManagerMixin,
    ToolManagerMixin,
    RetrieverManagerMixin,
    CallbackManagerMixin,
    RunManagerMixin,
)
</code></pre></div><p>继承这么多类，展开的话相当于</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">class</span> <span style="color:#50fa7b">BaseCallbackHandler</span>:
    <span style="color:#f1fa8c">&#34;&#34;&#34;Base callback handler that can be used to handle callbacks from langchain.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_llm_start</span>(
        self, serialized: Dict[<span style="color:#8be9fd;font-style:italic">str</span>, Any], prompts: List[<span style="color:#8be9fd;font-style:italic">str</span>], <span style="color:#ff79c6">**</span>kwargs: Any
    ) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when LLM starts running.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_chat_model_start</span>(
        self, serialized: Dict[<span style="color:#8be9fd;font-style:italic">str</span>, Any], messages: List[List[BaseMessage]], <span style="color:#ff79c6">**</span>kwargs: Any
    ) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when Chat Model starts running.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_llm_new_token</span>(self, token: <span style="color:#8be9fd;font-style:italic">str</span>, <span style="color:#ff79c6">**</span>kwargs: Any) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run on new LLM token. Only available when streaming is enabled.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_llm_end</span>(self, response: LLMResult, <span style="color:#ff79c6">**</span>kwargs: Any) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when LLM ends running.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_llm_error</span>(
        self, error: Union[Exception, KeyboardInterrupt], <span style="color:#ff79c6">**</span>kwargs: Any
    ) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when LLM errors.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_chain_start</span>(
        self, serialized: Dict[<span style="color:#8be9fd;font-style:italic">str</span>, Any], inputs: Dict[<span style="color:#8be9fd;font-style:italic">str</span>, Any], <span style="color:#ff79c6">**</span>kwargs: Any
    ) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when chain starts running.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_chain_end</span>(self, outputs: Dict[<span style="color:#8be9fd;font-style:italic">str</span>, Any], <span style="color:#ff79c6">**</span>kwargs: Any) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when chain ends running.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_chain_error</span>(
        self, error: Union[Exception, KeyboardInterrupt], <span style="color:#ff79c6">**</span>kwargs: Any
    ) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when chain errors.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_tool_start</span>(
        self, serialized: Dict[<span style="color:#8be9fd;font-style:italic">str</span>, Any], input_str: <span style="color:#8be9fd;font-style:italic">str</span>, <span style="color:#ff79c6">**</span>kwargs: Any
    ) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when tool starts running.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_tool_end</span>(self, output: Any, <span style="color:#ff79c6">**</span>kwargs: Any) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when tool ends running.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_tool_error</span>(
        self, error: Union[Exception, KeyboardInterrupt], <span style="color:#ff79c6">**</span>kwargs: Any
    ) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when tool errors.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_text</span>(self, text: <span style="color:#8be9fd;font-style:italic">str</span>, <span style="color:#ff79c6">**</span>kwargs: Any) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run on arbitrary text.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_agent_action</span>(self, action: AgentAction, <span style="color:#ff79c6">**</span>kwargs: Any) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run on agent action.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_agent_finish</span>(self, finish: AgentFinish, <span style="color:#ff79c6">**</span>kwargs: Any) <span style="color:#ff79c6">-&gt;</span> Any:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run on agent end.&#34;&#34;&#34;</span>
</code></pre></div><h3 id="回调在哪里传递">回调在哪里传递</h3>
<p>callbacks在整个 API 中的大多数对象（链、模型、工具、代理等）上都可用，位于两个不同的位置：</p>
<ul>
<li>构造函数回调：在构造函数中定义，例如LLMChain(callbacks=[handler], tags=[&lsquo;a-tag&rsquo;])。在这种情况下，回调将用于对该对象进行的所有调用，并且仅限于该对象，例如，如果将处理程序传递给构造函数LLMChain，则附加到该链的模型将不会使用它。</li>
<li>请求回调：在用于发出请求的“invoke”方法中定义。在这种情况下，回调将仅用于该特定请求及其包含的所有子请求（例如，对LLMChain的调用会触发对模型的调用，模型使用在方法中传递的相同处理程序invoke()）。在invoke()方法中回调是通过配置参数传递的。使用“调用”方法的示例（注意：相同的方法可用于batch、ainvoke和abatch方法。）</li>
</ul>
<p>这两者有什么区别?</p>
<ul>
<li>构造函数回调对于日志记录、监控等用例最有用，这些用例不特定于单个请求，而是特定于整个链。例如，如果您想记录对 发出的所有请LLMChain，您可以将处理程序传递给构造函数。</li>
<li>请求回调对于诸如流式传输之类的用例最有用，您希望将单个请求的输出流式传输到特定的Websocket连接，或其他类似的用例。例如，如果您想将单个请求的输出流式传输到 websocket，您可以将处理invoke()程序传递给该方法</li>
</ul>
<p>一个异步回调的例子</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain_core.messages <span style="color:#ff79c6">import</span> HumanMessage
<span style="color:#ff79c6">from</span> langchain_core.outputs <span style="color:#ff79c6">import</span> LLMResult
<span style="color:#ff79c6">from</span> langchain.callbacks.base <span style="color:#ff79c6">import</span> AsyncCallbackHandler, BaseCallbackHandler
<span style="color:#ff79c6">from</span> typing <span style="color:#ff79c6">import</span> Any, Dict, List
<span style="color:#ff79c6">import</span> asyncio
<span style="color:#ff79c6">from</span> pydantic <span style="color:#ff79c6">import</span> SecretStr

api_key <span style="color:#ff79c6">=</span> SecretStr(<span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>)
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>


<span style="color:#ff79c6">class</span> <span style="color:#50fa7b">MyCustomSyncHandler</span>(BaseCallbackHandler):
    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_llm_new_token</span>(self, token: <span style="color:#8be9fd;font-style:italic">str</span>, <span style="color:#ff79c6">**</span>kwargs) <span style="color:#ff79c6">-&gt;</span> <span style="color:#ff79c6">None</span>:
        <span style="color:#8be9fd;font-style:italic">print</span>(
            <span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;Sync handler being called in a `thread_pool_executor`: token: </span><span style="color:#f1fa8c">{</span>token<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">&#34;</span>)


<span style="color:#ff79c6">class</span> <span style="color:#50fa7b">MyCustomAsyncHandler</span>(AsyncCallbackHandler):
    <span style="color:#f1fa8c">&#34;&#34;&#34;Async callback handler that can be used to handle callbacks from langchain.&#34;&#34;&#34;</span>

    <span style="color:#ff79c6">async</span> <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_llm_start</span>(
        self, serialized: Dict[<span style="color:#8be9fd;font-style:italic">str</span>, Any], prompts: List[<span style="color:#8be9fd;font-style:italic">str</span>], <span style="color:#ff79c6">**</span>kwargs: Any
    ) <span style="color:#ff79c6">-&gt;</span> <span style="color:#ff79c6">None</span>:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when chain starts running.&#34;&#34;&#34;</span>
        <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;zzzz....&#34;</span>)
        <span style="color:#ff79c6">await</span> asyncio<span style="color:#ff79c6">.</span>sleep(<span style="color:#bd93f9">0.3</span>)
        class_name <span style="color:#ff79c6">=</span> serialized[<span style="color:#f1fa8c">&#34;name&#34;</span>]
        <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;Hi! I just woke up. Your llm is starting&#34;</span>)

    <span style="color:#ff79c6">async</span> <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">on_llm_end</span>(self, response: LLMResult, <span style="color:#ff79c6">**</span>kwargs: Any) <span style="color:#ff79c6">-&gt;</span> <span style="color:#ff79c6">None</span>:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Run when chain ends running.&#34;&#34;&#34;</span>
        <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;zzzz....&#34;</span>)
        <span style="color:#ff79c6">await</span> asyncio<span style="color:#ff79c6">.</span>sleep(<span style="color:#bd93f9">0.3</span>)
        <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;Hi! I just woke up. Your llm is ending&#34;</span>)


<span style="color:#ff79c6">async</span> <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># To enable streaming, we pass in `streaming=True` to the ChatModel constructor</span>
    <span style="color:#6272a4"># Additionally, we pass in a list with our custom handler</span>
    chat <span style="color:#ff79c6">=</span> ChatOpenAI(
        max_tokens<span style="color:#ff79c6">=</span><span style="color:#bd93f9">30</span>,
        base_url<span style="color:#ff79c6">=</span>openai_url,
        api_key<span style="color:#ff79c6">=</span>api_key,
        streaming<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
        callbacks<span style="color:#ff79c6">=</span>[MyCustomSyncHandler(), MyCustomAsyncHandler()],
    )

    <span style="color:#ff79c6">await</span> chat<span style="color:#ff79c6">.</span>agenerate([[HumanMessage(content<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Tell me a joke&#34;</span>)]])


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    asyncio<span style="color:#ff79c6">.</span>run(main())

</code></pre></div><p>如果打算使用async API，建议使用AsyncCallbackHandler以避免阻塞runloop。这里
在使用异步方法运行LLM/链/工具/代理时使用同步，它仍然可以工作。如果这个同步的CallbackHandler是线程安全的，
那就没有问题。</p>
<p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">zzzz....
Hi! I just woke up. Your llm is starting
Sync handler being called in a `thread_pool_executor`: token: 
Sync handler being called in a `thread_pool_executor`: token: Why
Sync handler being called in a `thread_pool_executor`: token:  couldn
Sync handler being called in a `thread_pool_executor`: token: &#39;t
Sync handler being called in a `thread_pool_executor`: token:  the
Sync handler being called in a `thread_pool_executor`: token:  bicycle
Sync handler being called in a `thread_pool_executor`: token:  stand
Sync handler being called in a `thread_pool_executor`: token:  up
Sync handler being called in a `thread_pool_executor`: token:  by
Sync handler being called in a `thread_pool_executor`: token:  itself
Sync handler being called in a `thread_pool_executor`: token: ?


Sync handler being called in a `thread_pool_executor`: token: Because
Sync handler being called in a `thread_pool_executor`: token:  it
Sync handler being called in a `thread_pool_executor`: token:  was
Sync handler being called in a `thread_pool_executor`: token:  two
Sync handler being called in a `thread_pool_executor`: token:  tired
Sync handler being called in a `thread_pool_executor`: token: !
Sync handler being called in a `thread_pool_executor`: token: 
zzzz....
Hi! I just woke up. Your llm is ending
</code></pre></div><h2 id="模型链chains">模型链Chains</h2>
<p>链是将大语言模型(LLM)和提示(Prompt)结合在一起，这样可以对文本进行一系列操作。使用链一个典型的流程：</p>
<ul>
<li>创建一个链，链接受输入</li>
<li>使用提示模版对其格式化</li>
<li>将格式化的内容发送给LLM</li>
</ul>
<p>可以把多个链组合在一起，或者链与其他组件组合形成一个更复杂的链。</p>
<h3 id="大语言模型链">大语言模型链</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> ChatPromptTemplate
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> LLMChain

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 1.初始化语言模型</span>
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)
    <span style="color:#6272a4"># 2.初始化提示模版：接受一个名为product的变量。该prompt将要求LLM生成一个描述制造该产品的公司的最佳名称</span>
    prompt <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(<span style="color:#f1fa8c">&#34;描述制造</span><span style="color:#f1fa8c">{product}</span><span style="color:#f1fa8c">该产品的公司的最佳名称是什么&#34;</span>)
    <span style="color:#6272a4"># 3.构建大语言模型链，链～=LLM+Prompt</span>
    chain <span style="color:#ff79c6">=</span> LLMChain(llm<span style="color:#ff79c6">=</span>llm, prompt<span style="color:#ff79c6">=</span>prompt)
    <span style="color:#6272a4"># 4.运行大语言模型链</span>
    product <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;大号床单套装&#34;</span>
    <span style="color:#8be9fd;font-style:italic">print</span>(chain<span style="color:#ff79c6">.</span>invoke(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;product&#34;</span>: product})<span style="color:#ff79c6">.</span>get(<span style="color:#f1fa8c">&#39;text&#39;</span>))

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&#34;豪华床品有限公司&#34;
</code></pre></div><h3 id="简单顺序链">简单顺序链</h3>
<p>顾名思义顺序链是按定义顺序执行的链，简单顺序链是顺序链中的最简单类型，其中每个步骤都有一个输入/输出，一个步骤的输出是下一个步骤的输入。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> ChatPromptTemplate
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> LLMChain, SimpleSequentialChain

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 1.初始化语言模型</span>
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)
    <span style="color:#6272a4"># 2.初始化提示模版1：这个提示将接受产品并返回最佳名称来描述该公司</span>
    prompt1 <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(<span style="color:#f1fa8c">&#34;描述制造</span><span style="color:#f1fa8c">{product}</span><span style="color:#f1fa8c">该产品的公司的最佳名称是什么&#34;</span>)
    <span style="color:#6272a4"># 3.初始化提示模版2：接受公司名称，然后输出该公司的长为20个单词的描述</span>
    prompt2 <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(<span style="color:#f1fa8c">&#34;写一个20个单词的描述对于这个公司：</span><span style="color:#f1fa8c">{company_name}</span><span style="color:#f1fa8c">&#34;</span>)
    <span style="color:#6272a4"># 4.构建大语言模型子链，链～=LLM+Prompt</span>
    chain_one <span style="color:#ff79c6">=</span> LLMChain(llm<span style="color:#ff79c6">=</span>llm, prompt<span style="color:#ff79c6">=</span>prompt1)
    chain_two <span style="color:#ff79c6">=</span> LLMChain(llm<span style="color:#ff79c6">=</span>llm, prompt<span style="color:#ff79c6">=</span>prompt2)
    <span style="color:#6272a4"># 5.构建一个简单顺序链，把两个子链组合起来</span>
    simple_chain <span style="color:#ff79c6">=</span> SimpleSequentialChain(chains<span style="color:#ff79c6">=</span>[chain_one, chain_two], verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
    <span style="color:#6272a4"># 6.运行简单顺序链</span>
    product <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;大号床单套装&#34;</span>
    simple_chain<span style="color:#ff79c6">.</span>invoke(product)

<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&gt; Entering new SimpleSequentialChain chain...
&#34;豪华床品有限公司&#34;
&#34;豪华床品有限公司&#34;提供高品质、舒适的床上用品，让您享受豪华睡眠体验，提升生活品质。
</code></pre></div><h3 id="复杂顺序链">复杂顺序链</h3>
<p>当有多个输入或多个输出时，就需要复杂顺序链来实现；简单顺序链只针对一个输入和一个输出时。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> ChatPromptTemplate
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> LLMChain, SequentialChain

api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>

<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 1.初始化语言模型</span>
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)
    <span style="color:#6272a4"># 2.子链1: 翻译成英语</span>
    prompt1 <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(
        <span style="color:#f1fa8c">&#34;把下面的文本翻译成英文: </span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>
        <span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{review}</span><span style="color:#f1fa8c">&#34;</span>
    )
    chain_one <span style="color:#ff79c6">=</span> LLMChain(llm<span style="color:#ff79c6">=</span>llm, prompt<span style="color:#ff79c6">=</span>prompt1, output_key<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;english_review&#34;</span>)
    <span style="color:#6272a4"># 2.子链2: 用一句话总结下面的文本</span>
    prompt2 <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(
        <span style="color:#f1fa8c">&#34;用一句话总结下面的文本：</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>
        <span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{english_review}</span><span style="color:#f1fa8c">&#34;</span>
    )
    chain_two <span style="color:#ff79c6">=</span> LLMChain(llm<span style="color:#ff79c6">=</span>llm, prompt<span style="color:#ff79c6">=</span>prompt2, output_key<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;summary&#34;</span>)
    <span style="color:#6272a4"># 4.子链3: 下面文本使用什么语言</span>
    prompt3 <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(
        <span style="color:#f1fa8c">&#34;下面的文本使用的是什么语言：</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>
        <span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{review}</span><span style="color:#f1fa8c">&#34;</span>
    )
    chain_three <span style="color:#ff79c6">=</span> LLMChain(llm<span style="color:#ff79c6">=</span>llm, prompt<span style="color:#ff79c6">=</span>prompt3, output_key<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;language&#34;</span>)
    <span style="color:#6272a4"># 4.子链4: 使用特定的语言对下面的总结写一个后续回复</span>
    prompt4 <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(
        <span style="color:#f1fa8c">&#34;使用特定的语言对下面的总结写一个后续回复：</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>
        <span style="color:#f1fa8c">&#34;总结: </span><span style="color:#f1fa8c">{summary}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">语言: </span><span style="color:#f1fa8c">{language}</span><span style="color:#f1fa8c">&#34;</span>
    )
    chain_four <span style="color:#ff79c6">=</span> LLMChain(llm<span style="color:#ff79c6">=</span>llm, prompt<span style="color:#ff79c6">=</span>prompt4, output_key<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;followup_message&#34;</span>)
    <span style="color:#6272a4"># 5.构建一个顺序链，把4个子链组合起来</span>
    chain <span style="color:#ff79c6">=</span> SequentialChain(
        chains<span style="color:#ff79c6">=</span>[chain_one, chain_two, chain_three, chain_four],
        input_variables<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;review&#34;</span>],
        output_variables<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;english_review&#34;</span>,<span style="color:#f1fa8c">&#34;summary&#34;</span>, <span style="color:#f1fa8c">&#34;followup_message&#34;</span>],
        verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
        return_all<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
    <span style="color:#6272a4"># 6.运行顺序链</span>
    review <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;Kubernetes 和更广泛的容器生态系统正发展为通用计算平台和生态系统，</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">    可以媲美甚至超越虚拟机 (VM)，作为现代云基础设施和应用程序的基本构建块。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">    该生态系统使组织能够提供高生产力的平台即服务 (PaaS)，</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">    解决围绕云原生开发的多个基础设施相关和操作相关任务与问题，</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">    以便开发团队专注于编码和创新&#34;</span>
    <span style="color:#8be9fd;font-style:italic">print</span>(chain<span style="color:#ff79c6">.</span>invoke(review))
     
<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()
</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">{&#39;review&#39;: &#39;Kubernetes 和更广泛的容器生态系统正发展为通用计算平台和生态系统，    可以媲美甚至超越虚拟机 (VM)，作为现代云基础设施和应用程序的基本构建块。    该生态系统使组织能够提供高生产力的平台即服务 (PaaS)，    解决围绕云原生开发的多个基础设施相关和操作相关任务与问题，    以便开发团队专注于编码和创新&#39;, &#39;english_review&#39;: &#39;Kubernetes and the broader container ecosystem are evolving into a universal computing platform and ecosystem that can rival or even surpass virtual machines (VMs) as the fundamental building blocks of modern cloud infrastructure and applications. This ecosystem enables organizations to deliver highly productive platform-as-a-service (PaaS), addressing multiple infrastructure and operational tasks and issues related to cloud-native development, allowing development teams to focus on coding and innovation.&#39;, &#39;summary&#39;: &#39;Kubernetes and containers are becoming the new standard for cloud infrastructure, enabling organizations to focus on coding and innovation.&#39;, &#39;followup_message&#39;: &#39;非常赞同这个总结！Kubernetes和容器正在成为云基础设施的新标准，让组织能够更专注于编码和创新。这种趋势对于推动技术发展和提高效率都有着重要的作用。希望更多的企业能够采用这些先进的技术，实现更快速的发展和创新。&#39;}
</code></pre></div><h3 id="路由链">路由链</h3>
<p>路由链顾名思义可以定义路由，具体路由到某一个子链上去，这样就可以实现更复杂的链操作。</p>
<p>路由器由两个组件组成：（类似网络中的路由概念）</p>
<ul>
<li>路由链：路由器链本身，负责选择要调用的下一个链</li>
<li>目的链：路由器链可以路由到的链</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> ChatPromptTemplate, PromptTemplate
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> LLMChain
<span style="color:#ff79c6">from</span> langchain.chains.router <span style="color:#ff79c6">import</span> MultiPromptChain
<span style="color:#ff79c6">from</span> langchain.chains.router.llm_router <span style="color:#ff79c6">import</span> LLMRouterChain
<span style="color:#ff79c6">from</span> langchain.chains.router.llm_router <span style="color:#ff79c6">import</span> RouterOutputParser


api_key <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 1.初始化语言模型</span>
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)

    <span style="color:#6272a4"># 2.初始化物理问题提示模版</span>
    physics_template <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;你是一个非常聪明的物理专家</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你擅长用一种简洁并且易于理解的方式去回答问题</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">当你不知道问题的答案时，你承认</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你不知道.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">这是一个问题:
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">{input}</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>
    <span style="color:#6272a4"># 3.初始化数学问题提示模版</span>
    math_template <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;你是一个非常优秀的数学家。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你擅长回答数学问题。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你之所以如此优秀,</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">是因为你能够将棘手的问题分解为组成部分,</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">回答组成部分，然后将它们组合在一起，回答更广泛的问题。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">这是一个问题：
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">{input}</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    <span style="color:#6272a4"># 4.初始化历史问题提示模版</span>
    history_template <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;你是以为非常优秀的历史学家。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你对一系列历史时期的人物、事件和背景有着极好的学识和理解</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你有能力思考、反思、辩证、讨论和评估过去。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你尊重历史证据，并有能力利用它来支持你的解释和判断。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">这是一个问题:
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">{input}</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    <span style="color:#6272a4"># 5.初始化计算机问题提示模版</span>
    computerscience_template <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;你是一个成功的计算机科学专家。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你有创造力、协作精神、</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">前瞻性思维、自信、解决问题的能力、</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">对理论和算法的理解以及出色的沟通技巧。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你非常擅长回答编程问题。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你之所以如此优秀，是因为你知道</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">如何通过以机器可以轻松解释的命令式步骤描述解决方案来解决问题,</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">并且你知道如何选择在时间复杂性和空间复杂性之间取得良好平衡的解决方案。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">这是一个问题:
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">{input}</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    <span style="color:#6272a4"># 6.对上述提示模版进行命名和描述，这些信息传递给路由链，路由链决定使用哪个子链</span>
    prompt_infos <span style="color:#ff79c6">=</span> [
        {
            <span style="color:#f1fa8c">&#34;name&#34;</span>: <span style="color:#f1fa8c">&#34;物理学&#34;</span>,
            <span style="color:#f1fa8c">&#34;desc&#34;</span>: <span style="color:#f1fa8c">&#34;擅长回答关于物理学的问题&#34;</span>,
            <span style="color:#f1fa8c">&#34;prompt_template&#34;</span>: physics_template
        },
        {
            <span style="color:#f1fa8c">&#34;name&#34;</span>: <span style="color:#f1fa8c">&#34;数学&#34;</span>,
            <span style="color:#f1fa8c">&#34;desc&#34;</span>: <span style="color:#f1fa8c">&#34;擅长回答数学问题&#34;</span>,
            <span style="color:#f1fa8c">&#34;prompt_template&#34;</span>: math_template
        },
        {
            <span style="color:#f1fa8c">&#34;name&#34;</span>: <span style="color:#f1fa8c">&#34;历史&#34;</span>,
            <span style="color:#f1fa8c">&#34;desc&#34;</span>: <span style="color:#f1fa8c">&#34;擅长回答历史问题&#34;</span>,
            <span style="color:#f1fa8c">&#34;prompt_template&#34;</span>: history_template
        },
        {
            <span style="color:#f1fa8c">&#34;name&#34;</span>: <span style="color:#f1fa8c">&#34;计算机科学&#34;</span>,
            <span style="color:#f1fa8c">&#34;desc&#34;</span>: <span style="color:#f1fa8c">&#34;擅长回答计算机科学问题&#34;</span>,
            <span style="color:#f1fa8c">&#34;prompt_template&#34;</span>: computerscience_template
        }
    ]

    <span style="color:#6272a4"># 7.基于提示模版信息创建对应的目的链</span>
    destination_chains <span style="color:#ff79c6">=</span> {}
    <span style="color:#ff79c6">for</span> p_info <span style="color:#ff79c6">in</span> prompt_infos:
        name <span style="color:#ff79c6">=</span> p_info[<span style="color:#f1fa8c">&#34;name&#34;</span>]
        prompt_tempalte <span style="color:#ff79c6">=</span> p_info[<span style="color:#f1fa8c">&#34;prompt_template&#34;</span>]
        prompt <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(
            template<span style="color:#ff79c6">=</span>prompt_tempalte)
        chain <span style="color:#ff79c6">=</span> LLMChain(
            llm<span style="color:#ff79c6">=</span>llm,
            prompt<span style="color:#ff79c6">=</span>prompt,
        )
        destination_chains[name] <span style="color:#ff79c6">=</span> chain

    destinations <span style="color:#ff79c6">=</span> [<span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{</span>p[<span style="color:#f1fa8c">&#39;name&#39;</span>]<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">: </span><span style="color:#f1fa8c">{</span>p[<span style="color:#f1fa8c">&#39;desc&#39;</span>]<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">&#34;</span> <span style="color:#ff79c6">for</span> p <span style="color:#ff79c6">in</span> prompt_infos]
    destinations_str <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span><span style="color:#ff79c6">.</span>join(destinations)

    <span style="color:#6272a4"># 8.创建默认目的链，类似路由表中的默认路由</span>
    default_prompt <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_template(<span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{input}</span><span style="color:#f1fa8c">&#34;</span>)
    default_chain <span style="color:#ff79c6">=</span> LLMChain(
        llm<span style="color:#ff79c6">=</span>llm,
        prompt<span style="color:#ff79c6">=</span>default_prompt,
    )

    <span style="color:#6272a4"># 9.定义不同链之间的路由模版，返回的格式要求为什么是destination和next_inputs，</span>
    <span style="color:#6272a4"># 跟RouterOutputParser相绑定的。为什么是4个花括号?因为要进行两次format，两个花括号相当于输出一个花括号</span>
    MULTI_PROMPT_ROUTER_TEMPLATE <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;
</span><span style="color:#f1fa8c">给语言模型一个原始文本输入,</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">让其选择最适合输入的模型提示.</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">系统将为您提供可用提示的名称以及最适合改提示的描述</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">如果你认为修改原始输入最终会导致语言模型做出更好的响应,</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">你也可以修改原始输入.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;&lt; 格式 &gt;&gt;
</span><span style="color:#f1fa8c">返回一个带有JSON对象的markdown代码片段, 该JSON对象的格式如下:
</span><span style="color:#f1fa8c">```json
</span><span style="color:#f1fa8c">{{{{
</span><span style="color:#f1fa8c">    &#34;destination&#34;: string 使用提示名字或者使用&#34;DEFAULT&#34;
</span><span style="color:#f1fa8c">    &#34;next_inputs&#34;: string 原始输入的改进版本
</span><span style="color:#f1fa8c">}}}}
</span><span style="color:#f1fa8c">```
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">记住：&#34;destination&#34;必须是下面指定的候选提示名称之一, </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">或者如果输入不太适合任何候选提示, </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">则可以是&#34;DEFAULT&#34;。</span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">记住:如果您认为不需要任何修改, </span><span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">则&#34;next_inputs&#34;可以只是原始输入。
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;&lt; 候选提示 &gt;&gt;
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">{destinations}</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;&lt; 输入 &gt;&gt;
</span><span style="color:#f1fa8c">{{input}}
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>

    <span style="color:#6272a4"># 10.构建路由链</span>
    router_template <span style="color:#ff79c6">=</span> MULTI_PROMPT_ROUTER_TEMPLATE<span style="color:#ff79c6">.</span>format(
        destinations<span style="color:#ff79c6">=</span>destinations_str
    )
    <span style="color:#8be9fd;font-style:italic">print</span>(router_template)
    <span style="color:#8be9fd;font-style:italic">print</span>(router_template)

    router_prompt <span style="color:#ff79c6">=</span> PromptTemplate(
        template<span style="color:#ff79c6">=</span>router_template,
        input_variables<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;input&#34;</span>],
    )
    router_prompt<span style="color:#ff79c6">.</span>output_parser <span style="color:#ff79c6">=</span> RouterOutputParser()

    router_chain <span style="color:#ff79c6">=</span> LLMRouterChain<span style="color:#ff79c6">.</span>from_llm(llm, router_prompt)

    <span style="color:#6272a4"># 11.创建多提示链</span>
    chain <span style="color:#ff79c6">=</span> MultiPromptChain(
        router_chain<span style="color:#ff79c6">=</span>router_chain,
        destination_chains<span style="color:#ff79c6">=</span>destination_chains,
        default_chain<span style="color:#ff79c6">=</span>default_chain,
        verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
    )

    <span style="color:#6272a4"># 12.运行路由链</span>
    <span style="color:#8be9fd;font-style:italic">print</span>(chain<span style="color:#ff79c6">.</span>invoke(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;1+3等于多少?&#34;</span>}))
    <span style="color:#8be9fd;font-style:italic">print</span>(chain<span style="color:#ff79c6">.</span>invoke(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;黑洞是什么?&#34;</span>}))
    <span style="color:#8be9fd;font-style:italic">print</span>(chain<span style="color:#ff79c6">.</span>invoke(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;五代十国是什么?&#34;</span>}))
    <span style="color:#8be9fd;font-style:italic">print</span>(chain<span style="color:#ff79c6">.</span>invoke(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;最流行的编程语言是什么?&#34;</span>}))
    <span style="color:#8be9fd;font-style:italic">print</span>(chain<span style="color:#ff79c6">.</span>invoke(<span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;你喜欢什么?&#34;</span>}))


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()

</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&gt; Entering new MultiPromptChain chain...
数学: {&#39;input&#39;: &#39;1 + 3 等于多少?&#39;}
&gt; Finished chain.
{&#39;input&#39;: &#39;1 + 3 等于多少?&#39;, &#39;text&#39;: &#39;1 + 3 = 4.&#39;}


&gt; Entering new MultiPromptChain chain...
物理学: {&#39;input&#39;: &#39;黑洞是什么?&#39;}
&gt; Finished chain.
{&#39;input&#39;: &#39;黑洞是什么?&#39;, &#39;text&#39;: &#39;黑洞是宇宙中一种非常密集的天体，它的引力非常强大，甚至连光都无法逃离它的吸引力。黑洞形成于恒星死亡时，其质量非常大，体积非常小，因此被称为“黑洞”。在黑洞的事件视界内，引力非常强大，甚至时间和空间都会被扭曲。目前科学家对黑洞的研究仍在进行中，仍有很多未解之谜。&#39;}


&gt; Entering new MultiPromptChain chain...
历史: {&#39;input&#39;: &#39;五代十国是中国历史上的一个时期，指的是五代时期和十国时期的合称。&#39;}
&gt; Finished chain.
{&#39;input&#39;: &#39;五代十国是中国历史上的一个时期，指的是五代时期和十国时期的合称。&#39;, &#39;text&#39;: &#39;请问你对五代十国时期的政治、经济和文化特点有什么深入的见解和分析？你认为这个时期对中国历史的发展有着怎样的影响？&#39;}


&gt; Entering new MultiPromptChain chain...
计算机科学: {&#39;input&#39;: &#39;最流行的编程语言是什么?&#39;}
&gt; Finished chain.
{&#39;input&#39;: &#39;最流行的编程语言是什么?&#39;, &#39;text&#39;: &#39;目前最流行的编程语言之一是Python。Python是一种简单易学、功能强大的编程语言，被广泛用于数据科学、人工智能、Web开发等领域。它具有丰富的库和工具，使得开发人员可以快速高效地完成各种任务。另外，JavaScript、Java、C++、C#等编程语言也在不同领域有着广泛的应用和较高的流行度。&#39;}


&gt; Entering new MultiPromptChain chain...
None: {&#39;input&#39;: &#39;你喜欢什么?&#39;}
&gt; Finished chain.
{&#39;input&#39;: &#39;你喜欢什么?&#39;, &#39;text&#39;: &#39;作为一个AI助手，我没有情感和喜好，我只是一个程序，可以帮助您解决问题和提供信息。请问有什么可以帮助您的吗？&#39;}
</code></pre></div><h3 id="检索问答链">检索问答链</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain_community.document_loaders.csv_loader <span style="color:#ff79c6">import</span> CSVLoader
<span style="color:#ff79c6">from</span> langchain_community.vectorstores.docarray <span style="color:#ff79c6">import</span> DocArrayInMemorySearch
<span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> OpenAIEmbeddings
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> RetrievalQA
<span style="color:#ff79c6">from</span> pydantic <span style="color:#ff79c6">import</span> SecretStr

api_key <span style="color:#ff79c6">=</span> SecretStr(<span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>)
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 使用LangChain文档加载器csv类型对数据进行导入, csv表格自造数据</span>
    file <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
    csv_loader <span style="color:#ff79c6">=</span> CSVLoader(file_path<span style="color:#ff79c6">=</span>file)
    docs <span style="color:#ff79c6">=</span> csv_loader<span style="color:#ff79c6">.</span>load()

    <span style="color:#6272a4"># 使用OpenAI的向量嵌入</span>
    embedding <span style="color:#ff79c6">=</span> OpenAIEmbeddings(
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)

    <span style="color:#6272a4"># 初始化向量存储，文档列表、向量嵌入作为参数</span>
    vector_db <span style="color:#ff79c6">=</span> DocArrayInMemorySearch<span style="color:#ff79c6">.</span>from_documents(docs, embedding)

    <span style="color:#6272a4"># 使用OpenAI语言模型</span>
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)

    <span style="color:#6272a4"># 使用检索问答链来回答问题，基于向量存储创建检索器</span>
    retriever <span style="color:#ff79c6">=</span> vector_db<span style="color:#ff79c6">.</span>as_retriever()

    <span style="color:#6272a4"># from_chain_type参数说明：</span>
    <span style="color:#6272a4"># llm：语言模型</span>
    <span style="color:#6272a4"># retriever：检索器</span>
    <span style="color:#6272a4"># chain_type：链类型</span>
    <span style="color:#6272a4"># chain_type = stuff, 是将所有查询得到的文档组合成一个文档传入下一步</span>
    <span style="color:#6272a4"># chain_type = map_reduce, 将所有块与问题一起传递给语言模型，获取回复，使用另一个语言模型调用将所有单独的回复总结成最终答案，它可以在任意数量的文档上运行。可以并行处理单个问题，同时也需要更多的调用。它将所有文档视为独立的</span>
    <span style="color:#6272a4"># chain_type = refine, 用于循环许多文档，际上是迭代的，建立在先前文档的答案之上，非常适合前后因果信息并随时间逐步构建答案，依赖于先前调用的结果。它通常需要更长的时间，并且基本上需要与map_reduce一样多的调用</span>
    <span style="color:#6272a4"># chain_type = map_rerank, 对每个文档进行单个语言模型调用，要求它返回一个分数，选择最高分，这依赖于语言模型知道分数应该是什么，需要告诉它，如果它与文档相关，则应该是高分</span>
    retrieva_qa <span style="color:#ff79c6">=</span> RetrievalQA<span style="color:#ff79c6">.</span>from_chain_type(
        llm<span style="color:#ff79c6">=</span>llm,
        retriever<span style="color:#ff79c6">=</span>retriever,
        chain_type<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;stuff&#34;</span>,
        verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
    )
    query <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;请用markdown表格的方式列出所有跟云相关的标题，并对每个标题进行抽象总结&#34;</span>
    result <span style="color:#ff79c6">=</span> retrieva_qa({<span style="color:#f1fa8c">&#34;query&#34;</span>: query})
    <span style="color:#8be9fd;font-style:italic">print</span>(result[<span style="color:#f1fa8c">&#39;result&#39;</span>])


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()

</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>| 标题                                       | 抽象总结                           |
|--------------------------------------------|------------------------------------|
| 一图带你看懂云原生                        | 通过图示解释云原生的概念和特点     |
| 云原生可观测平台国际产品调研          | 调研国际市场上的云原生可观测平台产品 |
| 云原生可观测平台国内产品调研          | 调研国内市场上的云原生可观测平台产品 |
| 云平台前端框架方案          | 探讨云平台前端框架解决方案 |
</code></pre><h2 id="基于文档的问答">基于文档的问答</h2>
<h3 id="直接使用向量存储查询">直接使用向量存储查询</h3>
<p>使用大语言模型构建一个基于给定文档和文档集合的问答系统是一种非常经典的应用场景。基于文档问答的这个实现，涉及到LangChain的其它组件，
比如：嵌入模型(Embedding Models)和向量存储。</p>
<blockquote>
<p>大型深度学习模型中的嵌入(Embedding)是指将高维度输入数据（如文本或图像）映射到低维度空间的向量表示。在自然语言处理（NLP）中，嵌入
通常用于将单词或短语映射到向量空间中的连续值，以便进行文本分类、情感分析、机器翻译等任务。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_community.document_loaders.csv_loader <span style="color:#ff79c6">import</span> CSVLoader
<span style="color:#ff79c6">from</span> langchain.indexes <span style="color:#ff79c6">import</span> VectorstoreIndexCreator
<span style="color:#ff79c6">from</span> langchain_community.vectorstores.docarray <span style="color:#ff79c6">import</span> DocArrayInMemorySearch
<span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> OpenAIEmbeddings
<span style="color:#ff79c6">from</span> pydantic <span style="color:#ff79c6">import</span> SecretStr

api_key <span style="color:#ff79c6">=</span> SecretStr(<span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>)
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 1.初始化语言模型</span>
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)

    <span style="color:#6272a4"># 2.使用LangChain文档加载器csv类型对数据进行导入, csv数据可以自行创造</span>
    file <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
    csv_loader <span style="color:#ff79c6">=</span> CSVLoader(file_path<span style="color:#ff79c6">=</span>file)

    <span style="color:#6272a4"># 3.基于文档加载器创建LangChain向量存储索引，这里使用向量内存存储</span>
    index <span style="color:#ff79c6">=</span> VectorstoreIndexCreator(
        vectorstore_cls<span style="color:#ff79c6">=</span>DocArrayInMemorySearch,
        embedding<span style="color:#ff79c6">=</span>OpenAIEmbeddings(
            api_key<span style="color:#ff79c6">=</span>api_key,
            base_url<span style="color:#ff79c6">=</span>openai_url))<span style="color:#ff79c6">.</span>from_loaders([csv_loader])

    <span style="color:#6272a4"># 4.查询创建的向量存储，问题要跟csv内容有所关联</span>
    query <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;请用markdown表格的方式列出所有跟云相关的标题，并对每个标题进行抽象总结&#34;</span>
    response <span style="color:#ff79c6">=</span> index<span style="color:#ff79c6">.</span>query(question<span style="color:#ff79c6">=</span>query, llm<span style="color:#ff79c6">=</span>llm)

    <span style="color:#8be9fd;font-style:italic">print</span>(response)


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()

</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>| 标题                        | 抽象总结         |
|----------------------------|-----------------|
| 一图带你看懂云原生            | 介绍云原生概念和特点   |
| 可观测平台国际产品调研         | 可观测平台国际产品调研 |
| 可观测平台国内产品调研         | 可观测平台国际产品调研 |
| 云平台前端框架方案            | 云平台前端框架方案介绍 |
</code></pre><h3 id="向量嵌入和向量存储">向量嵌入和向量存储</h3>
<p>大语言模型有上下文长度限制，直接处理长文档有点困难。要想实现长文档的问答，需引入向量嵌入(Embeddings)和向量存储(VectorStore)等技术；如何构建处理大规模长文档的问答系统？
1.使用Embeddings算法对文档进行向量化，语义相近的文本片段用相近的向量表示。
2.将向量化的文档切为小块，存入向量数据库；向量数据库对各文档片段进行索引，支持快速检索。</p>
<p>使用向量技术架构的话，当用户提问时，先将问题转化为向量，在向量数据库中快速查找到语义最相关的文档片段，然后再把这些文档片段和问题
一起发送给语言模型，返回生成的回答。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain_community.document_loaders.csv_loader <span style="color:#ff79c6">import</span> CSVLoader
<span style="color:#ff79c6">from</span> langchain_community.vectorstores.docarray <span style="color:#ff79c6">import</span> DocArrayInMemorySearch
<span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> OpenAIEmbeddings
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> RetrievalQA
<span style="color:#ff79c6">from</span> pydantic <span style="color:#ff79c6">import</span> SecretStr

api_key <span style="color:#ff79c6">=</span> SecretStr(<span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>)
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 使用LangChain文档加载器csv类型对数据进行导入</span>
    file <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
    csv_loader <span style="color:#ff79c6">=</span> CSVLoader(file_path<span style="color:#ff79c6">=</span>file)
    docs <span style="color:#ff79c6">=</span> csv_loader<span style="color:#ff79c6">.</span>load()

    <span style="color:#6272a4"># 使用OpenAI的向量嵌入</span>
    embedding <span style="color:#ff79c6">=</span> OpenAIEmbeddings(
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)

    <span style="color:#6272a4"># 初始化向量存储，文档列表、向量嵌入作为参数</span>
    vector_db <span style="color:#ff79c6">=</span> DocArrayInMemorySearch<span style="color:#ff79c6">.</span>from_documents(docs, embedding)

    <span style="color:#6272a4"># 返回一个文档列表，默认返回4个最相近语义的文档</span>
    docs <span style="color:#ff79c6">=</span> vector_db<span style="color:#ff79c6">.</span>similarity_search(<span style="color:#f1fa8c">&#34;推荐一篇跟存储相关的文章&#34;</span>)
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;返回文档的数量：</span><span style="color:#f1fa8c">{0}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span><span style="color:#ff79c6">.</span>format(<span style="color:#8be9fd;font-style:italic">len</span>(docs)))
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;第一个文档是：</span><span style="color:#f1fa8c">{0}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span><span style="color:#ff79c6">.</span>format(docs[<span style="color:#bd93f9">0</span>]))
    <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;第二个文档是：</span><span style="color:#f1fa8c">{0}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span><span style="color:#ff79c6">.</span>format(docs[<span style="color:#bd93f9">1</span>]))

    <span style="color:#6272a4"># 使用OpenAI语言模型</span>
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)

    <span style="color:#6272a4"># 把返回的文档列表，构造成提示发送给语言模型来回答</span>
    qdocs <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;</span><span style="color:#ff79c6">.</span>join([docs[i]<span style="color:#ff79c6">.</span>page_content <span style="color:#ff79c6">for</span> i <span style="color:#ff79c6">in</span> <span style="color:#8be9fd;font-style:italic">range</span>(<span style="color:#8be9fd;font-style:italic">len</span>(docs))])
    response <span style="color:#ff79c6">=</span> llm<span style="color:#ff79c6">.</span>invoke(
        <span style="color:#8be9fd;font-style:italic">input</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{</span>qdocs<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">问题：请用markdown表格的方式列出所有跟云相关的标题，并对每个标题进行抽象总结&#34;</span>)
    <span style="color:#8be9fd;font-style:italic">print</span>(response<span style="color:#ff79c6">.</span>content)

    <span style="color:#6272a4"># 使用检索问答链来回答问题，基于向量存储创建检索器</span>
    retriever <span style="color:#ff79c6">=</span> vector_db<span style="color:#ff79c6">.</span>as_retriever()

    retrieva_qa <span style="color:#ff79c6">=</span> RetrievalQA<span style="color:#ff79c6">.</span>from_chain_type(
        llm<span style="color:#ff79c6">=</span>llm,
        retriever<span style="color:#ff79c6">=</span>retriever,
        chain_type<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;stuff&#34;</span>,
        verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
    )
    query <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;请用markdown表格的方式列出所有跟云相关的标题，并对每个标题进行抽象总结&#34;</span>
    result <span style="color:#ff79c6">=</span> retrieva_qa({<span style="color:#f1fa8c">&#34;query&#34;</span>: query})
    <span style="color:#8be9fd;font-style:italic">print</span>(result[<span style="color:#f1fa8c">&#39;result&#39;</span>])


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()

</code></pre></div><p>输出：</p>
<pre tabindex="0"><code>返回文档的数量：4

第一个文档是：page_content='\ufeff标题: 一文快速部署并配置普罗米远端存储——VictoriaMetrics\n链接: https://xxx.com/teams/k100017/docs/f54d5dbc83f811eda4b5b6df5a597271?company_from=6df6b7dadb4311e880ee5254002b9121\n作者: xxx\n领域: 云原生' metadata={'source': '/Users/iceyao/Desktop/test_101.csv', 'row': 31}

第二个文档是：page_content='\ufeff标题: 高质量的技术分享应该包含哪些内容？\n链接: https://xxx.com/teams/k100017/docs/57540e2ef0da11eb8adaaad8bc976c66?company_from=6df6b7dadb4311e880ee5254002b9121\n作者: xxx\n领域: 工程规范' metadata={'source': '/Users/iceyao/Desktop/test_101.csv', 'row': 67}

| 标题                                                   | 抽象总结                               |
|--------------------------------------------------------|----------------------------------------|
| 一文快速部署并配置普罗米远端存储——VictoriaMetrics     | 部署和配置远端存储的快速指南             |
| 高质量的技术分享应该包含哪些内容？                       | 技术分享内容的要素和标准                 |
| 一文浅析kubernetes event及其持久化方案                  | 分析Kubernetes事件及其持久化解决方案     |
| 你不知道的Postman效率提升技巧                          | 提升使用Postman工具效率的技巧           |
</code></pre><p>这里用到了LangChain的检索问答链</p>
<h2 id="评估evaluation">评估Evaluation</h2>
<p>评估是检验语言模型问答质量的关键环节。评估可以检验语言模型在不同文档上的问答效果，还可以通过比较不同模型，选择最佳系统。此外，定期评估也可以检查模型质量的衰减。评估通常有两个目的：</p>
<ul>
<li>检验LLM应用是否达到了验收标准</li>
<li>分析改动对于LLM应用性能的影响</li>
</ul>
<p>基本的思路就是利用语言模型本身和链本身，来辅助评估其他的语言模型、链和应用程序。</p>
<p>excel样本数据：</p>
<table>
<thead>
<tr>
<th>产品名称</th>
<th>产品类型</th>
<th>产品简介</th>
<th>适用场景</th>
<th>融资主体</th>
<th>融资额度</th>
<th>融资期限</th>
<th>融资成本</th>
<th>担保方式</th>
<th>风险控制</th>
<th>优势</th>
<th>案例</th>
</tr>
</thead>
<tbody>
<tr>
<td>应收账款质押融资</td>
<td>动产融资</td>
<td>以应收账款为质押品获取融资</td>
<td>核心企业、中小企业</td>
<td>核心企业、中小企业</td>
<td>100万元以上</td>
<td>1个月-3年</td>
<td>5%-8%</td>
<td>应收账款质押、信用担保、保证担保等</td>
<td>应收账款真实性、债权清晰性、履约能力等</td>
<td>融资便捷、成本较低、提高资金利用率</td>
<td>某大型制造企业利用应收账款质押融资，获得了1000万元的流动资金，用于采购原材料，有效缓解了资金压力，促进生产经营。</td>
</tr>
<tr>
<td>仓单融资</td>
<td>动产融资</td>
<td>以仓单为质押品获取融资</td>
<td>核心企业、中小企业</td>
<td>核心企业、中小企业</td>
<td>100万元以上</td>
<td>1个月-3年</td>
<td>4%-7%</td>
<td>仓单质押、信用担保、保证担保等</td>
<td>货物真实性、权属清晰性、仓储安全等</td>
<td>融资便捷、成本较低、盘活存货资产</td>
<td>某贸易企业利用仓单融资，获得了500万元的流动资金，用于扩大进出口业务，提高了资金周转效率。</td>
</tr>
<tr>
<td>订单融资</td>
<td>信用融资</td>
<td>以订单为基础获取融资</td>
<td>核心企业、中小企业</td>
<td>核心企业、中小企业</td>
<td>100万元以上</td>
<td>1个月-1年</td>
<td>3%-6%</td>
<td>订单真实性、买方信用状况等</td>
<td>订单池管理、风险分散等</td>
<td>融资便捷、成本较低、提升供应链协同效率</td>
<td>某电商企业利用订单融资，获得了2000万元的流动资金，用于备货发货，满足了订单快速增长的需求。</td>
</tr>
<tr>
<td>动产抵押融资</td>
<td>动产融资</td>
<td>以动产（如设备、车辆等）为质押品获取融资</td>
<td>中小企业</td>
<td>中小企业</td>
<td>50万元以上</td>
<td>1个月-3年</td>
<td>5%-8%</td>
<td>动产抵押、信用担保、保证担保等</td>
<td>动产权属清晰性、评估价值等</td>
<td>融资便捷、提高资产利用率</td>
<td>某科技企业利用动产抵押融资，获得了100万元的流动资金，用于研发新产品，提升了企业竞争力。</td>
</tr>
<tr>
<td>保单融资</td>
<td>信用融资</td>
<td>以保单为质押品获取融资</td>
<td>核心企业、中小企业</td>
<td>核心企业、中小企业</td>
<td>100万元以上</td>
<td>1个月-1年</td>
<td>3%-6%</td>
<td>保单质押、信用担保、保证担保等</td>
<td>保单真实性、保费支付记录等</td>
<td>融资便捷、成本较低、盘活保单资产</td>
<td>某制造企业利用保单融资，获得了500万元的流动资金，用于采购原材料，降低了融资成本。</td>
</tr>
<tr>
<td>流水贷款</td>
<td>信用融资</td>
<td>以企业历史经营数据为基础获取融资</td>
<td>核心企业、中小企业</td>
<td>核心企业、中小企业</td>
<td>100万元以上</td>
<td>1个月-3年</td>
<td>4%-7%</td>
<td>企业财务数据、经营状况等</td>
<td>信用评级、风险监控等</td>
<td>融资便捷、无需抵押、手续简便</td>
<td>某零售企业利用流水贷款，获得了200万元的流动资金，</td>
</tr>
</tbody>
</table>
<h3 id="创建待评估的llm应用">创建待评估的LLM应用</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain_community.document_loaders.csv_loader <span style="color:#ff79c6">import</span> CSVLoader
<span style="color:#ff79c6">from</span> pydantic <span style="color:#ff79c6">import</span> SecretStr

api_key <span style="color:#ff79c6">=</span> SecretStr(<span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>)
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 使用LangChain文档加载器csv类型对数据进行导入</span>
    file <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
    csv_loader <span style="color:#ff79c6">=</span> CSVLoader(file_path<span style="color:#ff79c6">=</span>file)
    docs <span style="color:#ff79c6">=</span> csv_loader<span style="color:#ff79c6">.</span>load()

    <span style="color:#6272a4"># 使用OpenAI语言模型</span>
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)

    <span style="color:#6272a4"># 打印几条样本数据</span>
    <span style="color:#ff79c6">for</span> i <span style="color:#ff79c6">in</span> <span style="color:#8be9fd;font-style:italic">range</span>(<span style="color:#8be9fd;font-style:italic">len</span>(docs[<span style="color:#bd93f9">0</span>:<span style="color:#bd93f9">5</span>])):
        <span style="color:#8be9fd;font-style:italic">print</span>(docs[i]<span style="color:#ff79c6">.</span>page_content <span style="color:#ff79c6">+</span> <span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>)


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()

</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">产品名称: 应收账款质押融资
产品类型: 动产融资
产品简介: 以应收账款为质押品获取融资
适用场景: 核心企业、中小企业
融资主体: 核心企业、中小企业
融资额度: 100万元以上
融资期限: 1个月-3年
融资成本: 5%-8%
担保方式: 应收账款质押、信用担保、保证担保等
风险控制: 应收账款真实性、债权清晰性、履约能力等
优势: 融资便捷、成本较低、提高资金利用率
案例: 某大型制造企业利用应收账款质押融资，获得了1000万元的流动资金，用于采购原材料，有效缓解了资金压力，促进生产经营。

产品名称: 仓单融资
产品类型: 动产融资
产品简介: 以仓单为质押品获取融资
适用场景: 核心企业、中小企业
融资主体: 核心企业、中小企业
融资额度: 100万元以上
融资期限: 1个月-3年
融资成本: 4%-7%
担保方式: 仓单质押、信用担保、保证担保等
风险控制: 货物真实性、权属清晰性、仓储安全等
优势: 融资便捷、成本较低、盘活存货资产
案例: 某贸易企业利用仓单融资，获得了500万元的流动资金，用于扩大进出口业务，提高了资金周转效率。

产品名称: 订单融资
产品类型: 信用融资
产品简介: 以订单为基础获取融资
适用场景: 核心企业、中小企业
融资主体: 核心企业、中小企业
融资额度: 100万元以上
融资期限: 1个月-1年
融资成本: 3%-6%
担保方式: 订单真实性、买方信用状况等
风险控制: 订单池管理、风险分散等
优势: 融资便捷、成本较低、提升供应链协同效率
案例: 某电商企业利用订单融资，获得了2000万元的流动资金，用于备货发货，满足了订单快速增长的需求。

产品名称: 动产抵押融资
产品类型: 动产融资
产品简介: 以动产（如设备、车辆等）为质押品获取融资
适用场景: 中小企业
融资主体: 中小企业
融资额度: 50万元以上
融资期限: 1个月-3年
融资成本: 5%-8%
担保方式: 动产抵押、信用担保、保证担保等
风险控制: 动产权属清晰性、评估价值等
优势: 融资便捷、提高资产利用率
案例: 某科技企业利用动产抵押融资，获得了100万元的流动资金，用于研发新产品，提升了企业竞争力。

产品名称: 保单融资
产品类型: 信用融资
产品简介: 以保单为质押品获取融资
适用场景: 核心企业、中小企业
融资主体: 核心企业、中小企业
融资额度: 100万元以上
融资期限: 1个月-1年
融资成本: 3%-6%
担保方式: 保单质押、信用担保、保证担保等
风险控制: 保单真实性、保费支付记录等
优势: 融资便捷、成本较低、盘活保单资产
案例: 某制造企业利用保单融资，获得了500万元的流动资金，用于采购原材料，降低了融资成本。
</code></pre></div><h4 id="手动创建测试用例">手动创建测试用例</h4>
<p>这里文档格式是csv文件，CSVLoader对文件的每一行数据进行分割，根据输出的格式手动设置几条问答对</p>
<pre tabindex="0"><code>examples = [
    {
        &quot;query&quot;: &quot;仓单融资的产品的优点是什么?&quot;,
        &quot;answer&quot;: &quot;成本低、操作便捷&quot;
    },
    {
        &quot;query&quot;: &quot;订单融资产品适用哪些企业?&quot;,
        &quot;answer&quot;: &quot;有实力的核心企业，还有一些中小企业&quot;
    }
]
</code></pre><h4 id="llm自动生成测试用例">LLM自动生成测试用例</h4>
<p>一个模型评估的大致流程：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">手动创建问题和答案 -&gt; 使用LLM自动创建问答测试用例 -&gt; 使用同一个LLM回答 -&gt; 让另一个LLM进行答案判断
</code></pre></div><p>借助LangChain的<code>QAGenerateChain</code>可以自动创建大量问答测试集，自动化评估是LangChain框架的一大优势，极大降低开发RAG系统的门槛。
原生的QAGenerateChain只支持中文，这里需要继承下QAGenerateChain类，然后重写下from_llm方法</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain_community.document_loaders.csv_loader <span style="color:#ff79c6">import</span> CSVLoader
<span style="color:#ff79c6">from</span> langchain.evaluation.qa <span style="color:#ff79c6">import</span> QAGenerateChain
<span style="color:#ff79c6">from</span> langchain.base_language <span style="color:#ff79c6">import</span> BaseLanguageModel
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> PromptTemplate
<span style="color:#ff79c6">from</span> typing <span style="color:#ff79c6">import</span> Any

<span style="color:#ff79c6">from</span> pydantic <span style="color:#ff79c6">import</span> SecretStr

api_key <span style="color:#ff79c6">=</span> SecretStr(<span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>)
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>


template <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;You are a teacher coming up with questions to ask on a quiz.
</span><span style="color:#f1fa8c">Given the following document, please generate a question and answer based on that document.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Example Format:
</span><span style="color:#f1fa8c">&lt;Begin Document&gt;
</span><span style="color:#f1fa8c">...
</span><span style="color:#f1fa8c">&lt;End Document&gt;
</span><span style="color:#f1fa8c">QUESTION: question here
</span><span style="color:#f1fa8c">ANSWER: answer here
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">These questions should be detailed and be based explicitly on information in the document. Begin!
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;Begin Document&gt;
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">{doc}</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;End Document&gt;
</span><span style="color:#f1fa8c">请使用中文输出
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>
PROMPT <span style="color:#ff79c6">=</span> PromptTemplate(
    input_variables<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;doc&#34;</span>],
    template<span style="color:#ff79c6">=</span>template,
)


<span style="color:#6272a4"># 继承QAGenerateChain，重写from_llm方法</span>
<span style="color:#ff79c6">class</span> <span style="color:#50fa7b">ZhCNQAGenerateChain</span>(QAGenerateChain):
    <span style="color:#f1fa8c">&#34;&#34;&#34;LLM Chain for generating examples for question answering.&#34;&#34;&#34;</span>

    @classmethod
    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">from_llm</span>(cls, llm: BaseLanguageModel, <span style="color:#ff79c6">**</span>kwargs: Any) <span style="color:#ff79c6">-&gt;</span> QAGenerateChain:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Load QA Generate Chain from LLM.&#34;&#34;&#34;</span>
        <span style="color:#ff79c6">return</span> cls(llm<span style="color:#ff79c6">=</span>llm, prompt<span style="color:#ff79c6">=</span>PROMPT, <span style="color:#ff79c6">**</span>kwargs)


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 使用LangChain文档加载器csv类型对数据进行导入</span>
    file <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
    csv_loader <span style="color:#ff79c6">=</span> CSVLoader(file_path<span style="color:#ff79c6">=</span>file)
    docs <span style="color:#ff79c6">=</span> csv_loader<span style="color:#ff79c6">.</span>load()

    <span style="color:#6272a4"># 使用OpenAI语言模型</span>
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)


    <span style="color:#6272a4"># 创建ZhCNQAGenerateChain链</span>
    sample_qa_chain <span style="color:#ff79c6">=</span> ZhCNQAGenerateChain<span style="color:#ff79c6">.</span>from_llm(llm)

    <span style="color:#6272a4"># 调用apply方法自动创建问答对</span>
    examples <span style="color:#ff79c6">=</span> sample_qa_chain<span style="color:#ff79c6">.</span>apply([{<span style="color:#f1fa8c">&#34;doc&#34;</span>: t} <span style="color:#ff79c6">for</span> t <span style="color:#ff79c6">in</span> docs])

    <span style="color:#6272a4"># 打印问答对</span>
    <span style="color:#ff79c6">for</span> i <span style="color:#ff79c6">in</span> examples:
        <span style="color:#8be9fd;font-style:italic">print</span>(i[<span style="color:#f1fa8c">&#39;qa_pairs&#39;</span>])


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()

</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">{&#39;query&#39;: &#39;什么是“应收账款质押融资”产品的主要特点和优势？ &#39;, &#39;answer&#39;: &#39;“应收账款质押融资”产品的主要特点包括产品类型为动产融资，适用场景为核心企业和中小企业，融资额度为100万元以上，融资期限为1个月至3年，融资成本为5%-8%，担保方式包括应收账款质押、信用担保、保证担保等。其优势在于融资便捷、成本较低、提高资金利用率。&#39;}
{&#39;query&#39;: &#39;仓单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？&#39;, &#39;answer&#39;: &#39;仓单融资产品适用于核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。&#39;}
{&#39;query&#39;: &#39;什么是订单融资的产品简介和适用场景？融资额度和期限是多少？融资成本是多少？担保方式和风险控制措施是什么？&#39;, &#39;answer&#39;: &#39;订单融资是以订单为基础获取融资的信用融资产品，适用于核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年，融资成本为3%-6%。担保方式包括订单真实性和买方信用状况等，风险控制措施包括订单池管理和风险分散。&#39;}
{&#39;query&#39;: &#39;什么是动产抵押融资的产品类型和适用场景？&#39;, &#39;answer&#39;: &#39;动产抵押融资的产品类型是动产融资，适用场景是中小企业。&#39;}
{&#39;query&#39;: &#39;保单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？&#39;, &#39;answer&#39;: &#39;保单融资产品适用于核心企业和中小企业，融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年。&#39;}
{&#39;query&#39;: &#39;流水贷款的产品类型是什么？融资额度是多少？融资期限是多久？&#39;, &#39;answer&#39;: &#39;流水贷款的产品类型是信用融资，融资额度为100万元以上，融资期限为1个月至3年。&#39;}
</code></pre></div><h3 id="人工评估">人工评估</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#6272a4"># flake8: noqa</span>
<span style="color:#ff79c6">from</span> langchain.globals <span style="color:#ff79c6">import</span> set_debug
<span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain_community.document_loaders.csv_loader <span style="color:#ff79c6">import</span> CSVLoader
<span style="color:#ff79c6">from</span> langchain_community.vectorstores.docarray <span style="color:#ff79c6">import</span> DocArrayInMemorySearch
<span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> OpenAIEmbeddings
<span style="color:#ff79c6">from</span> langchain.indexes <span style="color:#ff79c6">import</span> VectorstoreIndexCreator
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> RetrievalQA
<span style="color:#ff79c6">from</span> langchain.evaluation.qa <span style="color:#ff79c6">import</span> QAGenerateChain
<span style="color:#ff79c6">from</span> langchain.base_language <span style="color:#ff79c6">import</span> BaseLanguageModel
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> PromptTemplate
<span style="color:#ff79c6">from</span> typing <span style="color:#ff79c6">import</span> Any

<span style="color:#ff79c6">from</span> pydantic <span style="color:#ff79c6">import</span> SecretStr

api_key <span style="color:#ff79c6">=</span> SecretStr(<span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>)
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>


template <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;You are a teacher coming up with questions to ask on a quiz.
</span><span style="color:#f1fa8c">Given the following document, please generate a question and answer based on that document.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Example Format:
</span><span style="color:#f1fa8c">&lt;Begin Document&gt;
</span><span style="color:#f1fa8c">...
</span><span style="color:#f1fa8c">&lt;End Document&gt;
</span><span style="color:#f1fa8c">QUESTION: question here
</span><span style="color:#f1fa8c">ANSWER: answer here
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">These questions should be detailed and be based explicitly on information in the document. Begin!
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;Begin Document&gt;
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">{doc}</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;End Document&gt;
</span><span style="color:#f1fa8c">请使用中文输出
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>
PROMPT <span style="color:#ff79c6">=</span> PromptTemplate(
    input_variables<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;doc&#34;</span>],
    template<span style="color:#ff79c6">=</span>template,
)

examples <span style="color:#ff79c6">=</span> [
    {
        <span style="color:#f1fa8c">&#39;query&#39;</span>: <span style="color:#f1fa8c">&#39;仓单融资的产品的优点是什么?&#39;</span>,
        <span style="color:#f1fa8c">&#39;answer&#39;</span>: <span style="color:#f1fa8c">&#39;成本低、操作便捷&#39;</span>
    },
    {
        <span style="color:#f1fa8c">&#39;query&#39;</span>: <span style="color:#f1fa8c">&#39;订单融资产品适用哪些企业?&#39;</span>,
        <span style="color:#f1fa8c">&#39;answer&#39;</span>: <span style="color:#f1fa8c">&#39;有实力的核心企业，还有一些中小企业&#39;</span>
    }
]


<span style="color:#6272a4"># 继承QAGenerateChain，重写from_llm方法</span>
<span style="color:#ff79c6">class</span> <span style="color:#50fa7b">ZhCNQAGenerateChain</span>(QAGenerateChain):
    <span style="color:#f1fa8c">&#34;&#34;&#34;LLM Chain for generating examples for question answering.&#34;&#34;&#34;</span>

    @classmethod
    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">from_llm</span>(cls, llm: BaseLanguageModel, <span style="color:#ff79c6">**</span>kwargs: Any) <span style="color:#ff79c6">-&gt;</span> QAGenerateChain:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Load QA Generate Chain from LLM.&#34;&#34;&#34;</span>
        <span style="color:#ff79c6">return</span> cls(llm<span style="color:#ff79c6">=</span>llm, prompt<span style="color:#ff79c6">=</span>PROMPT, <span style="color:#ff79c6">**</span>kwargs)


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 开启LangChain全局debug</span>
    set_debug(<span style="color:#ff79c6">True</span>)

    <span style="color:#6272a4"># 使用LangChain文档加载器csv类型对数据进行导入</span>
    file <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
    csv_loader <span style="color:#ff79c6">=</span> CSVLoader(file_path<span style="color:#ff79c6">=</span>file)
    docs <span style="color:#ff79c6">=</span> csv_loader<span style="color:#ff79c6">.</span>load()

    <span style="color:#6272a4"># 基于文档加载器创建LangChain向量存储索引，这里使用向量内存存储</span>
    index <span style="color:#ff79c6">=</span> VectorstoreIndexCreator(
        vectorstore_cls<span style="color:#ff79c6">=</span>DocArrayInMemorySearch,
        embedding<span style="color:#ff79c6">=</span>OpenAIEmbeddings(
            api_key<span style="color:#ff79c6">=</span>api_key,
            base_url<span style="color:#ff79c6">=</span>openai_url))<span style="color:#ff79c6">.</span>from_loaders([csv_loader])

    <span style="color:#6272a4"># 使用OpenAI语言模型</span>
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)

    <span style="color:#6272a4"># 创建检索QA链</span>
    retrieval_qa_chain <span style="color:#ff79c6">=</span> RetrievalQA<span style="color:#ff79c6">.</span>from_chain_type(
        llm<span style="color:#ff79c6">=</span>llm,
        retriever<span style="color:#ff79c6">=</span>index<span style="color:#ff79c6">.</span>vectorstore<span style="color:#ff79c6">.</span>as_retriever(),
        chain_type<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;stuff&#34;</span>,
        verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
    )

    <span style="color:#6272a4"># 创建ZhCNQAGenerateChain链</span>
    sample_qa_chain <span style="color:#ff79c6">=</span> ZhCNQAGenerateChain<span style="color:#ff79c6">.</span>from_llm(llm)

    <span style="color:#6272a4"># 调用apply方法自动创建问答对</span>
    llm_examples <span style="color:#ff79c6">=</span> sample_qa_chain<span style="color:#ff79c6">.</span>apply([{<span style="color:#f1fa8c">&#34;doc&#34;</span>: t} <span style="color:#ff79c6">for</span> t <span style="color:#ff79c6">in</span> docs])

    <span style="color:#6272a4"># 整合测试用例，将手动测试用例和LLM测试用例合并</span>
    llm_examples <span style="color:#ff79c6">=</span> [v <span style="color:#ff79c6">for</span> item <span style="color:#ff79c6">in</span> llm_examples <span style="color:#ff79c6">for</span> _, v <span style="color:#ff79c6">in</span> item<span style="color:#ff79c6">.</span>items()]
    new_examples <span style="color:#ff79c6">=</span> examples <span style="color:#ff79c6">+</span> llm_examples
    <span style="color:#6272a4"># 打印手动测试用例的第一个问题的LLM答案</span>
    <span style="color:#8be9fd;font-style:italic">print</span>(retrieval_qa_chain<span style="color:#ff79c6">.</span>invoke({<span style="color:#f1fa8c">&#34;query&#34;</span>: examples[<span style="color:#bd93f9">0</span>][<span style="color:#f1fa8c">&#39;query&#39;</span>]})) <span style="color:#6272a4"># type: ignore[misc]</span>


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()

</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>chain/start<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain<span style="color:#ff79c6">]</span> Entering Chain run with input:
<span style="color:#ff79c6">[</span>inputs<span style="color:#ff79c6">]</span>
<span style="color:#ff79c6">[</span>llm/start<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 2:llm:ChatOpenAI<span style="color:#ff79c6">]</span> Entering LLM run with input:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;prompts&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#f1fa8c">&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 应收账款质押融资\\n产品类型: 动产融资\\n产品简介: 以应收账款为质押品获取融资\\n适用场景: 核心企业、中小企业\\n融资主体: 核心企业、中小企业\\n融资额度: 100万元以上\\n融资期限: 1个月-3年\\n融资成本: 5%-8%\\n担保方式: 应收账款质押、信用担保、保证担保等\\n风险控制: 应收账款真实性、债权清晰性、履约能力等\\n优势: 融资便捷、成本较低、提高资金利用率\\n案例: 某大型制造企业利用应收账款质押融资，获得了1000万元的流动资金，用于采购原材料，有效缓解了资金压力，促进生产经营。&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 0}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
  <span style="color:#ff79c6">]</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/start<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 3:llm:ChatOpenAI<span style="color:#ff79c6">]</span> Entering LLM run with input:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;prompts&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#f1fa8c">&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 仓单融资\\n产品类型: 动产融资\\n产品简介: 以仓单为质押品获取融资\\n适用场景: 核心企业、中小企业\\n融资主体: 核心企业、中小企业\\n融资额度: 100万元以上\\n融资期限: 1个月-3年\\n融资成本: 4%-7%\\n担保方式: 仓单质押、信用担保、保证担保等\\n风险控制: 货物真实性、权属清晰性、仓储安全等\\n优势: 融资便捷、成本较低、盘活存货资产\\n案例: 某贸易企业利用仓单融资，获得了500万元的流动资金，用于扩大进出口业务，提高了资金周转效率。&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 1}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
  <span style="color:#ff79c6">]</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/start<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 4:llm:ChatOpenAI<span style="color:#ff79c6">]</span> Entering LLM run with input:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;prompts&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#f1fa8c">&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 订单融资\\n产品类型: 信用融资\\n产品简介: 以订单为基础获取融资\\n适用场景: 核心企业、中小企业\\n融资主体: 核心企业、中小企业\\n融资额度: 100万元以上\\n融资期限: 1个月-1年\\n融资成本: 3%-6%\\n担保方式: 订单真实性、买方信用状况等\\n风险控制: 订单池管理、风险分散等\\n优势: 融资便捷、成本较低、提升供应链协同效率\\n案例: 某电商企业利用订单融资，获得了2000万元的流动资金，用于备货发货，满足了订单快速增长的需求。&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 2}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
  <span style="color:#ff79c6">]</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/start<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 5:llm:ChatOpenAI<span style="color:#ff79c6">]</span> Entering LLM run with input:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;prompts&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#f1fa8c">&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 动产抵押融资\\n产品类型: 动产融资\\n产品简介: 以动产（如设备、车辆等）为质押品获取融资\\n适用场景: 中小企业\\n融资主体: 中小企业\\n融资额度: 50万元以上\\n融资期限: 1个月-3年\\n融资成本: 5%-8%\\n担保方式: 动产抵押、信用担保、保证担保等\\n风险控制: 动产权属清晰性、评估价值等\\n优势: 融资便捷、提高资产利用率\\n案例: 某科技企业利用动产抵押融资，获得了100万元的流动资金，用于研发新产品，提升了企业竞争力。&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 3}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
  <span style="color:#ff79c6">]</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/start<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 6:llm:ChatOpenAI<span style="color:#ff79c6">]</span> Entering LLM run with input:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;prompts&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#f1fa8c">&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 保单融资\\n产品类型: 信用融资\\n产品简介: 以保单为质押品获取融资\\n适用场景: 核心企业、中小企业\\n融资主体: 核心企业、中小企业\\n融资额度: 100万元以上\\n融资期限: 1个月-1年\\n融资成本: 3%-6%\\n担保方式: 保单质押、信用担保、保证担保等\\n风险控制: 保单真实性、保费支付记录等\\n优势: 融资便捷、成本较低、盘活保单资产\\n案例: 某制造企业利用保单融资，获得了500万元的流动资金，用于采购原材料，降低了融资成本。&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 4}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
  <span style="color:#ff79c6">]</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/start<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 7:llm:ChatOpenAI<span style="color:#ff79c6">]</span> Entering LLM run with input:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;prompts&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#f1fa8c">&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 流水贷款\\n产品类型: 信用融资\\n产品简介: 以企业历史经营数据为基础获取融资\\n适用场景: 核心企业、中小企业\\n融资主体: 核心企业、中小企业\\n融资额度: 100万元以上\\n融资期限: 1个月-3年\\n融资成本: 4%-7%\\n担保方式: 企业财务数据、经营状况等\\n风险控制: 信用评级、风险监控等\\n优势: 融资便捷、无需抵押、手续简便\\n案例: 某零售企业利用流水贷款，获得了200万元的流动资金，&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 5}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
  <span style="color:#ff79c6">]</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/end<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 2:llm:ChatOpenAI<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>17.97s<span style="color:#ff79c6">]</span> Exiting LLM run with output:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;generations&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#ff79c6">[</span>
      <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;text&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 什么是\&#34;应收账款质押融资\&#34;的产品类型？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？\nANSWER: \&#34;应收账款质押融资\&#34;的产品类型是动产融资。融资主体可以是核心企业或中小企业。融资额度为100万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括应收账款质押、信用担保、保证担保等。&#34;</span>,
        <span style="color:#f1fa8c">&#34;generation_info&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;finish_reason&#34;</span>: <span style="color:#f1fa8c">&#34;stop&#34;</span>,
          <span style="color:#f1fa8c">&#34;logprobs&#34;</span>: null
        <span style="color:#ff79c6">}</span>,
        <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;ChatGeneration&#34;</span>,
        <span style="color:#f1fa8c">&#34;message&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;lc&#34;</span>: 1,
          <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;constructor&#34;</span>,
          <span style="color:#f1fa8c">&#34;id&#34;</span>: <span style="color:#ff79c6">[</span>
            <span style="color:#f1fa8c">&#34;langchain&#34;</span>,
            <span style="color:#f1fa8c">&#34;schema&#34;</span>,
            <span style="color:#f1fa8c">&#34;messages&#34;</span>,
            <span style="color:#f1fa8c">&#34;AIMessage&#34;</span>
          <span style="color:#ff79c6">]</span>,
          <span style="color:#f1fa8c">&#34;kwargs&#34;</span>: <span style="color:#ff79c6">{</span>
            <span style="color:#f1fa8c">&#34;content&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 什么是\&#34;应收账款质押融资\&#34;的产品类型？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？\nANSWER: \&#34;应收账款质押融资\&#34;的产品类型是动产融资。融资主体可以是核心企业或中小企业。融资额度为100万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括应收账款质押、信用担保、保证担保等。&#34;</span>,
            <span style="color:#f1fa8c">&#34;additional_kwargs&#34;</span>: <span style="color:#ff79c6">{}</span>
          <span style="color:#ff79c6">}</span>
        <span style="color:#ff79c6">}</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">]</span>
  <span style="color:#ff79c6">]</span>,
  <span style="color:#f1fa8c">&#34;llm_output&#34;</span>: <span style="color:#ff79c6">{</span>
    <span style="color:#f1fa8c">&#34;token_usage&#34;</span>: <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;completion_tokens&#34;</span>: 179,
      <span style="color:#f1fa8c">&#34;prompt_tokens&#34;</span>: 402,
      <span style="color:#f1fa8c">&#34;total_tokens&#34;</span>: <span style="color:#bd93f9">581</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#f1fa8c">&#34;model_name&#34;</span>: <span style="color:#f1fa8c">&#34;gpt-3.5-turbo&#34;</span>,
    <span style="color:#f1fa8c">&#34;system_fingerprint&#34;</span>: <span style="color:#f1fa8c">&#34;fp_4f0b692a78&#34;</span>
  <span style="color:#ff79c6">}</span>,
  <span style="color:#f1fa8c">&#34;run&#34;</span>: null
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/end<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 3:llm:ChatOpenAI<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>17.97s<span style="color:#ff79c6">]</span> Exiting LLM run with output:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;generations&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#ff79c6">[</span>
      <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;text&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 仓单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？\nANSWER: 仓单融资产品适用于核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。&#34;</span>,
        <span style="color:#f1fa8c">&#34;generation_info&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;finish_reason&#34;</span>: <span style="color:#f1fa8c">&#34;stop&#34;</span>,
          <span style="color:#f1fa8c">&#34;logprobs&#34;</span>: null
        <span style="color:#ff79c6">}</span>,
        <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;ChatGeneration&#34;</span>,
        <span style="color:#f1fa8c">&#34;message&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;lc&#34;</span>: 1,
          <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;constructor&#34;</span>,
          <span style="color:#f1fa8c">&#34;id&#34;</span>: <span style="color:#ff79c6">[</span>
            <span style="color:#f1fa8c">&#34;langchain&#34;</span>,
            <span style="color:#f1fa8c">&#34;schema&#34;</span>,
            <span style="color:#f1fa8c">&#34;messages&#34;</span>,
            <span style="color:#f1fa8c">&#34;AIMessage&#34;</span>
          <span style="color:#ff79c6">]</span>,
          <span style="color:#f1fa8c">&#34;kwargs&#34;</span>: <span style="color:#ff79c6">{</span>
            <span style="color:#f1fa8c">&#34;content&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 仓单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？\nANSWER: 仓单融资产品适用于核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。&#34;</span>,
            <span style="color:#f1fa8c">&#34;additional_kwargs&#34;</span>: <span style="color:#ff79c6">{}</span>
          <span style="color:#ff79c6">}</span>
        <span style="color:#ff79c6">}</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">]</span>
  <span style="color:#ff79c6">]</span>,
  <span style="color:#f1fa8c">&#34;llm_output&#34;</span>: <span style="color:#ff79c6">{</span>
    <span style="color:#f1fa8c">&#34;token_usage&#34;</span>: <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;completion_tokens&#34;</span>: 175,
      <span style="color:#f1fa8c">&#34;prompt_tokens&#34;</span>: 379,
      <span style="color:#f1fa8c">&#34;total_tokens&#34;</span>: <span style="color:#bd93f9">554</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#f1fa8c">&#34;model_name&#34;</span>: <span style="color:#f1fa8c">&#34;gpt-3.5-turbo&#34;</span>,
    <span style="color:#f1fa8c">&#34;system_fingerprint&#34;</span>: <span style="color:#f1fa8c">&#34;fp_4f0b692a78&#34;</span>
  <span style="color:#ff79c6">}</span>,
  <span style="color:#f1fa8c">&#34;run&#34;</span>: null
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/end<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 4:llm:ChatOpenAI<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>17.97s<span style="color:#ff79c6">]</span> Exiting LLM run with output:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;generations&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#ff79c6">[</span>
      <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;text&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 产品名称为什么是订单融资？产品类型是什么？产品简介是什么？\nANSWER: 产品名称是订单融资，因为该产品是以订单为基础获取融资。产品类型是信用融资。产品简介是以订单为基础获取融资。&#34;</span>,
        <span style="color:#f1fa8c">&#34;generation_info&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;finish_reason&#34;</span>: <span style="color:#f1fa8c">&#34;stop&#34;</span>,
          <span style="color:#f1fa8c">&#34;logprobs&#34;</span>: null
        <span style="color:#ff79c6">}</span>,
        <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;ChatGeneration&#34;</span>,
        <span style="color:#f1fa8c">&#34;message&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;lc&#34;</span>: 1,
          <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;constructor&#34;</span>,
          <span style="color:#f1fa8c">&#34;id&#34;</span>: <span style="color:#ff79c6">[</span>
            <span style="color:#f1fa8c">&#34;langchain&#34;</span>,
            <span style="color:#f1fa8c">&#34;schema&#34;</span>,
            <span style="color:#f1fa8c">&#34;messages&#34;</span>,
            <span style="color:#f1fa8c">&#34;AIMessage&#34;</span>
          <span style="color:#ff79c6">]</span>,
          <span style="color:#f1fa8c">&#34;kwargs&#34;</span>: <span style="color:#ff79c6">{</span>
            <span style="color:#f1fa8c">&#34;content&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 产品名称为什么是订单融资？产品类型是什么？产品简介是什么？\nANSWER: 产品名称是订单融资，因为该产品是以订单为基础获取融资。产品类型是信用融资。产品简介是以订单为基础获取融资。&#34;</span>,
            <span style="color:#f1fa8c">&#34;additional_kwargs&#34;</span>: <span style="color:#ff79c6">{}</span>
          <span style="color:#ff79c6">}</span>
        <span style="color:#ff79c6">}</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">]</span>
  <span style="color:#ff79c6">]</span>,
  <span style="color:#f1fa8c">&#34;llm_output&#34;</span>: <span style="color:#ff79c6">{</span>
    <span style="color:#f1fa8c">&#34;token_usage&#34;</span>: <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;completion_tokens&#34;</span>: 87,
      <span style="color:#f1fa8c">&#34;prompt_tokens&#34;</span>: 362,
      <span style="color:#f1fa8c">&#34;total_tokens&#34;</span>: <span style="color:#bd93f9">449</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#f1fa8c">&#34;model_name&#34;</span>: <span style="color:#f1fa8c">&#34;gpt-3.5-turbo&#34;</span>,
    <span style="color:#f1fa8c">&#34;system_fingerprint&#34;</span>: <span style="color:#f1fa8c">&#34;fp_4f0b692a78&#34;</span>
  <span style="color:#ff79c6">}</span>,
  <span style="color:#f1fa8c">&#34;run&#34;</span>: null
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/end<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 5:llm:ChatOpenAI<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>17.97s<span style="color:#ff79c6">]</span> Exiting LLM run with output:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;generations&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#ff79c6">[</span>
      <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;text&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 什么是动产抵押融资的产品类型和适用场景？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？\nANSWER: 动产抵押融资的产品类型是动产融资，适用场景是中小企业。融资主体是中小企业，融资额度为50万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括动产抵押、信用担保、保证担保等。&#34;</span>,
        <span style="color:#f1fa8c">&#34;generation_info&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;finish_reason&#34;</span>: <span style="color:#f1fa8c">&#34;stop&#34;</span>,
          <span style="color:#f1fa8c">&#34;logprobs&#34;</span>: null
        <span style="color:#ff79c6">}</span>,
        <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;ChatGeneration&#34;</span>,
        <span style="color:#f1fa8c">&#34;message&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;lc&#34;</span>: 1,
          <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;constructor&#34;</span>,
          <span style="color:#f1fa8c">&#34;id&#34;</span>: <span style="color:#ff79c6">[</span>
            <span style="color:#f1fa8c">&#34;langchain&#34;</span>,
            <span style="color:#f1fa8c">&#34;schema&#34;</span>,
            <span style="color:#f1fa8c">&#34;messages&#34;</span>,
            <span style="color:#f1fa8c">&#34;AIMessage&#34;</span>
          <span style="color:#ff79c6">]</span>,
          <span style="color:#f1fa8c">&#34;kwargs&#34;</span>: <span style="color:#ff79c6">{</span>
            <span style="color:#f1fa8c">&#34;content&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 什么是动产抵押融资的产品类型和适用场景？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？\nANSWER: 动产抵押融资的产品类型是动产融资，适用场景是中小企业。融资主体是中小企业，融资额度为50万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括动产抵押、信用担保、保证担保等。&#34;</span>,
            <span style="color:#f1fa8c">&#34;additional_kwargs&#34;</span>: <span style="color:#ff79c6">{}</span>
          <span style="color:#ff79c6">}</span>
        <span style="color:#ff79c6">}</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">]</span>
  <span style="color:#ff79c6">]</span>,
  <span style="color:#f1fa8c">&#34;llm_output&#34;</span>: <span style="color:#ff79c6">{</span>
    <span style="color:#f1fa8c">&#34;token_usage&#34;</span>: <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;completion_tokens&#34;</span>: 181,
      <span style="color:#f1fa8c">&#34;prompt_tokens&#34;</span>: 367,
      <span style="color:#f1fa8c">&#34;total_tokens&#34;</span>: <span style="color:#bd93f9">548</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#f1fa8c">&#34;model_name&#34;</span>: <span style="color:#f1fa8c">&#34;gpt-3.5-turbo&#34;</span>,
    <span style="color:#f1fa8c">&#34;system_fingerprint&#34;</span>: <span style="color:#f1fa8c">&#34;fp_4f0b692a78&#34;</span>
  <span style="color:#ff79c6">}</span>,
  <span style="color:#f1fa8c">&#34;run&#34;</span>: null
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/end<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 6:llm:ChatOpenAI<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>17.98s<span style="color:#ff79c6">]</span> Exiting LLM run with output:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;generations&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#ff79c6">[</span>
      <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;text&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 保单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？\nANSWER: 保单融资产品适用于核心企业和中小企业，融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年。&#34;</span>,
        <span style="color:#f1fa8c">&#34;generation_info&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;finish_reason&#34;</span>: <span style="color:#f1fa8c">&#34;stop&#34;</span>,
          <span style="color:#f1fa8c">&#34;logprobs&#34;</span>: null
        <span style="color:#ff79c6">}</span>,
        <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;ChatGeneration&#34;</span>,
        <span style="color:#f1fa8c">&#34;message&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;lc&#34;</span>: 1,
          <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;constructor&#34;</span>,
          <span style="color:#f1fa8c">&#34;id&#34;</span>: <span style="color:#ff79c6">[</span>
            <span style="color:#f1fa8c">&#34;langchain&#34;</span>,
            <span style="color:#f1fa8c">&#34;schema&#34;</span>,
            <span style="color:#f1fa8c">&#34;messages&#34;</span>,
            <span style="color:#f1fa8c">&#34;AIMessage&#34;</span>
          <span style="color:#ff79c6">]</span>,
          <span style="color:#f1fa8c">&#34;kwargs&#34;</span>: <span style="color:#ff79c6">{</span>
            <span style="color:#f1fa8c">&#34;content&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 保单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？\nANSWER: 保单融资产品适用于核心企业和中小企业，融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年。&#34;</span>,
            <span style="color:#f1fa8c">&#34;additional_kwargs&#34;</span>: <span style="color:#ff79c6">{}</span>
          <span style="color:#ff79c6">}</span>
        <span style="color:#ff79c6">}</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">]</span>
  <span style="color:#ff79c6">]</span>,
  <span style="color:#f1fa8c">&#34;llm_output&#34;</span>: <span style="color:#ff79c6">{</span>
    <span style="color:#f1fa8c">&#34;token_usage&#34;</span>: <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;completion_tokens&#34;</span>: 114,
      <span style="color:#f1fa8c">&#34;prompt_tokens&#34;</span>: 364,
      <span style="color:#f1fa8c">&#34;total_tokens&#34;</span>: <span style="color:#bd93f9">478</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#f1fa8c">&#34;model_name&#34;</span>: <span style="color:#f1fa8c">&#34;gpt-3.5-turbo&#34;</span>,
    <span style="color:#f1fa8c">&#34;system_fingerprint&#34;</span>: <span style="color:#f1fa8c">&#34;fp_4f0b692a78&#34;</span>
  <span style="color:#ff79c6">}</span>,
  <span style="color:#f1fa8c">&#34;run&#34;</span>: null
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/end<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain &gt; 7:llm:ChatOpenAI<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>17.98s<span style="color:#ff79c6">]</span> Exiting LLM run with output:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;generations&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#ff79c6">[</span>
      <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;text&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 什么是流水贷款的产品类型和适用场景？\nANSWER: 流水贷款的产品类型是信用融资，适用场景是核心企业和中小企业。&#34;</span>,
        <span style="color:#f1fa8c">&#34;generation_info&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;finish_reason&#34;</span>: <span style="color:#f1fa8c">&#34;stop&#34;</span>,
          <span style="color:#f1fa8c">&#34;logprobs&#34;</span>: null
        <span style="color:#ff79c6">}</span>,
        <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;ChatGeneration&#34;</span>,
        <span style="color:#f1fa8c">&#34;message&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;lc&#34;</span>: 1,
          <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;constructor&#34;</span>,
          <span style="color:#f1fa8c">&#34;id&#34;</span>: <span style="color:#ff79c6">[</span>
            <span style="color:#f1fa8c">&#34;langchain&#34;</span>,
            <span style="color:#f1fa8c">&#34;schema&#34;</span>,
            <span style="color:#f1fa8c">&#34;messages&#34;</span>,
            <span style="color:#f1fa8c">&#34;AIMessage&#34;</span>
          <span style="color:#ff79c6">]</span>,
          <span style="color:#f1fa8c">&#34;kwargs&#34;</span>: <span style="color:#ff79c6">{</span>
            <span style="color:#f1fa8c">&#34;content&#34;</span>: <span style="color:#f1fa8c">&#34;QUESTION: 什么是流水贷款的产品类型和适用场景？\nANSWER: 流水贷款的产品类型是信用融资，适用场景是核心企业和中小企业。&#34;</span>,
            <span style="color:#f1fa8c">&#34;additional_kwargs&#34;</span>: <span style="color:#ff79c6">{}</span>
          <span style="color:#ff79c6">}</span>
        <span style="color:#ff79c6">}</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">]</span>
  <span style="color:#ff79c6">]</span>,
  <span style="color:#f1fa8c">&#34;llm_output&#34;</span>: <span style="color:#ff79c6">{</span>
    <span style="color:#f1fa8c">&#34;token_usage&#34;</span>: <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;completion_tokens&#34;</span>: 58,
      <span style="color:#f1fa8c">&#34;prompt_tokens&#34;</span>: 344,
      <span style="color:#f1fa8c">&#34;total_tokens&#34;</span>: <span style="color:#bd93f9">402</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#f1fa8c">&#34;model_name&#34;</span>: <span style="color:#f1fa8c">&#34;gpt-3.5-turbo&#34;</span>,
    <span style="color:#f1fa8c">&#34;system_fingerprint&#34;</span>: <span style="color:#f1fa8c">&#34;fp_4f0b692a78&#34;</span>
  <span style="color:#ff79c6">}</span>,
  <span style="color:#f1fa8c">&#34;run&#34;</span>: null
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>chain/end<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:ZhCNQAGenerateChain<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>18.01s<span style="color:#ff79c6">]</span> Exiting Chain run with output:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;outputs&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;qa_pairs&#34;</span>: <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;query&#34;</span>: <span style="color:#f1fa8c">&#34;什么是\&#34;应收账款质押融资\&#34;的产品类型？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？&#34;</span>,
        <span style="color:#f1fa8c">&#34;answer&#34;</span>: <span style="color:#f1fa8c">&#34;\&#34;应收账款质押融资\&#34;的产品类型是动产融资。融资主体可以是核心企业或中小企业。融资额度为100万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括应收账款质押、信用担保、保证担保等。&#34;</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;qa_pairs&#34;</span>: <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;query&#34;</span>: <span style="color:#f1fa8c">&#34;仓单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？&#34;</span>,
        <span style="color:#f1fa8c">&#34;answer&#34;</span>: <span style="color:#f1fa8c">&#34;仓单融资产品适用于核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。&#34;</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;qa_pairs&#34;</span>: <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;query&#34;</span>: <span style="color:#f1fa8c">&#34;产品名称为什么是订单融资？产品类型是什么？产品简介是什么？&#34;</span>,
        <span style="color:#f1fa8c">&#34;answer&#34;</span>: <span style="color:#f1fa8c">&#34;产品名称是订单融资，因为该产品是以订单为基础获取融资。产品类型是信用融资。产品简介是以订单为基础获取融资。&#34;</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;qa_pairs&#34;</span>: <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;query&#34;</span>: <span style="color:#f1fa8c">&#34;什么是动产抵押融资的产品类型和适用场景？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？&#34;</span>,
        <span style="color:#f1fa8c">&#34;answer&#34;</span>: <span style="color:#f1fa8c">&#34;动产抵押融资的产品类型是动产融资，适用场景是中小企业。融资主体是中小企业，融资额度为50万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括动产抵押、信用担保、保证担保等。&#34;</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;qa_pairs&#34;</span>: <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;query&#34;</span>: <span style="color:#f1fa8c">&#34;保单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？&#34;</span>,
        <span style="color:#f1fa8c">&#34;answer&#34;</span>: <span style="color:#f1fa8c">&#34;保单融资产品适用于核心企业和中小企业，融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年。&#34;</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;qa_pairs&#34;</span>: <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;query&#34;</span>: <span style="color:#f1fa8c">&#34;什么是流水贷款的产品类型和适用场景？&#34;</span>,
        <span style="color:#f1fa8c">&#34;answer&#34;</span>: <span style="color:#f1fa8c">&#34;流水贷款的产品类型是信用融资，适用场景是核心企业和中小企业。&#34;</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">}</span>
  <span style="color:#ff79c6">]</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>chain/start<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:RetrievalQA<span style="color:#ff79c6">]</span> Entering Chain run with input:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;query&#34;</span>: <span style="color:#f1fa8c">&#34;仓单融资的产品的优点是什么?&#34;</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>chain/start<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain<span style="color:#ff79c6">]</span> Entering Chain run with input:
<span style="color:#ff79c6">[</span>inputs<span style="color:#ff79c6">]</span>
<span style="color:#ff79c6">[</span>chain/start<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain<span style="color:#ff79c6">]</span> Entering Chain run with input:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;question&#34;</span>: <span style="color:#f1fa8c">&#34;仓单融资的产品的优点是什么?&#34;</span>,
  <span style="color:#f1fa8c">&#34;context&#34;</span>: <span style="color:#f1fa8c">&#34;产品名称: 仓单融资\n产品类型: 动产融资\n产品简介: 以仓单为质押品获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-3年\n融资成本: 4%-7%\n担保方式: 仓单质押、信用担保、保证担保等\n风险控制: 货物真实性、权属清晰性、仓储安全等\n优势: 融资便捷、成本较低、盘活存货资产\n案例: 某贸易企业利用仓单融资，获得了500万元的流动资金，用于扩大进出口业务，提高了资金周转效率。\n\n产品名称: 订单融资\n产品类型: 信用融资\n产品简介: 以订单为基础获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-1年\n融资成本: 3%-6%\n担保方式: 订单真实性、买方信用状况等\n风险控制: 订单池管理、风险分散等\n优势: 融资便捷、成本较低、提升供应链协同效率\n案例: 某电商企业利用订单融资，获得了2000万元的流动资金，用于备货发货，满足了订单快速增长的需求。\n\n产品名称: 保单融资\n产品类型: 信用融资\n产品简介: 以保单为质押品获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-1年\n融资成本: 3%-6%\n担保方式: 保单质押、信用担保、保证担保等\n风险控制: 保单真实性、保费支付记录等\n优势: 融资便捷、成本较低、盘活保单资产\n案例: 某制造企业利用保单融资，获得了500万元的流动资金，用于采购原材料，降低了融资成本。\n\n产品名称: 动产抵押融资\n产品类型: 动产融资\n产品简介: 以动产（如设备、车辆等）为质押品获取融资\n适用场景: 中小企业\n融资主体: 中小企业\n融资额度: 50万元以上\n融资期限: 1个月-3年\n融资成本: 5%-8%\n担保方式: 动产抵押、信用担保、保证担保等\n风险控制: 动产权属清晰性、评估价值等\n优势: 融资便捷、提高资产利用率\n案例: 某科技企业利用动产抵押融资，获得了100万元的流动资金，用于研发新产品，提升了企业竞争力。&#34;</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/start<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain &gt; 5:llm:ChatOpenAI<span style="color:#ff79c6">]</span> Entering LLM run with input:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;prompts&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#f1fa8c">&#34;System: Use the following pieces of context to answer the user&#39;s question. \nIf you don&#39;t know the answer, just say that you don&#39;t know, don&#39;t try to make up an answer.\n----------------\n产品名称: 仓单融资\n产品类型: 动产融资\n产品简介: 以仓单为质押品获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-3年\n融资成本: 4%-7%\n担保方式: 仓单质押、信用担保、保证担保等\n风险控制: 货物真实性、权属清晰性、仓储安全等\n优势: 融资便捷、成本较低、盘活存货资产\n案例: 某贸易企业利用仓单融资，获得了500万元的流动资金，用于扩大进出口业务，提高了资金周转效率。\n\n产品名称: 订单融资\n产品类型: 信用融资\n产品简介: 以订单为基础获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-1年\n融资成本: 3%-6%\n担保方式: 订单真实性、买方信用状况等\n风险控制: 订单池管理、风险分散等\n优势: 融资便捷、成本较低、提升供应链协同效率\n案例: 某电商企业利用订单融资，获得了2000万元的流动资金，用于备货发货，满足了订单快速增长的需求。\n\n产品名称: 保单融资\n产品类型: 信用融资\n产品简介: 以保单为质押品获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-1年\n融资成本: 3%-6%\n担保方式: 保单质押、信用担保、保证担保等\n风险控制: 保单真实性、保费支付记录等\n优势: 融资便捷、成本较低、盘活保单资产\n案例: 某制造企业利用保单融资，获得了500万元的流动资金，用于采购原材料，降低了融资成本。\n\n产品名称: 动产抵押融资\n产品类型: 动产融资\n产品简介: 以动产（如设备、车辆等）为质押品获取融资\n适用场景: 中小企业\n融资主体: 中小企业\n融资额度: 50万元以上\n融资期限: 1个月-3年\n融资成本: 5%-8%\n担保方式: 动产抵押、信用担保、保证担保等\n风险控制: 动产权属清晰性、评估价值等\n优势: 融资便捷、提高资产利用率\n案例: 某科技企业利用动产抵押融资，获得了100万元的流动资金，用于研发新产品，提升了企业竞争力。\nHuman: 仓单融资的产品的优点是什么?&#34;</span>
  <span style="color:#ff79c6">]</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>llm/end<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain &gt; 5:llm:ChatOpenAI<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1.78s<span style="color:#ff79c6">]</span> Exiting LLM run with output:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;generations&#34;</span>: <span style="color:#ff79c6">[</span>
    <span style="color:#ff79c6">[</span>
      <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;text&#34;</span>: <span style="color:#f1fa8c">&#34;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#34;</span>,
        <span style="color:#f1fa8c">&#34;generation_info&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;finish_reason&#34;</span>: <span style="color:#f1fa8c">&#34;stop&#34;</span>,
          <span style="color:#f1fa8c">&#34;logprobs&#34;</span>: null
        <span style="color:#ff79c6">}</span>,
        <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;ChatGeneration&#34;</span>,
        <span style="color:#f1fa8c">&#34;message&#34;</span>: <span style="color:#ff79c6">{</span>
          <span style="color:#f1fa8c">&#34;lc&#34;</span>: 1,
          <span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;constructor&#34;</span>,
          <span style="color:#f1fa8c">&#34;id&#34;</span>: <span style="color:#ff79c6">[</span>
            <span style="color:#f1fa8c">&#34;langchain&#34;</span>,
            <span style="color:#f1fa8c">&#34;schema&#34;</span>,
            <span style="color:#f1fa8c">&#34;messages&#34;</span>,
            <span style="color:#f1fa8c">&#34;AIMessage&#34;</span>
          <span style="color:#ff79c6">]</span>,
          <span style="color:#f1fa8c">&#34;kwargs&#34;</span>: <span style="color:#ff79c6">{</span>
            <span style="color:#f1fa8c">&#34;content&#34;</span>: <span style="color:#f1fa8c">&#34;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#34;</span>,
            <span style="color:#f1fa8c">&#34;additional_kwargs&#34;</span>: <span style="color:#ff79c6">{}</span>
          <span style="color:#ff79c6">}</span>
        <span style="color:#ff79c6">}</span>
      <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">]</span>
  <span style="color:#ff79c6">]</span>,
  <span style="color:#f1fa8c">&#34;llm_output&#34;</span>: <span style="color:#ff79c6">{</span>
    <span style="color:#f1fa8c">&#34;token_usage&#34;</span>: <span style="color:#ff79c6">{</span>
      <span style="color:#f1fa8c">&#34;completion_tokens&#34;</span>: 40,
      <span style="color:#f1fa8c">&#34;prompt_tokens&#34;</span>: 1065,
      <span style="color:#f1fa8c">&#34;total_tokens&#34;</span>: <span style="color:#bd93f9">1105</span>
    <span style="color:#ff79c6">}</span>,
    <span style="color:#f1fa8c">&#34;model_name&#34;</span>: <span style="color:#f1fa8c">&#34;gpt-3.5-turbo&#34;</span>,
    <span style="color:#f1fa8c">&#34;system_fingerprint&#34;</span>: <span style="color:#f1fa8c">&#34;fp_4f0b692a78&#34;</span>
  <span style="color:#ff79c6">}</span>,
  <span style="color:#f1fa8c">&#34;run&#34;</span>: null
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>chain/end<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1.78s<span style="color:#ff79c6">]</span> Exiting Chain run with output:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;text&#34;</span>: <span style="color:#f1fa8c">&#34;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#34;</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>chain/end<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1.80s<span style="color:#ff79c6">]</span> Exiting Chain run with output:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;output_text&#34;</span>: <span style="color:#f1fa8c">&#34;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#34;</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">[</span>chain/end<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>1:chain:RetrievalQA<span style="color:#ff79c6">]</span> <span style="color:#ff79c6">[</span>2.90s<span style="color:#ff79c6">]</span> Exiting Chain run with output:
<span style="color:#ff79c6">{</span>
  <span style="color:#f1fa8c">&#34;result&#34;</span>: <span style="color:#f1fa8c">&#34;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#34;</span>
<span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">{</span><span style="color:#f1fa8c">&#39;query&#39;</span>: <span style="color:#f1fa8c">&#39;仓单融资的产品的优点是什么?&#39;</span>, <span style="color:#f1fa8c">&#39;result&#39;</span>: <span style="color:#f1fa8c">&#39;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#39;</span><span style="color:#ff79c6">}</span>
</code></pre></div><p>设置全局debug后，可以看到整个上下文检索的过程，还可以看到token的消耗情况；最终得到的答案比手动测试用例多了<code>盘活存货资产</code>的描述</p>
<h3 id="使用llm进行评估">使用LLM进行评估</h3>
<p>用openai语言模型生成问答对，并回答这些问题；用ollama后端的qwen:7b语言模型进行答案判断</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#6272a4"># flake8: noqa</span>
<span style="color:#ff79c6">from</span> langchain.globals <span style="color:#ff79c6">import</span> set_debug
<span style="color:#ff79c6">from</span> langchain_openai.chat_models <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain_community.llms.ollama <span style="color:#ff79c6">import</span> Ollama
<span style="color:#ff79c6">from</span> langchain_community.document_loaders.csv_loader <span style="color:#ff79c6">import</span> CSVLoader
<span style="color:#ff79c6">from</span> langchain_community.vectorstores.docarray <span style="color:#ff79c6">import</span> DocArrayInMemorySearch
<span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> OpenAIEmbeddings
<span style="color:#ff79c6">from</span> langchain.indexes <span style="color:#ff79c6">import</span> VectorstoreIndexCreator
<span style="color:#ff79c6">from</span> langchain.chains <span style="color:#ff79c6">import</span> RetrievalQA
<span style="color:#ff79c6">from</span> langchain.evaluation.qa <span style="color:#ff79c6">import</span> QAGenerateChain, QAEvalChain
<span style="color:#ff79c6">from</span> langchain.base_language <span style="color:#ff79c6">import</span> BaseLanguageModel
<span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> PromptTemplate
<span style="color:#ff79c6">from</span> typing <span style="color:#ff79c6">import</span> Any

<span style="color:#ff79c6">from</span> pydantic <span style="color:#ff79c6">import</span> SecretStr

api_key <span style="color:#ff79c6">=</span> SecretStr(<span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>)
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
ollama_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;http://127.0.0.1:11434&#34;</span>


template <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;&#34;&#34;You are a teacher coming up with questions to ask on a quiz.
</span><span style="color:#f1fa8c">Given the following document, please generate a question and answer based on that document.
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">Example Format:
</span><span style="color:#f1fa8c">&lt;Begin Document&gt;
</span><span style="color:#f1fa8c">...
</span><span style="color:#f1fa8c">&lt;End Document&gt;
</span><span style="color:#f1fa8c">QUESTION: question here
</span><span style="color:#f1fa8c">ANSWER: answer here
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">These questions should be detailed and be based explicitly on information in the document. Begin!
</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;Begin Document&gt;
</span><span style="color:#f1fa8c"></span><span style="color:#f1fa8c">{doc}</span><span style="color:#f1fa8c">
</span><span style="color:#f1fa8c">&lt;End Document&gt;
</span><span style="color:#f1fa8c">请使用中文输出
</span><span style="color:#f1fa8c">&#34;&#34;&#34;</span>
PROMPT <span style="color:#ff79c6">=</span> PromptTemplate(
    input_variables<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;doc&#34;</span>],
    template<span style="color:#ff79c6">=</span>template,
)

<span style="color:#6272a4"># 继承QAGenerateChain，重写from_llm方法</span>
<span style="color:#ff79c6">class</span> <span style="color:#50fa7b">ZhCNQAGenerateChain</span>(QAGenerateChain):
    <span style="color:#f1fa8c">&#34;&#34;&#34;LLM Chain for generating examples for question answering.&#34;&#34;&#34;</span>

    @classmethod
    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">from_llm</span>(cls, llm: BaseLanguageModel, <span style="color:#ff79c6">**</span>kwargs: Any) <span style="color:#ff79c6">-&gt;</span> QAGenerateChain:
        <span style="color:#f1fa8c">&#34;&#34;&#34;Load QA Generate Chain from LLM.&#34;&#34;&#34;</span>
        <span style="color:#ff79c6">return</span> cls(llm<span style="color:#ff79c6">=</span>llm, prompt<span style="color:#ff79c6">=</span>PROMPT, <span style="color:#ff79c6">**</span>kwargs)


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 关闭LangChain全局debug</span>
    set_debug(<span style="color:#ff79c6">False</span>)

    <span style="color:#6272a4"># 1.使用LangChain文档加载器csv类型对数据进行导入</span>
    file <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
    csv_loader <span style="color:#ff79c6">=</span> CSVLoader(file_path<span style="color:#ff79c6">=</span>file)
    docs <span style="color:#ff79c6">=</span> csv_loader<span style="color:#ff79c6">.</span>load()

    <span style="color:#6272a4"># 2.基于文档加载器创建LangChain向量存储索引，这里使用向量内存存储</span>
    index <span style="color:#ff79c6">=</span> VectorstoreIndexCreator(
        vectorstore_cls<span style="color:#ff79c6">=</span>DocArrayInMemorySearch,
        embedding<span style="color:#ff79c6">=</span>OpenAIEmbeddings(
            api_key<span style="color:#ff79c6">=</span>api_key,
            base_url<span style="color:#ff79c6">=</span>openai_url))<span style="color:#ff79c6">.</span>from_loaders([csv_loader])

    <span style="color:#6272a4"># 3.声明OpenAI语言模型，用于自动生成LLM问答用例</span>
    openai_llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        api_key<span style="color:#ff79c6">=</span>api_key,
        base_url<span style="color:#ff79c6">=</span>openai_url)

    <span style="color:#6272a4"># 4.声明ollama模型(实际是llama2:13b)，用于评估问答答案</span>
    ollama_llm <span style="color:#ff79c6">=</span> Ollama(base_url<span style="color:#ff79c6">=</span>ollama_url,
                        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0</span>,
                        model<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;qwen:7b&#34;</span>
                        )

    <span style="color:#6272a4"># 5.声明检索QA链</span>
    retrieval_qa_chain <span style="color:#ff79c6">=</span> RetrievalQA<span style="color:#ff79c6">.</span>from_chain_type(
        llm<span style="color:#ff79c6">=</span>openai_llm,
        retriever<span style="color:#ff79c6">=</span>index<span style="color:#ff79c6">.</span>vectorstore<span style="color:#ff79c6">.</span>as_retriever(),
        chain_type<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;stuff&#34;</span>,
        verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
    )

    <span style="color:#6272a4"># 6.声明ZhCNQAGenerateChain链，基于QA生成链</span>
    qa_generate_chain <span style="color:#ff79c6">=</span> ZhCNQAGenerateChain<span style="color:#ff79c6">.</span>from_llm(openai_llm)

    <span style="color:#6272a4"># 7.ZhCNQAGenerateChain链调用apply方法自动创建问答对</span>
    llm_examples <span style="color:#ff79c6">=</span> qa_generate_chain<span style="color:#ff79c6">.</span>apply([{<span style="color:#f1fa8c">&#34;doc&#34;</span>: t} <span style="color:#ff79c6">for</span> t <span style="color:#ff79c6">in</span> docs])

    examples <span style="color:#ff79c6">=</span> [v <span style="color:#ff79c6">for</span> item <span style="color:#ff79c6">in</span> llm_examples <span style="color:#ff79c6">for</span> _, v <span style="color:#ff79c6">in</span> item<span style="color:#ff79c6">.</span>items()]

    <span style="color:#6272a4"># 8.检索QA链为测试用例生成预测</span>
    predictions <span style="color:#ff79c6">=</span> retrieval_qa_chain<span style="color:#ff79c6">.</span>batch(examples) <span style="color:#6272a4"># type: ignore[misc]</span>

    <span style="color:#6272a4"># 9.声明QA评估链</span>
    qa_eval_chain <span style="color:#ff79c6">=</span> QAEvalChain<span style="color:#ff79c6">.</span>from_llm(ollama_llm)

    <span style="color:#6272a4"># 10.QA评估链对其进行评估</span>
    evaluate_results <span style="color:#ff79c6">=</span> qa_eval_chain<span style="color:#ff79c6">.</span>evaluate(examples, predictions) <span style="color:#6272a4"># type: ignore[misc]</span>

    <span style="color:#ff79c6">for</span> i, _ <span style="color:#ff79c6">in</span> <span style="color:#8be9fd;font-style:italic">enumerate</span>(examples):
        <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;Example </span><span style="color:#f1fa8c">{</span>i<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">:&#34;</span>)
        <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;Question: &#34;</span> <span style="color:#ff79c6">+</span> predictions[i][<span style="color:#f1fa8c">&#39;query&#39;</span>])
        <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;Real Answer: &#34;</span> <span style="color:#ff79c6">+</span> predictions[i][<span style="color:#f1fa8c">&#39;answer&#39;</span>])
        <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;Predicted Answer: &#34;</span> <span style="color:#ff79c6">+</span> predictions[i][<span style="color:#f1fa8c">&#39;result&#39;</span>])
        <span style="color:#8be9fd;font-style:italic">print</span>(<span style="color:#f1fa8c">&#34;Predicted Grade: &#34;</span> <span style="color:#ff79c6">+</span> evaluate_results[i][<span style="color:#f1fa8c">&#39;results&#39;</span>])
        <span style="color:#8be9fd;font-style:italic">print</span>()


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()

</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&gt; Entering new RetrievalQA chain...


&gt; Entering new RetrievalQA chain...


&gt; Entering new RetrievalQA chain...


&gt; Entering new RetrievalQA chain...


&gt; Entering new RetrievalQA chain...


&gt; Entering new RetrievalQA chain...

&gt; Finished chain.

&gt; Finished chain.

&gt; Finished chain.

&gt; Finished chain.

&gt; Finished chain.

&gt; Finished chain.
Example 0:
Question: 什么是“应收账款质押融资”产品的主要特点和优势？请列举至少三点。
Real Answer: 该产品的主要特点和优势包括：以应收账款为质押品获取融资、适用于核心企业和中小企业、融资额度在100万元以上、融资期限为1个月至3年、融资成本在5%-8%之间、担保方式包括应收账款质押、信用担保、保证担保等、风险控制主要关注应收账款真实性、债权清晰性、履约能力等、优势在于融资便捷、成本较低、提高资金利用率。
Predicted Answer: “应收账款质押融资”产品的主要特点和优势包括：

1. **融资便捷**：通过将应收账款作为质押品，企业可以相对容易地获取融资，无需进行繁琐的审批流程，提高了融资的速度和效率。

2. **成本较低**：相比其他融资方式，应收账款质押融资的成本通常在5%-8%之间，相对较低，有助于降低企业的融资成本，提升盈利能力。

3. **提高资金利用率**：通过将应收账款作为质押品获得融资，企业可以有效地利用未来的收款权益，提前获取资金用于业务发展，提高了资金的利用效率和灵活性。
Predicted Grade: CORRECT


Example 1:
Question: 仓单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？
Real Answer: 仓单融资产品适用于核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。
Predicted Answer: 仓单融资产品的适用场景是核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度是100万元以上，融资期限为1个月到3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。
Predicted Grade: CORRECT


Example 2:
Question: 什么是订单融资的产品简介和适用场景？融资额度和期限是多少？融资成本是多少？担保方式和风险控制措施是什么？
Real Answer: 订单融资是以订单为基础获取融资的信用融资产品，适用于核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年，融资成本为3%-6%。担保方式包括订单真实性和买方信用状况等，风险控制措施包括订单池管理和风险分散。
Predicted Answer: 订单融资的产品简介是以订单为基础获取融资，适用场景是核心企业和中小企业。融资额度是100万元以上，融资期限为1个月到1年，融资成本为3%-6%。担保方式包括订单真实性和买方信用状况等，风险控制措施包括订单池管理和风险分散等。
Predicted Grade: CORRECT


Example 3:
Question: 什么是动产抵押融资的产品简介？ 
Real Answer: 以动产（如设备、车辆等）为质押品获取融资
Predicted Answer: 动产抵押融资的产品简介是以动产（如设备、车辆等）作为质押品来获取融资。
Predicted Grade: CORRECT


Example 4:
Question: 保单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？
Real Answer: 保单融资产品适用于核心企业和中小企业，融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年，融资成本为3%-6%。担保方式包括保单质押、信用担保、保证担保等。
Predicted Answer: 保单融资产品的适用场景是核心企业和中小企业。融资主体是核心企业和中小企业。融资额度是100万元以上，融资期限是1个月到1年。融资成本是3%-6%。担保方式包括保单质押、信用担保、保证担保等。
Predicted Grade: CORRECT


Example 5:
Question: 请问流水贷款的产品类型是什么？
Real Answer: 信用融资
Predicted Answer: 流水贷款的产品类型是信用融资。
Predicted Grade: CORRECT
</code></pre></div><p>从输出结果来看的话，每一个Example中包含了Question、Real Answer、Predicted Answer、Predicted Grade，Real Answer是
QA生成链基于openai语言模型生成的，Real Answer是QA检索链基于openai语言模型回答的，Predicted Grade是QA评估链基于qwen:7b语言模型
生成的。全自动的评估方式极大地简化了问答系统的评估和优化过程，开发者无需手动准备测试用例，也无需逐一判断正确性。</p>
<h2 id="代理agent">代理Agent</h2>
<p>代理作为语言模型的外部模块，可提供计算、逻辑、检索等功能的支持，使语言模型获得异常强大的推理和获取信息的超能力。LangChain的agent跟AI agent不是同一个概念。</p>
<p>AI agent、大模型、LangChain之间的关系？
AI agent是一种能够感知环境、进行决策和执行动作的智能实体。大模型相当于是AI agent的大脑，LangChain是快速构建AI agent的框架平台。AI agent～=大模型+插件+执行流程，对应人体的控制端、感知端、执行端</p>
<p>Agent类型区别：<a href="https://python.langchain.com/docs/modules/agents/agent_types/">https://python.langchain.com/docs/modules/agents/agent_types/</a></p>
<h3 id="使用llm-mathwikipedia工具">使用llm-math/wikipedia工具</h3>
<p>使用代理，需要满足三个条件：</p>
<ul>
<li>一个基础的LLM</li>
<li>进行交互的工具Tools</li>
<li>控制交互的代理Agents</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.agents <span style="color:#ff79c6">import</span> AgentExecutor, create_openai_tools_agent, load_tools
<span style="color:#ff79c6">from</span> pydantic <span style="color:#ff79c6">import</span> SecretStr
<span style="color:#ff79c6">from</span> langchain <span style="color:#ff79c6">import</span> hub

api_key <span style="color:#ff79c6">=</span> SecretStr(<span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>)
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    <span style="color:#6272a4"># 初始化一个基础的LLM</span>
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        base_url<span style="color:#ff79c6">=</span>openai_url,
        api_key<span style="color:#ff79c6">=</span>api_key,
        streaming<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
        verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
    )

    <span style="color:#6272a4"># 初始化工具，这里用到两个内置工具</span>
    <span style="color:#6272a4"># llm-math: 工具结合语言模型和计算器用以进行数学计算</span>
    <span style="color:#6272a4"># wikipedia: 工具通过API连接到wikipedia进行搜索查询</span>
    tools <span style="color:#ff79c6">=</span> load_tools(tool_names<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;llm-math&#34;</span>, <span style="color:#f1fa8c">&#34;wikipedia&#34;</span>], llm<span style="color:#ff79c6">=</span>llm)

    <span style="color:#6272a4"># 从hub上拉取prompt模版</span>
    prompt <span style="color:#ff79c6">=</span> hub<span style="color:#ff79c6">.</span>pull(<span style="color:#f1fa8c">&#34;hwchase17/openai-tools-agent&#34;</span>)

    <span style="color:#6272a4"># 初始化agent</span>
    agent <span style="color:#ff79c6">=</span> create_openai_tools_agent(llm, tools, prompt)

    <span style="color:#6272a4"># 运行agent</span>
    agent_executor <span style="color:#ff79c6">=</span> AgentExecutor(
        agent<span style="color:#ff79c6">=</span>agent,
        tools<span style="color:#ff79c6">=</span>tools,
        verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)

    agent_executor<span style="color:#ff79c6">.</span>invoke({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;计算300的25%&#34;</span>})
    agent_executor<span style="color:#ff79c6">.</span>invoke({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;曹德旺做了哪些善事&#34;</span>})


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()

</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&gt; Entering new AgentExecutor chain...

Invoking: `Calculator` with `300*0.25`


Answer: 75.025% of 300 is 75.

&gt; Finished chain.


&gt; Entering new AgentExecutor chain...

Invoking: `wikipedia` with `曹德旺`


Page: Cao Dewang
Summary: Cao Dewang (Chinese: 曹德旺; pinyin: Cáo Déwàng; born May 1946), also known as Cho Tak Wong or Tak Wong Cho, is a Chinese entrepreneur. He is the chairman of Fuyao Group, one of the largest glass manufacturers in the world. He is also a member of the Chinese People&#39;s Consultative Conference from Fujian, and chairman of both the China Automobile Glass Association and the Fujian Golf Players&#39; Association.

Page: Jack Wong
Summary: Jack Wong, or Huang Zhang (Chinese: 黄章; pinyin: Huáng Zhāng), is a Chinese billionaire entrepreneur. He is the founder and chairman of Meizu, a Chinese consumer electronics company.

Page: Crocodile Island (film)
Summary: Crocodile Island is a 2020 Chinese action monster film directed by Xu Shixing and Simon Zhao, and starring Gallen Lo as a single father who lands on a crocodile island with his daughter (Liao Yinyue) due to a plane malfunction and must battle with beast-sized creatures inhabiting the island. This web film was released for online streaming on 4 February 2020 on iQiyi. Crocodile Island became a commercial success, grossing ¥16.70 million against a budget of ¥8 million and is currently the highest-grossing web film of 2020 in China.根据维基百科，曹德旺是中国企业家，福耀集团董事长，也是中国汽车玻璃协会和福建高尔夫球员协会的主席。关于他做了哪些善事的具体信息可能需要更深入的研究。您是否希望我帮助您进一步了解曹德旺的善举？

&gt; Finished chain.
</code></pre></div><h3 id="使用pythonrepltool工具">使用PythonREPLTool工具</h3>
<p>使用PythonREPLTool工具将名字转化为拼音</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain_experimental.agents.agent_toolkits.python.base <span style="color:#ff79c6">import</span> create_python_agent
<span style="color:#ff79c6">from</span> langchain_experimental.tools <span style="color:#ff79c6">import</span> PythonREPLTool
<span style="color:#ff79c6">from</span> pydantic <span style="color:#ff79c6">import</span> SecretStr

api_key <span style="color:#ff79c6">=</span> SecretStr(<span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>)
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        base_url<span style="color:#ff79c6">=</span>openai_url,
        api_key<span style="color:#ff79c6">=</span>api_key,
        streaming<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
        verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
    )

    tool <span style="color:#ff79c6">=</span> PythonREPLTool()

    agent <span style="color:#ff79c6">=</span> create_python_agent(
        llm, tool, verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)

    customer_list <span style="color:#ff79c6">=</span> [<span style="color:#f1fa8c">&#34;张三&#34;</span>, <span style="color:#f1fa8c">&#34;李四&#34;</span>, <span style="color:#f1fa8c">&#34;王五&#34;</span>]

    agent<span style="color:#ff79c6">.</span>invoke(
        {<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">f</span><span style="color:#f1fa8c">&#34;使用pinyin拼音库这些客户名字转换为拼音，并打印输出列表: </span><span style="color:#f1fa8c">{</span>customer_list<span style="color:#f1fa8c">}</span><span style="color:#f1fa8c">。&#34;</span>})


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()

</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&gt; Entering new AgentExecutor chain...
I need to use the pinyin library to convert the names to pinyin.
Action: Python_REPL
Action Input: 
```python
from pypinyin import pinyin
names = [&#39;张三&#39;, &#39;李四&#39;, &#39;王五&#39;]
pinyin_names = [&#34;&#34;.join([y[0] for y in x]) for x in [pinyin(name, style=0) for name in names]]
print(pinyin_names)
```Python REPL can execute arbitrary code. Use with caution.

Observation: [&#39;zhangsan&#39;, &#39;lisi&#39;, &#39;wangwu&#39;]

Thought:The names have been successfully converted to pinyin.
Final Answer: [&#39;zhangsan&#39;, &#39;lisi&#39;, &#39;wangwu&#39;]

&gt; Finished chain.
</code></pre></div><p>从输出结果来看，可以看出agent自主决策的一个过程</p>
<h3 id="自定义工具">自定义工具</h3>
<p>LangChain tool函数装饰器可以应用于任何函数，将函数转化为LangChain工具，成为agent可以调用的工具.
这里以创建自定义时间的工具为例：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#ff79c6">from</span> langchain.agents <span style="color:#ff79c6">import</span> tool
<span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> ChatOpenAI
<span style="color:#ff79c6">from</span> langchain.agents <span style="color:#ff79c6">import</span> AgentExecutor, create_openai_tools_agent
<span style="color:#ff79c6">from</span> pydantic <span style="color:#ff79c6">import</span> SecretStr
<span style="color:#ff79c6">from</span> datetime <span style="color:#ff79c6">import</span> date
<span style="color:#ff79c6">from</span> langchain <span style="color:#ff79c6">import</span> hub

api_key <span style="color:#ff79c6">=</span> SecretStr(<span style="color:#f1fa8c">&#34;sk-xxx&#34;</span>)
openai_url <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;https://api.chatanywhere.com.cn/v1&#34;</span>


@tool
<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">time</span>(text: <span style="color:#8be9fd;font-style:italic">str</span>) <span style="color:#ff79c6">-&gt;</span> <span style="color:#8be9fd;font-style:italic">str</span>:
    <span style="color:#f1fa8c">&#34;&#34;&#34;
</span><span style="color:#f1fa8c">    &#34;&#34;&#34;</span>
    <span style="color:#ff79c6">return</span> <span style="color:#8be9fd;font-style:italic">str</span>(date<span style="color:#ff79c6">.</span>today())


<span style="color:#ff79c6">def</span> <span style="color:#50fa7b">main</span>():
    llm <span style="color:#ff79c6">=</span> ChatOpenAI(
        temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.0</span>,
        base_url<span style="color:#ff79c6">=</span>openai_url,
        api_key<span style="color:#ff79c6">=</span>api_key,
        streaming<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
        verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
    )

    tools <span style="color:#ff79c6">=</span> [time]

    <span style="color:#6272a4"># 从hub上拉取prompt模版</span>
    prompt <span style="color:#ff79c6">=</span> hub<span style="color:#ff79c6">.</span>pull(<span style="color:#f1fa8c">&#34;hwchase17/openai-tools-agent&#34;</span>)

    <span style="color:#6272a4"># 初始化agent</span>
    agent <span style="color:#ff79c6">=</span> create_openai_tools_agent(llm, tools, prompt)

    <span style="color:#6272a4"># 运行agent</span>
    agent_executor <span style="color:#ff79c6">=</span> AgentExecutor(
        agent<span style="color:#ff79c6">=</span>agent,
        tools<span style="color:#ff79c6">=</span>tools,
        verbose<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)

    agent_executor<span style="color:#ff79c6">.</span>invoke({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;今天的日期是多少&#34;</span>})


<span style="color:#ff79c6">if</span> __name__ <span style="color:#ff79c6">==</span> <span style="color:#f1fa8c">&#34;__main__&#34;</span>:
    main()

</code></pre></div><p>输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">&gt; Entering new AgentExecutor chain...

Invoking: `time` with `{&#39;text&#39;: &#39;today&#39;}`


2024-03-27今天是2024年3月27日。

&gt; Finished chain.
</code></pre></div><h2 id="lcellangchain-expression-language">LCEL(LangChain Expression Language)</h2>
<p>LangChain表达式语言（LCEL）是一种轻松地将链组合在一起的声明性方式。 LCEL 从第一天起就被设计为支持将原型投入生产，无需更改代码，从最简单的“提示+LLM”链到最复杂的链</p>
<h1 id="llm大模型部署">LLM大模型部署</h1>
<h2 id="麒麟v10系统llm大模型部署">麒麟v10系统LLM大模型部署</h2>
<h3 id="准备工作">准备工作</h3>
<h4 id="硬件环境">硬件环境</h4>
<table>
<thead>
<tr>
<th>类型</th>
<th>规格</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>24核 AMD EPYC 7402</td>
</tr>
<tr>
<td>内存</td>
<td>32GB</td>
</tr>
<tr>
<td>GPU</td>
<td>2 * NVIDIA GeForce RTX 4090 显存24GB</td>
</tr>
<tr>
<td>操作系统</td>
<td>银河麒麟v10</td>
</tr>
<tr>
<td>架构</td>
<td>x86_64</td>
</tr>
<tr>
<td>内核版本</td>
<td>4.19.90-23.43.v2101.ky10.x86_64</td>
</tr>
<tr>
<td>GPU驱动版本</td>
<td>535.146.02</td>
</tr>
<tr>
<td>CUDA版本</td>
<td>12.2</td>
</tr>
</tbody>
</table>
<h4 id="下载huggingface_hub工具">下载huggingface_hub工具</h4>
<p>HuggingFace Hub是一个用于分享和获取自然语言处理（NLP）模型和相关资源的平台；类似代码界的GitHub。</p>
<p>pip3安装huggingface_hub工具</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#6272a4"># pip3 install -U huggingface_hub</span>
</code></pre></div><p>验证huggingface-cli</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost ~<span style="color:#ff79c6">]</span><span style="color:#6272a4"># huggingface-cli env</span>

Copy-and-paste the text below in your GitHub issue.

- huggingface_hub version: 0.20.3
- Platform: Linux-4.19.90-23.43.v2101.ky10.x86_64-x86_64-with-glibc2.28
- Python version: 3.11.7
- Running in iPython ?: No
- Running in notebook ?: No
- Running in Google Colab ?: No
- Token path ?: /root/.cache/huggingface/token
- Has saved token ?: False
- Configured git credential helpers:
- FastAI: N/A
- Tensorflow: N/A
- Torch: 2.1.2
- Jinja2: 3.1.3
- Graphviz: N/A
- Pydot: N/A
- Pillow: 10.2.0
- hf_transfer: N/A
- gradio: N/A
- tensorboard: N/A
- numpy: 1.26.3
- pydantic: 1.10.13
- aiohttp: N/A
- ENDPOINT: https://huggingface.co
- HF_HUB_CACHE: /root/.cache/huggingface/hub
- HF_ASSETS_CACHE: /root/.cache/huggingface/assets
- HF_TOKEN_PATH: /root/.cache/huggingface/token
- HF_HUB_OFFLINE: False
- HF_HUB_DISABLE_TELEMETRY: False
- HF_HUB_DISABLE_PROGRESS_BARS: None
- HF_HUB_DISABLE_SYMLINKS_WARNING: False
- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False
- HF_HUB_DISABLE_IMPLICIT_TOKEN: False
- HF_HUB_ENABLE_HF_TRANSFER: False
- HF_HUB_ETAG_TIMEOUT: <span style="color:#bd93f9">10</span>
- HF_HUB_DOWNLOAD_TIMEOUT: <span style="color:#bd93f9">10</span>
</code></pre></div><p>修改HF环境变量，使用国内镜像地址<code>https://hf-mirror.com</code>，加速下载模型</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost ~<span style="color:#ff79c6">]</span><span style="color:#6272a4"># export HF_ENDPOINT=https://hf-mirror.com</span>
</code></pre></div><h4 id="下载模型">下载模型</h4>
<p>下载模型，以通义千问14b-chat模型为例</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#6272a4"># 创建模型目录，用于存放下载的模型文件</span>
<span style="color:#ff79c6">[</span>root@localhost ~<span style="color:#ff79c6">]</span><span style="color:#6272a4"># mkdir -p /opt/ice/models/qwen-14b-chat </span>

<span style="color:#ff79c6">[</span>root@localhost ~<span style="color:#ff79c6">]</span><span style="color:#6272a4"># huggingface-cli download \</span>
--resume-download --local-dir-use-symlinks False <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>--local-dir /opt/ice/models/qwen-14b-chat <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>qwen/qwen-14b-chat
Consider using <span style="color:#f1fa8c">`</span>hf_transfer<span style="color:#f1fa8c">`</span> <span style="color:#ff79c6">for</span> faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer <span style="color:#ff79c6">for</span> more details.
Fetching <span style="color:#bd93f9">37</span> files:   0%|                                                                                                                              | 0/37 <span style="color:#ff79c6">[</span>00:00&lt;?, ?it/s<span style="color:#ff79c6">]</span>downloading https://hf-mirror.com/qwen/qwen-14b-chat/resolve/cdaff792392504e679496a9f386acf3c1e4333a5/.gitattributes to /root/.cache/huggingface/hub/models--qwen--qwen-14b-chat/blobs/a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete
downloading https://hf-mirror.com/qwen/qwen-14b-chat/resolve/cdaff792392504e679496a9f386acf3c1e4333a5/LICENSE to /root/.cache/huggingface/hub/models--qwen--qwen-14b-chat/blobs/5be33384d19169a98eee863ff09c74eb32e37696.incomplete
</code></pre></div><p>下载需要登录的模型(Gated Model)，添加参数<code>--token hf_***</code>参数，hf_***是access token，token从这里获取<code>https://huggingface.co/settings/tokens</code></p>
<h4 id="配置nvidia-docker-runtime">配置nvidia docker runtime</h4>
<p>下载nvidia runtime和工具包，下面启动大模型是采用docker方式来启动的，原生docker runtime不支持GPU</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost<span style="color:#ff79c6">]</span><span style="color:#6272a4"># yum install -y nvidia-container-toolkit nvidia-container-runtime</span>
</code></pre></div><p>生成nvidia runtime的docker配置</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost<span style="color:#ff79c6">]</span><span style="color:#6272a4"># nvidia-ctk runtime configure --runtime=docker</span>
INFO<span style="color:#ff79c6">[</span>0000<span style="color:#ff79c6">]</span> Loading docker config from /etc/docker/daemon.json
INFO<span style="color:#ff79c6">[</span>0000<span style="color:#ff79c6">]</span> Config file does not exist, creating new one
INFO<span style="color:#ff79c6">[</span>0000<span style="color:#ff79c6">]</span> Wrote updated config to /etc/docker/daemon.json
INFO<span style="color:#ff79c6">[</span>0000<span style="color:#ff79c6">]</span> It is recommended that the docker daemon be restarted.
<span style="color:#ff79c6">[</span>root@localhost qwen-14b<span style="color:#ff79c6">]</span><span style="color:#6272a4"># cat /etc/docker/daemon.json</span>
<span style="color:#ff79c6">{</span>
    <span style="color:#f1fa8c">&#34;runtimes&#34;</span>: <span style="color:#ff79c6">{</span>
        <span style="color:#f1fa8c">&#34;nvidia&#34;</span>: <span style="color:#ff79c6">{</span>
            <span style="color:#f1fa8c">&#34;args&#34;</span>: <span style="color:#ff79c6">[]</span>,
            <span style="color:#f1fa8c">&#34;path&#34;</span>: <span style="color:#f1fa8c">&#34;nvidia-container-runtime&#34;</span>
        <span style="color:#ff79c6">}</span>
    <span style="color:#ff79c6">}</span>
<span style="color:#ff79c6">}</span>

<span style="color:#ff79c6">[</span>root@localhost<span style="color:#ff79c6">]</span><span style="color:#6272a4"># systemctl restart docker</span>
</code></pre></div><p>验证nvidia runtime是否安装成功</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost<span style="color:#ff79c6">]</span><span style="color:#6272a4"># docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi</span>
</code></pre></div><h3 id="非vllm方式运行大模型">非vLLM方式运行大模型</h3>
<h4 id="下载api-for-open-llm">下载api-for-open-llm</h4>
<p>api-for-open-llm是开源大模型的统一后端接口，与OpenAI的响应保持一致。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#6272a4"># git clone https://github.com/xusenlinzy/api-for-open-llm.git</span>
</code></pre></div><h4 id="api-for-open-llm配置参数">api-for-open-llm配置参数</h4>
<p>拷贝一份配置</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#6272a4"># cd api-for-open-llm/</span>
<span style="color:#6272a4"># cp .env.example .env</span>
</code></pre></div><p>编辑参数</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost api-for-open-llm<span style="color:#ff79c6">]</span><span style="color:#6272a4"># vim .env</span>
<span style="color:#8be9fd;font-style:italic">PORT</span><span style="color:#ff79c6">=</span><span style="color:#bd93f9">8000</span>

<span style="color:#6272a4"># model related</span>
<span style="color:#8be9fd;font-style:italic">MODEL_NAME</span><span style="color:#ff79c6">=</span>qwen      <span style="color:#6272a4"># 模型名</span>
<span style="color:#8be9fd;font-style:italic">MODEL_PATH</span><span style="color:#ff79c6">=</span>/opt/ice/models/qwen-14b-chat <span style="color:#6272a4"># 下载好的模型文件路径</span>
<span style="color:#8be9fd;font-style:italic">EMBEDDING_NAME</span><span style="color:#ff79c6">=</span>
<span style="color:#8be9fd;font-style:italic">ADAPTER_MODEL_PATH</span><span style="color:#ff79c6">=</span>
<span style="color:#8be9fd;font-style:italic">QUANTIZE</span><span style="color:#ff79c6">=</span><span style="color:#bd93f9">16</span>
<span style="color:#8be9fd;font-style:italic">CONTEXT_LEN</span><span style="color:#ff79c6">=</span>
<span style="color:#8be9fd;font-style:italic">LOAD_IN_8BIT</span><span style="color:#ff79c6">=</span><span style="color:#8be9fd;font-style:italic">false</span>
<span style="color:#8be9fd;font-style:italic">LOAD_IN_4BIT</span><span style="color:#ff79c6">=</span><span style="color:#8be9fd;font-style:italic">false</span>
<span style="color:#8be9fd;font-style:italic">USING_PTUNING_V2</span><span style="color:#ff79c6">=</span><span style="color:#8be9fd;font-style:italic">false</span>
<span style="color:#8be9fd;font-style:italic">STREAM_INTERVERL</span><span style="color:#ff79c6">=</span><span style="color:#bd93f9">2</span>
<span style="color:#8be9fd;font-style:italic">PROMPT_NAME</span><span style="color:#ff79c6">=</span>

<span style="color:#6272a4"># device related</span>
<span style="color:#8be9fd;font-style:italic">DEVICE</span><span style="color:#ff79c6">=</span>

<span style="color:#6272a4"># &#34;auto&#34;, &#34;cuda:0&#34;, &#34;cuda:1&#34;, ...</span>
<span style="color:#8be9fd;font-style:italic">DEVICE_MAP</span><span style="color:#ff79c6">=</span>auto
<span style="color:#8be9fd;font-style:italic">GPUS</span><span style="color:#ff79c6">=</span>
<span style="color:#8be9fd;font-style:italic">NUM_GPUs</span><span style="color:#ff79c6">=</span><span style="color:#bd93f9">2</span>
<span style="color:#8be9fd;font-style:italic">DTYPE</span><span style="color:#ff79c6">=</span>half

<span style="color:#6272a4"># api related</span>
<span style="color:#8be9fd;font-style:italic">API_PREFIX</span><span style="color:#ff79c6">=</span>/v1

<span style="color:#8be9fd;font-style:italic">USE_STREAMER_V2</span><span style="color:#ff79c6">=</span><span style="color:#8be9fd;font-style:italic">false</span>
<span style="color:#8be9fd;font-style:italic">ENGINE</span><span style="color:#ff79c6">=</span>default
</code></pre></div><h4 id="api-for-open-llm加载模型">api-for-open-llm加载模型</h4>
<p>api-for-open-llm启动方式有docker和本地方式，推荐使用docker</p>
<p>使用docker方式的话，需要先构建llm-api镜像</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost api-for-open-llm<span style="color:#ff79c6">]</span><span style="color:#6272a4"># docker build -f docker/Dockerfile -t llm-api:pytorch .</span>
</code></pre></div><p>docker启动</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost api-for-open-llm<span style="color:#ff79c6">]</span><span style="color:#6272a4"># docker run -it -d \</span>
--gpus all <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>--ipc<span style="color:#ff79c6">=</span>host -p 8000:8000 <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>--name<span style="color:#ff79c6">=</span>llm-api --ulimit <span style="color:#8be9fd;font-style:italic">memlock</span><span style="color:#ff79c6">=</span>-1 <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>--ulimit <span style="color:#8be9fd;font-style:italic">stack</span><span style="color:#ff79c6">=</span><span style="color:#bd93f9">67108864</span> <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>-v <span style="color:#f1fa8c">`</span><span style="color:#8be9fd;font-style:italic">pwd</span><span style="color:#f1fa8c">`</span>:/workspace -v /opt/ice/models/qwen-14b-chat:/opt/ice/models/qwen-14b-chat <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>llm-api:pytorch <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>python api/server.py
</code></pre></div><p>查看docker实例启动日志，验证模型是否启动成功</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost api-for-open-llm<span style="color:#ff79c6">]</span><span style="color:#6272a4"># docker logs -f llm-api</span>
<span style="color:#ff79c6">=============</span>
<span style="color:#ff79c6">==</span> <span style="color:#8be9fd;font-style:italic">PyTorch</span> <span style="color:#ff79c6">==</span>
<span style="color:#ff79c6">=============</span>

NVIDIA Release 23.10 <span style="color:#ff79c6">(</span>build 71422337<span style="color:#ff79c6">)</span>
PyTorch Version 2.1.0a0+32f93b1

Container image Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> 2023, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.

Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> 2014-2023 Facebook Inc.
Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> 2011-2014 Idiap Research Institute <span style="color:#ff79c6">(</span>Ronan Collobert<span style="color:#ff79c6">)</span>
Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> 2012-2014 Deepmind Technologies    <span style="color:#ff79c6">(</span>Koray Kavukcuoglu<span style="color:#ff79c6">)</span>
Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> 2011-2012 NEC Laboratories America <span style="color:#ff79c6">(</span>Koray Kavukcuoglu<span style="color:#ff79c6">)</span>
Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> 2011-2013 NYU                      <span style="color:#ff79c6">(</span>Clement Farabet<span style="color:#ff79c6">)</span>
Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> 2006-2010 NEC Laboratories America <span style="color:#ff79c6">(</span>Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston<span style="color:#ff79c6">)</span>
Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> <span style="color:#bd93f9">2006</span>      Idiap Research Institute <span style="color:#ff79c6">(</span>Samy Bengio<span style="color:#ff79c6">)</span>
Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> 2001-2004 Idiap Research Institute <span style="color:#ff79c6">(</span>Ronan Collobert, Samy Bengio, Johnny Mariethoz<span style="color:#ff79c6">)</span>
Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> <span style="color:#bd93f9">2015</span>      Google Inc.
Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> <span style="color:#bd93f9">2015</span>      Yangqing Jia
Copyright <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications <span style="color:#ff79c6">(</span>c<span style="color:#ff79c6">)</span> NVIDIA CORPORATION &amp; AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

2024-02-01 01:53:11.597 | DEBUG    | api.config:&lt;module&gt;:265 - SETTINGS: <span style="color:#ff79c6">{</span>
    <span style="color:#f1fa8c">&#34;host&#34;</span>: <span style="color:#f1fa8c">&#34;0.0.0.0&#34;</span>,
    <span style="color:#f1fa8c">&#34;port&#34;</span>: 8000,
    <span style="color:#f1fa8c">&#34;api_prefix&#34;</span>: <span style="color:#f1fa8c">&#34;/v1&#34;</span>,
    <span style="color:#f1fa8c">&#34;engine&#34;</span>: <span style="color:#f1fa8c">&#34;default&#34;</span>,
    <span style="color:#f1fa8c">&#34;model_name&#34;</span>: <span style="color:#f1fa8c">&#34;qwen&#34;</span>,
    <span style="color:#f1fa8c">&#34;model_path&#34;</span>: <span style="color:#f1fa8c">&#34;/opt/ice/models/qwen-14b-chat&#34;</span>,
    <span style="color:#f1fa8c">&#34;adapter_model_path&#34;</span>: null,
    <span style="color:#f1fa8c">&#34;resize_embeddings&#34;</span>: false,
    <span style="color:#f1fa8c">&#34;dtype&#34;</span>: <span style="color:#f1fa8c">&#34;half&#34;</span>,
    <span style="color:#f1fa8c">&#34;device&#34;</span>: <span style="color:#f1fa8c">&#34;cuda&#34;</span>,
    <span style="color:#f1fa8c">&#34;device_map&#34;</span>: <span style="color:#f1fa8c">&#34;auto&#34;</span>,
    <span style="color:#f1fa8c">&#34;gpus&#34;</span>: null,
    <span style="color:#f1fa8c">&#34;num_gpus&#34;</span>: 2,
    <span style="color:#f1fa8c">&#34;only_embedding&#34;</span>: false,
    <span style="color:#f1fa8c">&#34;embedding_name&#34;</span>: null,
    <span style="color:#f1fa8c">&#34;embedding_size&#34;</span>: -1,
    <span style="color:#f1fa8c">&#34;embedding_device&#34;</span>: <span style="color:#f1fa8c">&#34;cuda&#34;</span>,
    <span style="color:#f1fa8c">&#34;quantize&#34;</span>: 16,
    <span style="color:#f1fa8c">&#34;load_in_8bit&#34;</span>: false,
    <span style="color:#f1fa8c">&#34;load_in_4bit&#34;</span>: false,
    <span style="color:#f1fa8c">&#34;using_ptuning_v2&#34;</span>: false,
    <span style="color:#f1fa8c">&#34;pre_seq_len&#34;</span>: 128,
    <span style="color:#f1fa8c">&#34;context_length&#34;</span>: -1,
    <span style="color:#f1fa8c">&#34;chat_template&#34;</span>: null,
    <span style="color:#f1fa8c">&#34;rope_scaling&#34;</span>: null,
    <span style="color:#f1fa8c">&#34;flash_attn&#34;</span>: false,
    <span style="color:#f1fa8c">&#34;trust_remote_code&#34;</span>: false,
    <span style="color:#f1fa8c">&#34;tokenize_mode&#34;</span>: <span style="color:#f1fa8c">&#34;auto&#34;</span>,
    <span style="color:#f1fa8c">&#34;tensor_parallel_size&#34;</span>: 1,
    <span style="color:#f1fa8c">&#34;gpu_memory_utilization&#34;</span>: 0.9,
    <span style="color:#f1fa8c">&#34;max_num_batched_tokens&#34;</span>: -1,
    <span style="color:#f1fa8c">&#34;max_num_seqs&#34;</span>: 256,
    <span style="color:#f1fa8c">&#34;quantization_method&#34;</span>: null,
    <span style="color:#f1fa8c">&#34;use_streamer_v2&#34;</span>: false,
    <span style="color:#f1fa8c">&#34;api_keys&#34;</span>: null,
    <span style="color:#f1fa8c">&#34;activate_inference&#34;</span>: true,
    <span style="color:#f1fa8c">&#34;interrupt_requests&#34;</span>: true,
    <span style="color:#f1fa8c">&#34;n_gpu_layers&#34;</span>: 0,
    <span style="color:#f1fa8c">&#34;main_gpu&#34;</span>: 0,
    <span style="color:#f1fa8c">&#34;tensor_split&#34;</span>: null,
    <span style="color:#f1fa8c">&#34;n_batch&#34;</span>: 512,
    <span style="color:#f1fa8c">&#34;n_threads&#34;</span>: 24,
    <span style="color:#f1fa8c">&#34;n_threads_batch&#34;</span>: 24,
    <span style="color:#f1fa8c">&#34;rope_scaling_type&#34;</span>: -1,
    <span style="color:#f1fa8c">&#34;rope_freq_base&#34;</span>: 0.0,
    <span style="color:#f1fa8c">&#34;rope_freq_scale&#34;</span>: 0.0,
    <span style="color:#f1fa8c">&#34;tgi_endpoint&#34;</span>: null,
    <span style="color:#f1fa8c">&#34;tei_endpoint&#34;</span>: null,
    <span style="color:#f1fa8c">&#34;max_concurrent_requests&#34;</span>: 256,
    <span style="color:#f1fa8c">&#34;max_client_batch_size&#34;</span>: <span style="color:#bd93f9">32</span>
<span style="color:#ff79c6">}</span>
2024-02-01 01:53:22.457 | INFO     | api.adapter.patcher:patch_tokenizer:119 - Add eos token: &lt;|endoftext|&gt;
2024-02-01 01:53:22.457 | INFO     | api.adapter.patcher:patch_tokenizer:126 - Add pad token: &lt;|endoftext|&gt;
/root/.cache/huggingface/modules/transformers_modules/qwen-14b-chat/modeling_qwen.py:969: DeprecationWarning: The <span style="color:#f1fa8c">&#39;warn&#39;</span> method is deprecated, use <span style="color:#f1fa8c">&#39;warning&#39;</span> instead
  logger.warn<span style="color:#ff79c6">(</span><span style="color:#f1fa8c">&#34;Try importing flash-attention for faster inference...&#34;</span><span style="color:#ff79c6">)</span>
Try importing flash-attention <span style="color:#ff79c6">for</span> faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Loading checkpoint shards: 100% 15/15 <span style="color:#ff79c6">[</span>00:06&lt;00:00,  2.41it/s<span style="color:#ff79c6">]</span>
2024-02-01 01:53:29.693 | INFO     | api.models:create_generate_model:61 - Using default engine
2024-02-01 01:53:29.693 | INFO     | api.core.default:_check_construct_prompt:128 - Using Qwen Model <span style="color:#ff79c6">for</span> Chat!
INFO:     Started server process <span style="color:#ff79c6">[</span>1<span style="color:#ff79c6">]</span>
INFO:     Waiting <span style="color:#ff79c6">for</span> application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 <span style="color:#ff79c6">(</span>Press CTRL+C to quit<span style="color:#ff79c6">)</span>
</code></pre></div><h3 id="vllm方式运行大模型">vLLM方式运行大模型</h3>
<p>vLLM是来自UC Berkeley的LMSYS在LLM推理方面的最新工作（发布Vicuna大模型的那个团队），最大亮点是采用Paged Attention技术，
结合Continuous Batching，极大地优化了realtime场景下的LLM serving 的 throughput 与内存使用。</p>
<p>除了vLLM外可以加速大模型推理，还有FlashAttention；vLLM的核心是PagedAttention，FlashAttention是一种重新排序注意力计算的算法，它利用平铺、重计算等经典技术来显著提升计算速度，并将序列长度中的内存使用实现从二次到线性减少。Flash Attention的主要目的是加速和节省内存。</p>
<p>FlashAttention-2需要GPU支持：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">FlashAttention-2 currently supports:

Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100). Support for Turing GPUs (T4, RTX 2080) is coming soon, please use FlashAttention 1.x for Turing GPUs for now.
Datatype fp16 and bf16 (bf16 requires Ampere, Ada, or Hopper GPUs).
All head dimensions up to 256. Head dim &gt; 192 backward requires A100/A800 or H100/H800.
</code></pre></div><h4 id="编译vllm镜像">编译vLLM镜像</h4>
<p>正常操作系统用<code>vllm/vllm-openai</code>这个镜像就可以了，但是通义千问模型需要额外装包，所以重新编译下vllm-openai镜像</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost<span style="color:#ff79c6">]</span><span style="color:#6272a4"># vim Dockerfile</span>
<span style="color:#6272a4"># 使用vllm/vllm-openai作为基础镜像</span>
FROM vllm/vllm-openai:v0.3.0

<span style="color:#6272a4"># 通义千问需要额外安装包</span>
RUN pip install tiktoken <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>  -i https://pypi.tuna.tsinghua.edu.cn/simple
</code></pre></div><p>docker build编译</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost<span style="color:#ff79c6">]</span><span style="color:#6272a4"># docker build -f Dockerfile -t vllm/vllm-openai:kylin-v10 .</span>
</code></pre></div><h4 id="vllm加载模型">vLLM加载模型</h4>
<p>docker运行vLLM镜像</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost<span style="color:#ff79c6">]</span><span style="color:#6272a4"># docker run -it -d \</span>
--gpus all -v /opt/ice/models/qwen-14b-chat:/opt/qwen-14b-chat <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>--name vllm-api <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>-p 8000:8000 --ipc<span style="color:#ff79c6">=</span>host <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>vllm/vllm-openai:kylin-v10 <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>--model /opt/qwen-14b-chat <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>--enforce-eager <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>--trust-remote-code <span style="color:#f1fa8c">\
</span><span style="color:#f1fa8c"></span>--tensor-parallel-size <span style="color:#bd93f9">2</span>
</code></pre></div><p>注：vLLM方式启动带的<code>--model</code>这个参数，调用api接口的时候也需要传入同样的值；<code>--tensor-parallel-size</code>用于在多个GPU间分配工作</p>
<p>查看显卡占用，模型分布在两张卡上，所谓的模型并行</p>
<pre tabindex="0"><code>[root@localhost vllm]# nvidia-smi
Thu Feb  1 19:32:24 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4090        Off | 00000000:81:00.0 Off |                  Off |
| 55%   61C    P2             241W / 450W |  22026MiB / 24564MiB |     97%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce RTX 4090        Off | 00000000:C1:00.0 Off |                  Off |
| 68%   66C    P2             243W / 450W |  20796MiB / 24564MiB |     90%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    151961      C   python3                                   22010MiB |
|    1   N/A  N/A    154943      C   ray::RayWorkerVllm.execute_method         20780MiB |
+---------------------------------------------------------------------------------------+
</code></pre><p>查看vllm-api容器日志，验证是否启动成功</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#ff79c6">[</span>root@localhost vllm<span style="color:#ff79c6">]</span><span style="color:#6272a4"># docker logs -f vllm-api</span>
INFO 02-01 10:14:02 api_server.py:727<span style="color:#ff79c6">]</span> args: Namespace<span style="color:#ff79c6">(</span><span style="color:#8be9fd;font-style:italic">host</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">port</span><span style="color:#ff79c6">=</span>8000, <span style="color:#8be9fd;font-style:italic">allow_credentials</span><span style="color:#ff79c6">=</span>False, <span style="color:#8be9fd;font-style:italic">allowed_origins</span><span style="color:#ff79c6">=[</span><span style="color:#f1fa8c">&#39;*&#39;</span><span style="color:#ff79c6">]</span>, <span style="color:#8be9fd;font-style:italic">allowed_methods</span><span style="color:#ff79c6">=[</span><span style="color:#f1fa8c">&#39;*&#39;</span><span style="color:#ff79c6">]</span>, <span style="color:#8be9fd;font-style:italic">allowed_headers</span><span style="color:#ff79c6">=[</span><span style="color:#f1fa8c">&#39;*&#39;</span><span style="color:#ff79c6">]</span>, <span style="color:#8be9fd;font-style:italic">served_model_name</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">chat_template</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">response_role</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;assistant&#39;</span>, <span style="color:#8be9fd;font-style:italic">ssl_keyfile</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">ssl_certfile</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">model</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;/opt/qwen-14b-chat&#39;</span>, <span style="color:#8be9fd;font-style:italic">tokenizer</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">revision</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">tokenizer_revision</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">tokenizer_mode</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;auto&#39;</span>, <span style="color:#8be9fd;font-style:italic">trust_remote_code</span><span style="color:#ff79c6">=</span>True, <span style="color:#8be9fd;font-style:italic">download_dir</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">load_format</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;auto&#39;</span>, <span style="color:#8be9fd;font-style:italic">dtype</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;auto&#39;</span>, <span style="color:#8be9fd;font-style:italic">max_model_len</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">worker_use_ray</span><span style="color:#ff79c6">=</span>False, <span style="color:#8be9fd;font-style:italic">pipeline_parallel_size</span><span style="color:#ff79c6">=</span>1, <span style="color:#8be9fd;font-style:italic">tensor_parallel_size</span><span style="color:#ff79c6">=</span>2, <span style="color:#8be9fd;font-style:italic">max_parallel_loading_workers</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">block_size</span><span style="color:#ff79c6">=</span>16, <span style="color:#8be9fd;font-style:italic">seed</span><span style="color:#ff79c6">=</span>0, <span style="color:#8be9fd;font-style:italic">swap_space</span><span style="color:#ff79c6">=</span>4, <span style="color:#8be9fd;font-style:italic">gpu_memory_utilization</span><span style="color:#ff79c6">=</span>0.9, <span style="color:#8be9fd;font-style:italic">max_num_batched_tokens</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">max_num_seqs</span><span style="color:#ff79c6">=</span>256, <span style="color:#8be9fd;font-style:italic">max_paddings</span><span style="color:#ff79c6">=</span>256, <span style="color:#8be9fd;font-style:italic">disable_log_stats</span><span style="color:#ff79c6">=</span>False, <span style="color:#8be9fd;font-style:italic">quantization</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">enforce_eager</span><span style="color:#ff79c6">=</span>True, <span style="color:#8be9fd;font-style:italic">max_context_len_to_capture</span><span style="color:#ff79c6">=</span>8192, <span style="color:#8be9fd;font-style:italic">engine_use_ray</span><span style="color:#ff79c6">=</span>False, <span style="color:#8be9fd;font-style:italic">disable_log_requests</span><span style="color:#ff79c6">=</span>False, <span style="color:#8be9fd;font-style:italic">max_log_len</span><span style="color:#ff79c6">=</span>None<span style="color:#ff79c6">)</span>
2024-02-01 10:14:07,259	INFO worker.py:1724 -- Started a <span style="color:#8be9fd;font-style:italic">local</span> Ray instance.

INFO 02-01 10:14:08 llm_engine.py:70<span style="color:#ff79c6">]</span> Initializing an LLM engine with config: <span style="color:#8be9fd;font-style:italic">model</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;/opt/qwen-14b-chat&#39;</span>, <span style="color:#8be9fd;font-style:italic">tokenizer</span><span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;/opt/qwen-14b-chat&#39;</span>, <span style="color:#8be9fd;font-style:italic">tokenizer_mode</span><span style="color:#ff79c6">=</span>auto, <span style="color:#8be9fd;font-style:italic">revision</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">tokenizer_revision</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">trust_remote_code</span><span style="color:#ff79c6">=</span>True, <span style="color:#8be9fd;font-style:italic">dtype</span><span style="color:#ff79c6">=</span>torch.float16, <span style="color:#8be9fd;font-style:italic">max_seq_len</span><span style="color:#ff79c6">=</span>2048, <span style="color:#8be9fd;font-style:italic">download_dir</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">load_format</span><span style="color:#ff79c6">=</span>auto, <span style="color:#8be9fd;font-style:italic">tensor_parallel_size</span><span style="color:#ff79c6">=</span>2, <span style="color:#8be9fd;font-style:italic">quantization</span><span style="color:#ff79c6">=</span>None, <span style="color:#8be9fd;font-style:italic">enforce_eager</span><span style="color:#ff79c6">=</span>True, <span style="color:#8be9fd;font-style:italic">seed</span><span style="color:#ff79c6">=</span>0<span style="color:#ff79c6">)</span>
WARNING 02-01 10:14:09 tokenizer.py:62<span style="color:#ff79c6">]</span> Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 02-01 10:14:23 llm_engine.py:275<span style="color:#ff79c6">]</span> <span style="color:#6272a4"># GPU blocks: 981, # CPU blocks: 655</span>
WARNING 02-01 10:14:25 tokenizer.py:62<span style="color:#ff79c6">]</span> Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 02-01 10:14:25 api_server.py:123<span style="color:#ff79c6">]</span> No chat template provided. Chat API will not work.
INFO:     Started server process <span style="color:#ff79c6">[</span>1<span style="color:#ff79c6">]</span>
INFO:     Waiting <span style="color:#ff79c6">for</span> application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 <span style="color:#ff79c6">(</span>Press CTRL+C to quit<span style="color:#ff79c6">)</span>
</code></pre></div><h3 id="验证大模型接口">验证大模型接口</h3>
<p>无论是用vllm启动还是非vllm启动的大模型服务，它们的api请求参数是一致的<code>/v1/completions/</code></p>
<p>请求body参数解释</p>
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>string 必填</td>
<td></td>
<td>使用的模型ID。可以使用模型API列表接口查看所有可用的模型，有关模型的描述，请参阅模型概述。</td>
</tr>
<tr>
<td>prompt</td>
<td>string或array 可选</td>
<td>&lt;|endoftext|&gt;</td>
<td>生成完成的提示，编码为字符串、字符串数组、token数组或token数组的数组。请注意， &lt;|endoftext|&gt;是模型在训练期间看到的文档分隔符，因此，如果未指定提示，则模型将从新文档的开头生成。</td>
</tr>
<tr>
<td>suffix</td>
<td>string 可选</td>
<td>null</td>
<td>插入文本完成后出现的后缀。</td>
</tr>
<tr>
<td>max_tokens</td>
<td>nteger 可选</td>
<td>16</td>
<td>完成时要生成的最大token数量。提示的token计数加上max_tokens不能超过模型的上下文长度。大多数模型的上下文长度为2048个token（最新模型除外，支持4096个）</td>
</tr>
<tr>
<td>temperature</td>
<td>number 可选</td>
<td>1</td>
<td>使用什么样的采样温度，介于0和1之间。较高的值（如0.8）将使输出更加随机，而较低的值（例如0.2）将使其更加集中和确定。通常建议更改它或top_p，但不能同时更改两者。</td>
</tr>
<tr>
<td>top_p</td>
<td>number 可选</td>
<td>1</td>
<td>一种用温度采样的替代品，称为核采样，其中模型考虑了具有top_p概率质量的token的结果。因此，0.1意味着只考虑包含前10%概率质量的token。通常建议改变它或temperature，但不能同时更改两者。</td>
</tr>
<tr>
<td>n</td>
<td>integer 可选</td>
<td>1</td>
<td>每个提示要生成多少个完成。注意：由于此参数会生成许多完成，因此它可以快速消耗您的token配额。小心使用，并确保您对max_tokens和stop有合理的设置。</td>
</tr>
<tr>
<td>stream</td>
<td>boolean 可选</td>
<td>false</td>
<td>是否流回部分进度。如果设置，token将在可用时作为仅数据服务器发送的事件发送，流将以data:[DONE]消息终止。</td>
</tr>
<tr>
<td>logprobs</td>
<td>interger 可选</td>
<td>null</td>
<td>按可能性概率选择token的个数。例如，如果logprobs为5，API将返回5个最有可能的token的列表。API将始终返回采样token的logprob，因此响应中可能最多有logprobs+1元素。logprobs的最大值为5。</td>
</tr>
<tr>
<td>echo</td>
<td>boolean 可选</td>
<td>false</td>
<td>除了完成之外，回显提示</td>
</tr>
<tr>
<td>stop</td>
<td>string或array 可选</td>
<td>null</td>
<td>最多4个序列，API将停止生成进一步的token。返回的文本将不包含停止序列。</td>
</tr>
<tr>
<td>presence_penalty</td>
<td>number 可选</td>
<td>0</td>
<td>取值范围：-2.0~2.0。正值根据新token到目前为止是否出现在文本中来惩罚它们，这增加了模型谈论新主题的可能性。</td>
</tr>
<tr>
<td>best_of</td>
<td>interger 可选</td>
<td>1</td>
<td>在服务器端生成best_of个完成，并返回“最佳”（每个token的日志概率最高）。结果无法流式传输。与n一起使用时，best_of控制候选完成的数量，n指定要返回的数量–best_of必须大于n。注意：由于此参数会生成许多完成，因此它可以快速消耗token配额。小心使用并确保您对max_tokens和stop进行了合理的设置。</td>
</tr>
<tr>
<td>logit_bias</td>
<td>map 可选</td>
<td>null</td>
<td>修改完成时出现指定token的可能性。接受将token（由其在GPT token生成器中的token ID指定）映射到从-100到100的相关偏差值的json对象。您可以使用此token工具（适用于GPT-2和GPT-3）将文本转换为token ID。在数学上，偏差在采样之前被添加到模型生成的逻辑中。确切的效果因模型而异，但-1和1之间的值应该会降低或增加选择的可能性；-100或100这样的值应该导致相关token的禁止或独占选择。例如，可以传递｛“50256”：-100｝以防止生成&lt;|endoftext|&gt;的token。</td>
</tr>
<tr>
<td>user</td>
<td>string 可选</td>
<td></td>
<td>代表最终用户的唯一标识符，可帮助OpenAI监控和检测滥用。</td>
</tr>
</tbody>
</table>
<p>curl发个prompt请求，查看是否正常返回</p>
<pre tabindex="0"><code>[root@localhost ~]# curl http://localhost:8000/v1/completions -H &quot;Content-Type: application/json&quot; -d '{&quot;model&quot;: &quot;/opt/qwen-14b-chat&quot;,&quot;prompt&quot;: &quot;Hi,深圳排名前十的好玩的公园？&quot;,&quot;max_tokens&quot;: 1000,&quot;temperature&quot;: 0, &quot;stop&quot;: &quot;&lt;|endoftext|&gt;&quot;}'

# 输出
{
    &quot;id&quot;: &quot;cmpl-dcee971a-fc6b-45f5-bd56-d7433553d9e5&quot;,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: &quot;stop&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;text&quot;: &quot;深圳有哪些好玩的公园呢？公园里景色优美，空气清新，是休闲放松的好去处。\n深圳公园多，而且多有特色，比如深圳湾公园、莲花山公园、笔架山公园、洪湖公园、大梅沙海滨公园等等，都是深圳市民和游客经常去的地方。这些公园各有特色，各有特色，各有各自的特色。\n如果时间充足，可以一一游玩。如果时间紧凑，可以挑选一些有代表性的公园游玩。今天就带大家游览一下深圳十大公园。\n深圳湾公园位于深圳市南山区西部，东临深圳湾，西至白石洲，北靠福田区，南接蛇口。公园包括深圳湾栈道、深圳湾运动公园、深圳湾休闲文化公园等。深圳湾公园是深圳最浪漫的公园，最适合新人拍摄婚纱照，也适合全家出游。\n莲花山公园位于深圳市中心区北端红荔路与新洲路交汇处，占地194公顷，是深圳最大的公园。公园包括山顶广场、风筝广场、观景台、莲花山音乐厅、深圳改革开放展览馆等。莲花山公园是深圳市民最喜欢的公园之一，也是深圳最著名的公园。\n笔架山公园位于深圳市中心区，北临深南大道，西接福华三路。公园包括笔架山、笔架山公墓、笔架山儿童公园、笔架山植物园等。笔架山公园是深圳最安静的公园，最适合周末休闲。\n洪湖公园位于深圳市中心区，南临深南大道，北接红荔路，东临深南大道，西接华强北路。公园包括洪湖、洪湖公园、洪湖街、洪湖街等。洪湖公园是深圳最浪漫的公园，最适合新人拍摄婚纱照，也适合全家出游。\n大梅沙海滨公园位于深圳市盐田区东部，西临大梅沙海滨，东至盐田港。公园包括大梅沙海滨、大梅沙海滨公园、大梅沙海滨步行街等。大梅沙海滨公园是深圳最受欢迎的公园，也是深圳最著名的公园。\n深圳湾公园、莲花山公园、笔架山公园、洪湖公园、大梅沙海滨公园都是深圳市民和游客经常去的地方，也是深圳最受欢迎的公园。\n深圳湾公园位于深圳市南山区西部，东临深圳湾，西至白石洲，北靠福田区，南接蛇口。公园包括深圳湾栈道、深圳湾运动公园、深圳湾休闲文化公园等。深圳湾公园是深圳最浪漫的公园，最适合新人拍摄婚纱照，也适合全家出游。\n莲花山公园位于深圳市中心区北端红荔路与新洲路交汇处，占地194公顷，是深圳最大的公园。公园包括山顶广场、风筝广场、观景台、莲花山音乐厅、深圳改革开放展览馆等。莲花山公园是深圳市民最喜欢的公园之一，也是深圳最著名的公园。\n笔架山公园位于深圳市中心区，北临深南大道，西接福华三路。公园包括笔架山、笔架山公墓、笔架山儿童公园、笔架山植物园等。笔架山公园是深圳最安静的公园，最适合周末休闲。\n洪湖公园位于深圳市中心区，南临深南大道，北接红荔路，东临深南大道，西接华强北路。公园包括洪湖、洪湖公园、洪湖街、洪湖街等。洪湖公园是深圳最浪漫的公园，最适合新人拍摄婚纱照，也适合全家出游。\n大梅沙海滨公园位于深圳市盐田区东部，西临大梅沙海滨，东至盐田港。公园包括大梅沙海滨、大梅沙海滨公园、大梅沙海滨步行街等。大梅沙海滨公园是深圳最受欢迎的公园，也是深圳最著名的公园。\n以上就是深圳十大公园，包括深圳湾公园、莲花山公园、笔架山公园、洪湖公园、大梅沙海滨公园等。深圳十大公园各有特色，各有特色，各有各自的特色。&quot;
        }
    ],
    &quot;created&quot;: 1706767165,
    &quot;model&quot;: &quot;/opt/qwen-14b-chat&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;usage&quot;: {
        &quot;completion_tokens&quot;: 859,
        &quot;prompt_tokens&quot;: 10,
        &quot;total_tokens&quot;: 869
    }
}
</code></pre><h3 id="常见错误">常见错误</h3>
<p>问题：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 7.80 GiB total capacity; 5.21 GiB already allocated; 
173.38 MiB free; 5.27 GiB reserved in total by PyTorch) 
If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation. 
See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
</code></pre></div><p>解决：</p>
<blockquote>
<p>最优设置策略：将max_split_size_mb设置为小于OOM发生时的显存请求大小最小值的最大整数值，
这里请求是1024MB所以可以设置为1024MB，<code>PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024</code>。</p>
</blockquote>
<h2 id="华为昇腾llm大模型部署arm架构-麒麟v10系统">华为昇腾LLM大模型部署(ARM架构 麒麟v10系统)</h2>
<h3 id="准备工作-1">准备工作</h3>
<h4 id="安装docker">安装docker</h4>
<p>安装docker，参考链接：<a href="https://little-star.love/posts/6da98871/">https://little-star.love/posts/6da98871/</a></p>
<p>麒麟v10系统aarch64通过yum安装docker的话会有不少依赖问题，所以这里通过编译好的二进制直接部署安装。</p>
<p>1.下载docker压缩包，拷贝二进制到<code>/usr/bin/</code>目录下</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#6272a4"># wget -c https://download.docker.com/linux/static/stable/aarch64/docker-20.10.9.tgz</span>
<span style="color:#6272a4"># tar xf docker-20.10.9.tgz</span>
<span style="color:#6272a4"># cp -p docker/* /usr/bin/</span>
</code></pre></div><p>2.添加docker.service systemd启动脚本</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#6272a4"># vim /usr/lib/systemd/system/docker.service</span>

<span style="color:#ff79c6">[</span>Unit<span style="color:#ff79c6">]</span>
<span style="color:#8be9fd;font-style:italic">Description</span><span style="color:#ff79c6">=</span>Docker Application Container Engine
<span style="color:#8be9fd;font-style:italic">Documentation</span><span style="color:#ff79c6">=</span>https://docs.docker.com
<span style="color:#8be9fd;font-style:italic">After</span><span style="color:#ff79c6">=</span>network-online.target docker.socket firewalld.service containerd.service
<span style="color:#8be9fd;font-style:italic">Wants</span><span style="color:#ff79c6">=</span>network-online.target
<span style="color:#8be9fd;font-style:italic">Requires</span><span style="color:#ff79c6">=</span>docker.socket containerd.service

<span style="color:#ff79c6">[</span>Service<span style="color:#ff79c6">]</span>
<span style="color:#8be9fd;font-style:italic">Type</span><span style="color:#ff79c6">=</span>notify
<span style="color:#6272a4"># the default is not to use systemd for cgroups because the delegate issues still</span>
<span style="color:#6272a4"># exists and systemd currently does not support the cgroup feature set required</span>
<span style="color:#6272a4"># for containers run by docker</span>
<span style="color:#8be9fd;font-style:italic">ExecStart</span><span style="color:#ff79c6">=</span>/usr/bin/dockerd -H fd:// --containerd<span style="color:#ff79c6">=</span>/run/containerd/containerd.sock
<span style="color:#8be9fd;font-style:italic">ExecReload</span><span style="color:#ff79c6">=</span>/bin/kill -s HUP <span style="color:#8be9fd;font-style:italic">$MAINPID</span>
<span style="color:#8be9fd;font-style:italic">TimeoutStartSec</span><span style="color:#ff79c6">=</span><span style="color:#bd93f9">0</span>
<span style="color:#8be9fd;font-style:italic">RestartSec</span><span style="color:#ff79c6">=</span><span style="color:#bd93f9">2</span>
<span style="color:#8be9fd;font-style:italic">Restart</span><span style="color:#ff79c6">=</span>always

<span style="color:#6272a4"># Note that StartLimit* options were moved from &#34;Service&#34; to &#34;Unit&#34; in systemd 229.</span>
<span style="color:#6272a4"># Both the old, and new location are accepted by systemd 229 and up, so using the old location</span>
<span style="color:#6272a4"># to make them work for either version of systemd.</span>
<span style="color:#8be9fd;font-style:italic">StartLimitBurst</span><span style="color:#ff79c6">=</span><span style="color:#bd93f9">3</span>

<span style="color:#6272a4"># Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.</span>
<span style="color:#6272a4"># Both the old, and new name are accepted by systemd 230 and up, so using the old name to make</span>
<span style="color:#6272a4"># this option work for either version of systemd.</span>
<span style="color:#8be9fd;font-style:italic">StartLimitInterval</span><span style="color:#ff79c6">=</span>60s

<span style="color:#6272a4"># Having non-zero Limit*s causes performance problems due to accounting overhead</span>
<span style="color:#6272a4"># in the kernel. We recommend using cgroups to do container-local accounting.</span>
<span style="color:#8be9fd;font-style:italic">LimitNOFILE</span><span style="color:#ff79c6">=</span>infinity
<span style="color:#8be9fd;font-style:italic">LimitNPROC</span><span style="color:#ff79c6">=</span>infinity
<span style="color:#8be9fd;font-style:italic">LimitCORE</span><span style="color:#ff79c6">=</span>infinity

<span style="color:#6272a4"># Comment TasksMax if your systemd version does not support it.</span>
<span style="color:#6272a4"># Only systemd 226 and above support this option.</span>
<span style="color:#8be9fd;font-style:italic">TasksMax</span><span style="color:#ff79c6">=</span>infinity

<span style="color:#6272a4"># set delegate yes so that systemd does not reset the cgroups of docker containers</span>
<span style="color:#8be9fd;font-style:italic">Delegate</span><span style="color:#ff79c6">=</span>yes

<span style="color:#6272a4"># kill only the docker process, not all processes in the cgroup</span>
<span style="color:#8be9fd;font-style:italic">KillMode</span><span style="color:#ff79c6">=</span>process
<span style="color:#8be9fd;font-style:italic">OOMScoreAdjust</span><span style="color:#ff79c6">=</span>-500

<span style="color:#ff79c6">[</span>Install<span style="color:#ff79c6">]</span>
<span style="color:#8be9fd;font-style:italic">WantedBy</span><span style="color:#ff79c6">=</span>multi-user.target
</code></pre></div><p>3.添加docker.socket systemd启动脚本</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#6272a4"># vim /usr/lib/systemd/system/docker.socket</span>

<span style="color:#ff79c6">[</span>Unit<span style="color:#ff79c6">]</span>
<span style="color:#8be9fd;font-style:italic">Description</span><span style="color:#ff79c6">=</span>Docker Socket <span style="color:#ff79c6">for</span> the API

<span style="color:#ff79c6">[</span>Socket<span style="color:#ff79c6">]</span>
<span style="color:#6272a4"># If /var/run is not implemented as a symlink to /run, you may need to</span>
<span style="color:#6272a4"># specify ListenStream=/var/run/docker.sock instead.</span>
<span style="color:#8be9fd;font-style:italic">ListenStream</span><span style="color:#ff79c6">=</span>/run/docker.sock
<span style="color:#8be9fd;font-style:italic">SocketMode</span><span style="color:#ff79c6">=</span><span style="color:#bd93f9">0660</span>
<span style="color:#8be9fd;font-style:italic">SocketUser</span><span style="color:#ff79c6">=</span>root
<span style="color:#8be9fd;font-style:italic">SocketGroup</span><span style="color:#ff79c6">=</span>docker

<span style="color:#ff79c6">[</span>Install<span style="color:#ff79c6">]</span>
<span style="color:#8be9fd;font-style:italic">WantedBy</span><span style="color:#ff79c6">=</span>sockets.target
</code></pre></div><p>4.添加containerd.service systemd启动脚本</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#6272a4"># vim /usr/lib/systemd/system/containerd.service</span>

<span style="color:#ff79c6">[</span>Unit<span style="color:#ff79c6">]</span>
<span style="color:#8be9fd;font-style:italic">Description</span><span style="color:#ff79c6">=</span>containerd container runtime
<span style="color:#8be9fd;font-style:italic">Documentation</span><span style="color:#ff79c6">=</span>https://containerd.io
<span style="color:#8be9fd;font-style:italic">After</span><span style="color:#ff79c6">=</span>network.target local-fs.target

<span style="color:#ff79c6">[</span>Service<span style="color:#ff79c6">]</span>
<span style="color:#8be9fd;font-style:italic">ExecStartPre</span><span style="color:#ff79c6">=</span>-/sbin/modprobe overlay
<span style="color:#8be9fd;font-style:italic">ExecStart</span><span style="color:#ff79c6">=</span>/usr/bin/containerd

<span style="color:#8be9fd;font-style:italic">Type</span><span style="color:#ff79c6">=</span>notify
<span style="color:#8be9fd;font-style:italic">Delegate</span><span style="color:#ff79c6">=</span>yes
<span style="color:#8be9fd;font-style:italic">KillMode</span><span style="color:#ff79c6">=</span>process
<span style="color:#8be9fd;font-style:italic">Restart</span><span style="color:#ff79c6">=</span>always
<span style="color:#8be9fd;font-style:italic">RestartSec</span><span style="color:#ff79c6">=</span><span style="color:#bd93f9">5</span>
<span style="color:#6272a4"># Having non-zero Limit*s causes performance problems due to accounting overhead</span>
<span style="color:#6272a4"># in the kernel. We recommend using cgroups to do container-local accounting.</span>
<span style="color:#8be9fd;font-style:italic">LimitNPROC</span><span style="color:#ff79c6">=</span>infinity
<span style="color:#8be9fd;font-style:italic">LimitCORE</span><span style="color:#ff79c6">=</span>infinity
<span style="color:#8be9fd;font-style:italic">LimitNOFILE</span><span style="color:#ff79c6">=</span>infinity
<span style="color:#6272a4"># Comment TasksMax if your systemd version does not supports it.</span>
<span style="color:#6272a4"># Only systemd 226 and above support this version.</span>
<span style="color:#8be9fd;font-style:italic">TasksMax</span><span style="color:#ff79c6">=</span>infinity
<span style="color:#8be9fd;font-style:italic">OOMScoreAdjust</span><span style="color:#ff79c6">=</span>-999

<span style="color:#ff79c6">[</span>Install<span style="color:#ff79c6">]</span>
<span style="color:#8be9fd;font-style:italic">WantedBy</span><span style="color:#ff79c6">=</span>multi-user.target
</code></pre></div><p>5.启动docker服务</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#6272a4"># groupadd docker</span>
<span style="color:#6272a4"># systemctl daemon-reload</span>
<span style="color:#6272a4"># systemctl start docker</span>
<span style="color:#6272a4"># systemctl enable docker</span>
</code></pre></div><p>6.验证docker服务</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#6272a4"># docker info</span>

Client:
 Context:    default
 Debug Mode: <span style="color:#8be9fd;font-style:italic">false</span>

Server:
 Containers: <span style="color:#bd93f9">0</span>
  Running: <span style="color:#bd93f9">0</span>
  Paused: <span style="color:#bd93f9">0</span>
  Stopped: <span style="color:#bd93f9">0</span>
 Images: <span style="color:#bd93f9">0</span>
 Server Version: 20.10.9
 Storage Driver: overlay2
  Backing Filesystem: xfs
  Supports d_type: <span style="color:#8be9fd;font-style:italic">true</span>
  Native Overlay Diff: <span style="color:#8be9fd;font-style:italic">true</span>
  userxattr: <span style="color:#8be9fd;font-style:italic">false</span>
 Logging Driver: json-file
 Cgroup Driver: cgroupfs
 Cgroup Version: <span style="color:#bd93f9">1</span>
 Plugins:
  Volume: <span style="color:#8be9fd;font-style:italic">local</span>
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file <span style="color:#8be9fd;font-style:italic">local</span> logentries splunk syslog
 Swarm: inactive
 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: 5b46e404f6b9f661a205e28d59c982d3634148f8
 runc version: v1.0.2-0-g52b36a2d
 init version: de40ad0
 Security Options:
  seccomp
   Profile: default
 Kernel Version: 4.19.90-23.8.v2101.ky10.aarch64
 Operating System: Kylin Linux Advanced Server V10 <span style="color:#ff79c6">(</span>Tercel<span style="color:#ff79c6">)</span>
 OSType: linux
 Architecture: aarch64
 CPUs: <span style="color:#bd93f9">128</span>
 Total Memory: 1.996TiB
 Name: localhost.localdomain
 ID: PI7U:7E6V:4CJA:YEJP:KXIU:YJ4S:24JA:CS6L:ONVR:FNMO:PCHW:RNJ7
 Docker Root Dir: /var/lib/docker
 Debug Mode: <span style="color:#8be9fd;font-style:italic">false</span>
 Registry: https://index.docker.io/v1/
 Labels:
 Experimental: <span style="color:#8be9fd;font-style:italic">false</span>
 Insecure Registries:
  127.0.0.0/8
 Live Restore Enabled: <span style="color:#8be9fd;font-style:italic">false</span>
 Product License: Community Engine
</code></pre></div><p>[todo]
昇腾适配待补充</p>
<h1 id="llm大模型应用---qanything知识库">LLM大模型应用 - QAnything知识库</h1>
<p>QAnything版本：d5c59260a1af74f01a9b378b567435ab0caa40df</p>
<h2 id="linux环境离线安装">Linux环境离线安装</h2>
<p>docker、docker-compose预装好</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#6272a4"># 先在联网机器上下载docker镜像</span>
docker pull quay.io/coreos/etcd:v3.5.5
docker pull minio/minio:RELEASE.2023-03-20T20-16-18Z
docker pull milvusdb/milvus:v2.3.4
docker pull mysql:latest
docker pull freeren/qanything:v1.2.1

<span style="color:#6272a4"># 打包镜像</span>
docker save quay.io/coreos/etcd:v3.5.5 minio/minio:RELEASE.2023-03-20T20-16-18Z milvusdb/milvus:v2.3.4 mysql:latest freeren/qanything:v1.2.1 -o qanything_offline.tar

<span style="color:#6272a4"># 下载QAnything代码</span>
wget https://github.com/netease-youdao/QAnything/archive/refs/heads/master.zip

<span style="color:#6272a4"># 把镜像qanything_offline.tar和代码QAnything-master.zip拷贝到断网机器上</span>
cp master.zip qanything_offline.tar /path/to/your/offline/machine

<span style="color:#6272a4"># 在断网机器上加载镜像</span>
docker load -i qanything_offline.tar

<span style="color:#6272a4"># 解压代码，运行</span>
unzip master.zip
<span style="color:#8be9fd;font-style:italic">cd</span> QAnything-master
bash run.sh
</code></pre></div><p>脚本运行过程中，会产生交互让你输入remote远程服务器安装还是local本地化安装</p>
<h1 id="llm大模型评测">LLM大模型评测</h1>
<p><a href="https://ai-bot.cn/">https://ai-bot.cn/</a>，这个网站类似AI工具的链接导航，有MMLU、Open LLM Leaderboard、OpenCompass等；其中中文大模型评测的话，推荐OpenCompass，国产
大模型阿里通义千问在其github展示的评测表现也是基于OpenCompass.</p>
<h1 id="llm大模型推理">LLM大模型推理</h1>
<p>量化：模型使用16位浮点数作为权重进行训练，可以将其缩小到4位整数以进行推理，而不会失去太多的功率，收益是会节省大量的GPU计算资源。</p>
<p>llama.cpp主要解决的是推理过程中的性能问题。主要有两点优化：</p>
<ul>
<li>llama.cpp使用的是C/C++语言写的机器学习张量库ggml</li>
<li>llama.cpp提供了模型量化的工具</li>
</ul>
<h1 id="参考链接">参考链接</h1>
<ul>
<li><a href="https://datawhalechina.github.io/prompt-engineering-for-developers/#/README">面向开发者的LLM入门教程</a></li>
<li><a href="https://github.com/xusenlinzy/api-for-open-llm/blob/master/docs/SCRIPT.md">https://github.com/xusenlinzy/api-for-open-llm/blob/master/docs/SCRIPT.md</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/641999400">https://zhuanlan.zhihu.com/p/641999400</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/111235300">NVidia Docker介绍</a></li>
<li><a href="https://python.langchain.com/docs/get_started/introduction">https://python.langchain.com/docs/get_started/introduction</a></li>
<li><a href="https://www.chenshaowen.com/blog/llama-cpp-that-is-a-llm-deployment-tool.html">llama.cpp部署实践</a></li>
</ul>


                
                
<div class="entry-shang text-center">
    
	    <p>「真诚赞赏，手留余香」</p>
	
	<button class="zs show-zs btn btn-bred">赞赏支持</button>
</div>
<div class="zs-modal-bg"></div>
<div class="zs-modal-box">
	<div class="zs-modal-head">
		<button type="button" class="close">×</button>
		<span class="author"><a href="https://www.iceyao.com.cn"><img src="/img/favicon.png" />爱折腾的工程师</a></span>
        
	        <p class="tip"><i></i><span>真诚赞赏，手留余香</span></p>
		
 
	</div>
	<div class="zs-modal-body">
		<div class="zs-modal-btns">
			<button class="btn btn-blink" data-num="2">2元</button>
			<button class="btn btn-blink" data-num="5">5元</button>
			<button class="btn btn-blink" data-num="10">10元</button>
			<button class="btn btn-blink" data-num="50">50元</button>
			<button class="btn btn-blink" data-num="100">100元</button>
			<button class="btn btn-blink" data-num="1">任意金额</button>
		</div>
		<div class="zs-modal-pay">
			<button class="btn btn-bred" id="pay-text">2元</button>
			<p>使用<span id="pay-type">微信</span>扫描二维码完成支付</p>
			<img src="/img/reward/wechat-2.png"  id="pay-image"/>
		</div>
	</div>
	<div class="zs-modal-footer">
		<label><input type="radio" name="zs-type" value="wechat" class="zs-type" checked="checked"><span ><span class="zs-wechat"><img src="/img/reward/wechat-btn.png"/></span></label>
		<label><input type="radio" name="zs-type" value="alipay" class="zs-type" class="zs-alipay"><img src="/img/reward/alipay-btn.png"/></span></label>
	</div>
</div>
<script type="text/javascript" src="/js/reward.js"></script>

                

                
                <hr>
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2023/07/19/gitlab-ee-readnotes/" data-toggle="tooltip" data-placement="top" title="Gitlab极狐版体验笔记">&larr;
                            Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2024/03/21/python-knowledge/" data-toggle="tooltip" data-placement="top" title="Python知识点大杂烩">Next
                            Post &rarr;</a>
                    </li>
                    
                </ul>
                

                


<script src="https://giscus.app/client.js"
        data-repo="yaoice/yaoice.github.io"
        data-repo-id="R_kgDOJnxqVg"
        data-category="General"
        data-category-id="DIC_kwDOJnxqVs4CWwUs"
        data-mapping="pathname"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>


            </div>

            
            
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
            

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                        
                        
                        
                        
                        
                        <a href="/tags/devops" title="devops">
                            devops
                        </a>
                        
                        
                        
                        <a href="/tags/go" title="go">
                            go
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/k8s" title="k8s">
                            k8s
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/openstack" title="openstack">
                            openstack
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/tkestack" title="tkestack">
                            tkestack
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E7%BB%83%E8%BD%A6" title="练车">
                            练车
                        </a>
                        
                        
                    </div>
                </section>
                

                
                
            </div>
        </div>
    </div>
</article>




<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">                  
                    
                    <li>
                        <a href="mailto:yao3690093@gmail.com">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		           
                    
                    
                    
                    

		            
                    
                    <li>
                        <a target="_blank" href="/img/wechat.jpeg">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-weixin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		            
                    
                    <li>
                        <a target="_blank" href="https://github.com/yaoice">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		            
                    
                    
                    
                    
                    
                    
                    
                    
                    
            
            
            
           
                   <li>
                       <a href='' rel="alternate" type="application/rss+xml" title="爱折腾的工程师" >
                           <span class="fa-stack fa-lg">
                               <i class="fas fa-circle fa-stack-2x"></i>
                               <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                           </span>
                       </a>
                   </li>
            
             </ul>
		<p class="copyright text-muted">
                    Copyright &copy; 爱折腾的工程师 2024
                </p>
            </div>
        </div>
    </div>
</footer>




<script>
    function loadAsync(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>






<script>
    
    if($('#tag_cloud').length !== 0){
        loadAsync("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>


<script>
    loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>



<script>
    (function(){
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https'){
       bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else{
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>


<script>
    
    var _baId = '92c175994ded75a3cd2074bc1123e2be';

    
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>




<script type="text/javascript">
    function generateCatalog(selector) {

        
        
        
        
            _containerSelector = 'div.post-container'
        

        
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        
        $(selector).html('')

        
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    


    loadAsync("\/js\/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>






</body>
</html>
