<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="爱折腾的工程师"><meta property="og:type" content="article"><meta property="og:image" content="https://www.iceyao.com.cn//img/post-bg-unix-linux.jpg"><meta property="twitter:image" content="https://www.iceyao.com.cn//img/post-bg-unix-linux.jpg"><meta name=title content="LLM学习笔记"><meta property="og:title" content="LLM学习笔记"><meta property="twitter:title" content="LLM学习笔记"><meta name=description content="LLM学习笔记"><meta property="og:description" content="LLM学习笔记"><meta property="twitter:description" content="LLM学习笔记"><meta property="twitter:card" content="summary"><meta name=keyword content="iceyao, IceYao's Blog, 博客, 个人网站, 互联网, Web, 云原生, PaaS, Istio, Kubernetes, 微服务, Microservice"><link rel="shortcut icon" href=/img/favicon.ico><title>LLM学习笔记 | 爱折腾的工程师 | IceYao's Blog</title>
<link rel=canonical href=/2024/01/18/llm-develop-readnotes/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-9J7CKFVPPM"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-9J7CKFVPPM")}</script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>爱折腾的工程师</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/notes//>NOTES</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(/img/post-bg-unix-linux.jpg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/llm title=LLM>LLM</a></div><h1>LLM学习笔记</h1><h2 class=subheading></h2><span class=meta>Posted by
iceyao
on
Thursday, January 18, 2024</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=背景>背景</h1><p>记录LLM学习的过程，前两章实践内容:面向开发者的提示工程和使用LangChain开发应用程序来源于<a href=https://datawhalechina.github.io/llm-cookbook>Github开源项目llm-cookbook</a>，其它章节来源于官网、开源项目等实践记录。</p><h1 id=环境配置>环境配置</h1><p>1.申请OpenAI API Key</p><p>在OpenAI官网注册的账号默认有18美金的额度用于消耗token，3个月内有效。国内也有免费的内测Key供申请
<code>https://github.com/chatanywhere/GPT_API_free</code>，需要绑定Github账号来申请。</p><p>2.Golang tool库封装</p><p>封装一个CreateChatCompletion函数，放在同个package下引用</p><pre tabindex=0><code>package main

import (
	&#34;context&#34;
	// 官方推荐的第三方OpenAl golang sdk
	&#34;github.com/sashabaranov/go-openai&#34;
)

const (
	Token          = &#34;&lt;填入你的token&gt;&#34;
	// 使用国内免费申请的Key的话，就用这个host用于转发请求
	OpenAIProxyURL = &#34;https://api.chatanywhere.com.cn/v1&#34;
)

type openAIClient struct {
	*openai.Client
}

func newOpenAIClient() *openAIClient {
	config := openai.DefaultConfig(Token)
	config.BaseURL = OpenAIProxyURL
	c := openai.NewClientWithConfig(config)
	return &amp;openAIClient{c}
}

func (c *openAIClient) CreateChatCompletion(ctx context.Context, content string) (openai.ChatCompletionResponse, error) {
	return c.Client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
		Model: openai.GPT3Dot5Turbo,
		Messages: []openai.ChatCompletionMessage{
			{
				Role:    openai.ChatMessageRoleUser,
				Content: content,
			},
		},
		Temperature: 0,
	})
}
</code></pre><h1 id=面向开发者的提示工程>面向开发者的提示工程</h1><p>随着ChatGPT为代表的大语言大模型大爆发出现之后，Prompt已经成为与大模型输入的代称。一般将大模型的输入称为<code>Prompt</code>，将大模型的返回输出称为<code>Completion</code>。
合理的Prompt设计决定了大模型能力的上限和下限，学会充分、高效使用LLM，Prompt Engineering的技能强烈需要。<code>Prompt Engineering</code>是针对特定任务构造充分
发挥大模型能力的Prompt的技巧。</p><h2 id=简介introduction>简介Introduction</h2><p>随着LLM的发展，大致分为两种类型：</p><ul><li>基础LLM：基于文本训练数据，训练出预测下一个单词能力的模型。通常通过在互联网和其他来源的大量数据上训练，来确定紧接着出现的最可能的词。</li><li>指令微调LLM：通过专门的训练，可以更好地理解并遵循指令。指令微调LLM的训练通常基于预训练语言模型，先在大规模文本数据上进行预训练，掌握语言的基本规律。
在此基础上进行进一步的训练与微调（finetune），输入是指令，输出是对这些指令的正确回复。还可以通过RLHF（人类反馈强化学习）技术，增强模型的能力。</li></ul><p>这里的实践重点介绍针对指令微调LLM的最佳实践。</p><h2 id=提示原则guidelines>提示原则Guidelines</h2><p>高效Prompt的两个关键原则：</p><ul><li>编写清晰、具体的指令；清晰明确地表达需求，提供足够的上下文，让大语言模型准确理解需求。</li><li>给予模型充足思考时间；加入逐步推理的要求，给模型留充分思考时间，生成的结果更准确可靠。</li></ul><p>掌握这两个关键原则，是大语言模型成功的重要基石。</p><h3 id=编写清晰具体的指令>编写清晰具体的指令</h3><h4 id=使用分隔符>使用分隔符</h4><p>分隔符可以将不同的指令、上下文、输入隔开，防止<code>提示词注入</code>，输入的文本可能包含与预设的Prompt冲突的内容，如果不加分隔，这些输入可能扰乱模型的输出。</p><p>示例：给出一段话，让GPT进行总结，使用"&ldquo;作为分隔符</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>您应该提供尽可能清晰、具体的指示，以表达您希望模型执行的任务。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>这将引导模型朝向所需的输出，并降低收到无关或不正确响应的可能性。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>不要将写清晰的提示词与写简短的提示词混淆。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>在许多情况下，更长的提示词可以为模型提供更多的清晰度和上下文信息，从而导致更详细和相关的输出。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    `</span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	把用两个双引号括起来的文本总结成一句话。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	&#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	`</span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>提供清晰、具体的指示可以引导模型朝向所需的输出，避免无关或不正确的响应。长度更长的提示词可以提供更多的清晰度和上下文信息，导致更详细和相关的输出。
</code></pre><h4 id=结构化输出>结构化输出</h4><p>结构化输出，指的是类似json、html等结构。</p><p>示例：让GPT生成三本书的标题、作者和类别，并以JSON格式返回，其中JSON的key已指定</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	请生成包括书名、作者和类别的三本虚构的、非真实存在的中文书籍清单，\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>并以JSON格式提供，其中包含以下键:book_id、title、author、genre。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), prompt)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>[
  {
    &#34;book_id&#34;: 1,
    &#34;title&#34;: &#34;夜色旅人&#34;,
    &#34;author&#34;: &#34;张三&#34;,
    &#34;genre&#34;: &#34;奇幻&#34;
  },
  {
    &#34;book_id&#34;: 2,
    &#34;title&#34;: &#34;梦境之城&#34;,
    &#34;author&#34;: &#34;李四&#34;,
    &#34;genre&#34;: &#34;科幻&#34;
  },
  {
    &#34;book_id&#34;: 3,
    &#34;title&#34;: &#34;幻想之门&#34;,
    &#34;author&#34;: &#34;王五&#34;,
    &#34;genre&#34;: &#34;魔幻&#34;
  }
]
</code></pre><h4 id=模型检查>模型检查</h4><p>如果任务包含不一定能满足的条件，可以让模型检查这些条件，如果不满足，可以让其停止执行后续的流程。可以加入一些边界情况的考虑，以避免意外的结果或错误发生。</p><p>示例：分别给模型两段文本，一是制作茶的步骤，二是一段没有明确步骤的文本。要求模型判断是否其包含一系列指令，包含则按照给定格式重新编写指令，不包含则返回"未提供步骤&rdquo;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#6272a4>// 制作茶的步骤
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text1 <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	泡一杯茶很容易。首先，需要把水烧开。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>在等待期间，拿一个杯子并把茶包放进去。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>一旦水足够热，就把它倒在茶包上。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>等待一会儿，让茶叶浸泡。几分钟后，取出茶包。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>如果您愿意，可以加一些糖或牛奶调味。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>就这样，您可以享受一杯美味的茶了。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>您将获得由两个双引号括起来的文本。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>如果它包含一系列的指令，则需要按照以下格式重新编写这些指令：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>第一步 - ...
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>第二步 - …
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>…
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>第N步 - …
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>如果文本中不包含一系列的指令，则直接写“未提供步骤”。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text1))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>第一步 - 把水烧开。
第二步 - 拿一个杯子并把茶包放进去。
第三步 - 把热水倒在茶包上。
第四步 - 等待几分钟让茶叶浸泡，然后取出茶包。
第五步 - 可以选择加入糖或牛奶调味。
第六步 - 就这样，您可以享受一杯美味的茶了。
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#6272a4>// 一段没有明确步骤的文本
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text2 <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	泡一杯茶很容易。首先，需要把水烧开。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>在等待期间，拿一个杯子并把茶包放进去。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>一旦水足够热，就把它倒在茶包上。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>等待一会儿，让茶叶浸泡。几分钟后，取出茶包。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>如果您愿意，可以加一些糖或牛奶调味。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>就这样，您可以享受一杯美味的茶了。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>您将获得由两个双引号括起来的文本。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>如果它包含一系列的指令，则需要按照以下格式重新编写这些指令：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>第一步 - ...
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>第二步 - …
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>…
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>第N步 - …
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>如果文本中不包含一系列的指令，则直接写“未提供步骤”。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text2))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>未提供步骤
</code></pre><h4 id=提供少量示例>提供少量示例</h4><p>Few-shot prompting，在模型执行具体任务之前，给模型1～2个样例，让模型了解要求和期望输出的样式</p><p>示例：利用少量样例，可以预热语言模型，这是一个让模型快速上手新任务的有效策略。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>您的任务是以一致的风格回答问题。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;孩子&gt;: 请教我何为耐心。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;祖父母&gt;: 挖出最深峡谷的河流源于一处不起眼的泉眼；最宏伟的交响乐从单一的音符开始；最复杂的挂毯以一根孤独的线开始编织。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;孩子&gt;: 请教我何为韧性。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), prompt)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>&lt;祖父母&gt;: 韧性就像一根弹簧，能够在压力和挑战面前保持弹性和坚韧。它是一种适应和持久的能力，能够在困的本质。
</code></pre><h3 id=给模型时间思考>给模型时间思考</h3><p>给模型充足的时间思考，可以提高模型的准确性，好比让一个人在短时间内去解决一个非常棘手的问题，难度可见之大。</p><h4 id=指定完成任务的步骤>指定完成任务的步骤</h4><p>示例：描述了杰克和吉尔的故事，并给出提示词执行以下操作：首先，用一句话概括三个反引号限定的文本。第二，将摘要翻译成英语。第三，在英语摘要中列出每个名称。
第四，输出包含以下键的JSON对象：英语摘要和人名个数。要求输出以换行符分隔</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>在一个迷人的村庄里，兄妹杰克和吉尔出发去一个山顶井里打水。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>他们一边唱着欢乐的歌，一边往上爬，\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>然而不幸降临——杰克绊了一块石头，从山上滚了下来，吉尔紧随其后。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>虽然略有些摔伤，但他们还是回到了温馨的家中。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>尽管出了这样的意外，他们的冒险精神依然没有减弱，继续充满愉悦地探索。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>执行以下操作：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>1-用一句话概括下面用两个双反引号括起来的文本。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>2-将摘要翻译成英语。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>3-在英语摘要中列出每个人名。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>4-输出一个 JSON 对象，其中包含以下键：english_summary，num_names。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>请用换行符分隔您的答案。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>1- 兄妹在迷人的村庄冒险后受伤但仍然乐观。
2- Siblings Jack and Jill set off to fetch water from a well on a mountaintop in a charming village.
3- Jack, Jill
4- {
   &#34;english_summary&#34;: &#34;Siblings Jack and Jill set off to fetch water from a well on a mountaintop in a charming village. Despite getting injured in an unfortunate accident, they still maintained their adventurous spirit and continued to explore joyfully.&#34;,
   &#34;num_names&#34;: 2
}
</code></pre><p>可以将prompt改进，确切指定输出格式</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>在一个迷人的村庄里，兄妹杰克和吉尔出发去一个山顶井里打水。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>他们一边唱着欢乐的歌，一边往上爬，\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>然而不幸降临——杰克绊了一块石头，从山上滚了下来，吉尔紧随其后。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>虽然略有些摔伤，但他们还是回到了温馨的家中。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>尽管出了这样的意外，他们的冒险精神依然没有减弱，继续充满愉悦地探索。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>1-用一句话概括下面用双引号括起来的文本。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>2-将摘要翻译成英语。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>3-在英语摘要中列出每个名称。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>4-输出一个 JSON 对象，其中包含以下键：English_summary，num_names。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>请使用以下格式：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>文本：&lt;要总结的文本&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>摘要：&lt;摘要&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>翻译：&lt;摘要的翻译&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>名称：&lt;英语摘要中的名称列表&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>输出 JSON：&lt;带有 English_summary 和 num_names 的 JSON&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>摘要：故事讲述了杰克和吉尔在山顶井打水时发生意外，但他们的冒险精神依然没有减弱。

翻译：Summary: The story tells about an accident that happened to Jack and Jill while they were geng water from a well on the mountaintop, but their adventurous spirit remained undiminished.

名称：Jack, Jill

输出 JSON：{&#34;English_summary&#34;: &#34;The story tells about an accident that happened to Jack and Jill we they were getting water from a well on the mountaintop, but their adventurous spirit remained undiminished.&#34;, &#34;num_names&#34;: 2}
</code></pre><h4 id=指导模型自主思考>指导模型自主思考</h4><p>设计prompt时，可以通过明确指导语言模型进行自主思考。在prompt中要求语言模型先自己尝试解决问题，思考对应的解决方法，再与提供的解答进行对比，校验正确性。</p><p>示例：给出一个问题和一份来自学生的解答，要求模型判断解答是否正确</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>判断学生的解决方案是否正确。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>问题:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>我正在建造一个太阳能发电站，需要帮助计算财务。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    土地费用为 100美元/平方英尺
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    我可以以 250美元/平方英尺的价格购买太阳能电池板
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    我已经谈判好了维护合同，每年需要支付固定的10万美元，并额外支付每平方英尺10美元
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    作为平方英尺数的函数，首年运营的总费用是多少。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>学生的解决方案：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>设x为发电站的大小，单位为平方英尺。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>费用：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    土地费用：100x
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    太阳能电池板费用：250x
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    维护费用：100,000美元+100x
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    总费用：100x+250x+100,000美元+100x=450x+100,000美元
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), prompt)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>学生的解决方案是正确的。总费用可以表示为450x+100,000美元。
</code></pre><p>实际上这个解决方案是错误的，总费用应该是360x+100,000美元，因为没让模型自行先计算，然后跟学生的解决方法对比，那么就有可能误导模型以为学生的解法就是正确的。
接下来调整思路，让模型先自行解决这个问题，再根据自己的解法跟学生的解法进行对比，然后再判断学生的解法是否正确。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>请判断学生的解决方案是否正确，请通过如下步骤解决这个问题：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>步骤：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    首先，自己解决问题。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    然后将您的解决方案与学生的解决方案进行比较，对比计算得到的总费用与学生计算的总费用是否一致，并评估学生的解决方案是否正确。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    在自己完成问题之前，请勿决定学生的解决方案是否正确。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>使用以下格式：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    问题：问题文本
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    学生的解决方案：学生的解决方案文本
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    实际解决方案和步骤：实际解决方案和步骤文本
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    学生计算的总费用：学生计算得到的总费用
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    实际计算的总费用：实际计算出的总费用
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    学生计算的费用和实际计算的费用是否相同：是或否
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    学生的解决方案和实际解决方案是否相同：是或否
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    学生的成绩：正确或不正确
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>问题：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    我正在建造一个太阳能发电站，需要帮助计算财务。 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - 土地费用为每平方英尺100美元
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - 我可以以每平方英尺250美元的价格购买太阳能电池板
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - 我已经谈判好了维护合同，每年需要支付固定的10万美元，并额外支付每平方英尺10美元;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    作为平方英尺数的函数，首年运营的总费用是多少。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>学生的解决方案：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    设x为发电站的大小，单位为平方英尺。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    费用：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    1. 土地费用：100x美元
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    2. 太阳能电池板费用：250x美元
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    3. 维护费用：100,000+100x=10万美元+10x美元
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    总费用：100x美元+250x美元+10万美元+100x美元=450x+10万美元
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>实际解决方案和步骤：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), prompt)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>首先，我们需要计算土地费用、太阳能电池板费用和维护费用的总和。

土地费用：100美元/平方英尺 * x平方英尺 = 100x美元
太阳能电池板费用：250美元/平方英尺 * x平方英尺 = 250x美元
维护费用：10万美元 + 10美元/平方英尺 * x平方英尺 = 10万美元 + 10x美元

总费用：100x美元 + 250x美元 + 10万美元 + 10x美元 = 360x + 10万美元

学生计算的总费用：450x + 10万美元
实际计算的总费用：360x + 10万美元
学生计算的费用和实际计算的费用是否相同：否
学生的解决方案和实际解决方案是否相同：否
学生的成绩：不正确
</code></pre><h3 id=局限性>局限性</h3><p>模型偶尔会生成一些看似真实，实际上不存在的知识。模型经过大量预训练，虽然掌握了丰富知识，但是难以判断自己的知识边界，可能会做出错误推断。这个现象称为幻觉（Hallucination），
是语言模型的一大陷阱。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>告诉我华为公司生产的GT Watch运动手表的相关信息
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), prompt)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>华为公司生产的GT Watch运动手表是一款运动智能手表，具有多项健康功能和运动追踪功能，如心率监测、睡眠 Watch还支持多种运动模式，包括跑步、骑行、游泳等，
可全面监测和分析用户的运动数据。此外，GT Watch还观，适合运动和日常佩戴。
</code></pre><p>可以通过Prompt设计减少幻觉的发生，比如让语言模型直接引用文本中的原句，然后再进行解答。语言模型的幻觉问题事关应用的可靠性与安全性。，采取prompt优化措施可以缓解，这也是未来语言模型
进化的重要方向之一。</p><h3 id=英文原版prompt>英文原版Prompt</h3><p>1.1 使用分隔符隔离不同输入部分</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	You should express what you want a model to do by \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>providing instructions that are as clear and \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>specific as you can possibly make them. \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>This will guide the model towards the desired output, \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>and reduce the chances of receiving irrelevant \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>or incorrect responses. Don&#39;t confuse writing a \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>clear prompt with writing a short prompt. \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>In many cases, longer prompts provide more clarity \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>and context for the model, which can lead to \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>more detailed and relevant outputs.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Summarize the text delimited by two double quotes \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>into a single sentence.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>Clear and specific instructions for a model will guide it towards the desired output, reducing the chances of irrelevant or incorrect responses, 
and longer prompts can provide more clarity and context, leading to more detailed and relevant outputs.
</code></pre><p>1.2 结构化输出</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Generate a list of three made-up book titles along \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>with their authors and genres. 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Provide them in JSON format with the following keys: 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>book_id, title, author, genre.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), prompt)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>{
  &#34;books&#34;: [
    {
      &#34;book_id&#34;: 1,
      &#34;title&#34;: &#34;The Forgotten Symphony&#34;,
      &#34;author&#34;: &#34;Madeline Harper&#34;,
      &#34;genre&#34;: &#34;Mystery&#34;
    },
    {
      &#34;book_id&#34;: 2,
      &#34;title&#34;: &#34;Echoes of Eternity&#34;,
      &#34;author&#34;: &#34;Jackson Pierce&#34;,
      &#34;genre&#34;: &#34;Science Fiction&#34;
    },
    {
      &#34;book_id&#34;: 3,
      &#34;title&#34;: &#34;The Enchanted Garden&#34;,
      &#34;author&#34;: &#34;Sophie Evans&#34;,
      &#34;genre&#34;: &#34;Fantasy&#34;
    }
  ]
}
</code></pre><p>1.3 模型检查是否满足条件</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text1 <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	Making a cup of tea is easy! First, you need to get some \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	water boiling. While that&#39;s happening, \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	grab a cup and put a tea bag in it. Once the water is \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	hot enough, just pour it over the tea bag. \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	Let it sit for a bit so the tea can steep. After a \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	few minutes, take out the tea bag. If you \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	like, you can add some sugar or milk to taste. \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	And that&#39;s it! You&#39;ve got yourself a delicious \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	cup of tea to enjoy.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	`</span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>You will be provided with text delimited by two double quotes. 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>If it contains a sequence of instructions, \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>re-write those instructions in the following format:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Step 1 - ...
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Step 2 - …
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>…
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Step N - …
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>If the text does not contain a sequence of instructions, \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>then simply write \&#34;No steps provided.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text1))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>Step 1 - Get some water boiling.
Step 2 - Grab a cup and put a tea bag in it.
Step 3 - Pour hot water over the tea bag.
Step 4 - Let the tea steep for a few minutes.
Step 5 - Remove the tea bag.
Step 6 - Add sugar or milk to taste.
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text2 <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>The sun is shining brightly today, and the birds are \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>singing. It&#39;s a beautiful day to go for a \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>walk in the park. The flowers are blooming, and the \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>trees are swaying gently in the breeze. People \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>are out and about, enjoying the lovely weather. \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Some are having picnics, while others are playing \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>games or simply relaxing on the grass. It&#39;s a \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>perfect day to spend time outdoors and appreciate the \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>beauty of nature.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	`</span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>You will be provided with text delimited by two double quotes. 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>If it contains a sequence of instructions, \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>re-write those instructions in the following format:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Step 1 - ...
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Step 2 - …
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>…
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Step N - …
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>If the text does not contain a sequence of instructions, \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>then simply write \&#34;No steps provided.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text2))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>No steps provided.
</code></pre><p>1.4 提供少量示例</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Your task is to answer in a consistent style.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;child&gt;: Teach me about patience.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;grandparent&gt;: The river that carves the deepest \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>valley flows from a modest spring; the \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>grandest symphony originates from a single note; \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>the most intricate tapestry begins with a solitary thread.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;child&gt;: Teach me about resilience.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), prompt)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>&lt;grandparent&gt;: Resilience is like the mighty oak that withstands the strongest of storms, bending but never breaking. It is the spirit that rises from adversity, the strength that perseveres in the face of challenges. Like the phoenix that rises from the ashes, resilience is the ability to bounce back and thrive, no matter the circumstances.
</code></pre><p>2.1 指定完成任务所需的步骤</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>In a charming village, siblings Jack and Jill set out on \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>a quest to fetch water from a hilltop \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>well. As they climbed, singing joyfully, misfortune \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>struck—Jack tripped on a stone and tumbled \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>down the hill, with Jill following suit. \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Though slightly battered, the pair returned home to \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>comforting embraces. Despite the mishap, \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>their adventurous spirits remained undimmed, and they \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>continued exploring with delight.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	`</span>
</span></span><span style=display:flex><span>	prompt1 <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Perform the following actions: 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>1 - Summarize the following text delimited by two double \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>backticks with 1 sentence.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>2 - Translate the summary into French.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>3 - List each name in the French summary.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>4 - Output a json object that contains the following \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>keys: french_summary, num_names.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Separate your answers with line breaks.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Text:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt1, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(<span style=color:#f1fa8c>&#34;Completion for prompt 1: \n&#34;</span>, resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>Completion for prompt 1: 
 1 - Jack and Jill, despite a mishap on their quest for water, returned home with adventurous spirits undimmed.

2 - Jack et Jill, malgré un accident dans leur quête d&#39;eau, sont rentrés chez eux avec des esprits aventureux intacts.

3 - Jack, Jill

4 - 
{
  &#34;french_summary&#34;: &#34;Jack et Jill, malgré un accident dans leur quête d&#39;eau, sont rentrés chez eux avec des esprits aventureux intacts.&#34;,
  &#34;num_names&#34;: 2
}
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>In a charming village, siblings Jack and Jill set out on \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>a quest to fetch water from a hilltop \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>well. As they climbed, singing joyfully, misfortune \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>struck—Jack tripped on a stone and tumbled \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>down the hill, with Jill following suit. \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Though slightly battered, the pair returned home to \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>comforting embraces. Despite the mishap, \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>their adventurous spirits remained undimmed, and they \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>continued exploring with delight.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>	`</span>
</span></span><span style=display:flex><span>	prompt2 <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Your task is to perform the following actions: 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>1 - Summarize the following text delimited by &lt;&gt; with 1 sentence.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>2 - Translate the summary into French.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>3 - List each name in the French summary.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>4 - Output a json object that contains the 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>following keys: french_summary, num_names.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Use the following format:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Text: &lt;text to summarize&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Summary: &lt;summary&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Translation: &lt;summary translation&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Names: &lt;list of names in French summary&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Output JSON: &lt;json with summary and num_names&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Text: &lt;%s&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt2, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(<span style=color:#f1fa8c>&#34;Completion for prompt 2: \n&#34;</span>, resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>Completion for prompt 2: 
 Summary: Jack and Jill, siblings, go on a quest to fetch water from a hilltop well but face misfortune on the way back home.

Translation: Jack et Jill, frère et sœur, partent à la quête d&#39;eau d&#39;un puits au sommet d&#39;une colline mais font face à la malchance en rentrant chez eux.

Names: Jack, Jill

Output JSON:
{
  &#34;french_summary&#34;: &#34;Jack et Jill, frère et sœur, partent à la quête d&#39;eau d&#39;un puits au sommet d&#39;une colline mais font face à la malchance en rentrant chez eux.&#34;,
  &#34;num_names&#34;: 2
}
</code></pre><p>2.2 指导模型在下结论之前找出一个自己的解法</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Determine if the student&#39;s solution is correct or not.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Question:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>I&#39;m building a solar power installation and I need \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c> help working out the financials. 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>- Land costs $100 / square foot
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>- I can buy solar panels for $250 / square foot
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>- I negotiated a contract for maintenance that will cost \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>me a flat $100k per year, and an additional $10 / square \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>foot
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>What is the total cost for the first year of operations 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>as a function of the number of square feet.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Student&#39;s Solution:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Let x be the size of the installation in square feet.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Costs:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>1. Land cost: 100x
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>2. Solar panel cost: 250x
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>3. Maintenance cost: 100,000 + 100x
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), prompt)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>The student&#39;s solution is correct. They have correctly calculated the total cost for the first year of operations as a function of the number of square feet.
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Your task is to determine if the student&#39;s solution \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>is correct or not.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>To solve the problem do the following:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>- First, work out your own solution to the problem. 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>- Then compare your solution to the student&#39;s solution \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>and evaluate if the student&#39;s solution is correct or not. 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Don&#39;t decide if the student&#39;s solution is correct until 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>you have done the problem yourself.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Use the following format:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Question:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>I&#39;m building a solar power installation and I need \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c> help working out the financials. 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>- Land costs $100 / square foot
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>- I can buy solar panels for $250 / square foot
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>- I negotiated a contract for maintenance that will cost \ 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>me a flat $100k per year, and an additional $10 / square \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>foot
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>What is the total cost for the first year of operations 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>as a function of the number of square feet.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Student&#39;s Solution:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Let x be the size of the installation in square feet.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Costs:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>1. Land cost: 100x
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>2. Solar panel cost: 250x
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>3. Maintenance cost: 100,000 + 100x
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Actual solution:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Is the student&#39;s solution the same as actual solution \
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>just calculated:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Student grade:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), prompt)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>Question:
I&#39;m building a solar power installation and I need help working out the financials. 
- Land costs $100 / square foot
- I can buy solar panels for $250 / square foot
- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot
What is the total cost for the first year of operations as a function of the number of square feet.

Student&#39;s Solution:
Let x be the size of the installation in square feet.
Costs:
1. Land cost: 100x
2. Solar panel cost: 250x
3. Maintenance cost: 100,000 + 10x
Total cost: 100x + 250x + 100,000 + 10x = 360x + 100,000

Actual solution:
Total cost = Land cost + Solar panel cost + Maintenance cost
Total cost = 100x + 250x + 100,000 + 10x
Total cost = 360x + 100,000

Is the student&#39;s solution the same as the actual solution just calculated:
No, the student&#39;s solution for the maintenance cost is incorrect. The correct maintenance cost is $100,000 + $10x, not $100,000 + 10x. Therefore, the student&#39;s total cost is also incorrect.

Student grade:
F. The student&#39;s solution is not correct. They made an error in calculating the maintenance cost.
</code></pre><p>3.1 幻觉</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), prompt)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>The AeroGlide UltraSlim Smart Toothbrush by Boie is a toothbrush designed to provide a comfortable and effective brushing experience. It features a slim and sleek design that allows it to easily reach all areas of the mouth, including tight spaces and the back of the mouth. 

The toothbrush is equipped with smart technology that monitors your brushing habits and provides real-time feedback to help you improve your technique. It also has a gentle vibrating motion that helps to remove plaque and food particles without causing discomfort or irritation to the gums.

Additionally, the AeroGlide UltraSlim Smart Toothbrush is made with high-quality, durable materials that are designed to last, and it is easy to clean and maintain. It is a convenient and practical option for those looking to upgrade their oral hygiene routine.
</code></pre><h2 id=迭代优化iterative>迭代优化Iterative</h2><p>没有适用于所有场景的最佳Prompt，开发高效Prompt的关键在于找个一个好的迭代优化过程。通过快速试错迭代，有效确定特定应用的最佳Prompt形式。</p><h3 id=产品说明书生成营销产品描述>产品说明书生成营销产品描述</h3><p>示例：给定一份椅子的资料页。描述说它属于中世纪灵感系列，产自意大利，并介绍了材料、构造、尺寸、可选配件等参数。</p><h4 id=初始提示>初始提示</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>概述
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    美丽的中世纪风格办公家具系列的一部分，包括文件柜、办公桌、书柜、会议桌等。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    多种外壳颜色和底座涂层可选。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    可选塑料前后靠背装饰（SWC-100）或10种面料和6种皮革的全面装饰（SWC-110）。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    底座涂层选项为：不锈钢、哑光黑色、光泽白色或铬。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    椅子可带或不带扶手。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    适用于家庭或商业场所。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    符合合同使用资格。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>结构
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    五个轮子的塑料涂层铝底座。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    气动椅子调节，方便升降。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>尺寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    宽度53厘米|20.87英寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    深度51厘米|20.08英寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    高度80厘米|31.50英寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    座椅高度44厘米|17.32英寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    座椅深度41厘米|16.14英寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>选项
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    软地板或硬地板滚轮选项。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    两种座椅泡沫密度可选：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺）。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    无扶手或8个位置PU扶手。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>材料
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>外壳底座滑动件
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    改性尼龙PA6/PA66涂层的铸铝。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    外壳厚度：10毫米。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    座椅
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    HD36泡沫
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>原产国
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    意大利
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>您的任务是帮助营销团队基于技术说明书创建一个产品的营销描述。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>根据两个双引号标记的技术说明书中提供的信息，编写一个产品描述。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>技术说明: &#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>产品描述：

我们自豪地推出了美丽的中世纪风格办公家具系列的一部分——我们的办公椅。这款办公椅包括了文件柜、办公桌、书柜、会议桌等，为您的办公室带来了高雅与实用的完美结合。

这款办公椅可选择多种外壳颜色和底座涂层，同时可选塑料前后靠背装饰或多种面料和皮革装饰。底座涂层选项包括不锈钢、哑光黑色、光泽白色或铬，椅子可带或不带扶手，适用于家庭或商业场所，并符合合同使用资格。

其结构设计采用了五个轮子的塑料涂层铝底座，并配备气动椅子调节，方便升降。

此外，我们的办公椅尺寸紧凑，适合各种场所使用。您可以选择软地板或硬地板滚轮，两种座椅泡沫密度可选，以及选择带有或无扶手的版本。

这款办公椅使用改性尼龙PA6/PA66涂层的铸铝和HD36泡沫材料制成，原产国为意大利。

无论是家庭办公室还是商业办公室，我们的办公椅都能为您的工作场所带来舒适和高雅。
</code></pre><h4 id=优化1解决生成文本太长>优化1：解决生成文本太长</h4><p>在Prompt中添加长度限制，生成更简洁的文案</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>概述
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    美丽的中世纪风格办公家具系列的一部分，包括文件柜、办公桌、书柜、会议桌等。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    多种外壳颜色和底座涂层可选。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    可选塑料前后靠背装饰（SWC-100）或10种面料和6种皮革的全面装饰（SWC-110）。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    底座涂层选项为：不锈钢、哑光黑色、光泽白色或铬。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    椅子可带或不带扶手。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    适用于家庭或商业场所。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    符合合同使用资格。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>结构
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    五个轮子的塑料涂层铝底座。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    气动椅子调节，方便升降。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>尺寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    宽度53厘米|20.87英寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    深度51厘米|20.08英寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    高度80厘米|31.50英寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    座椅高度44厘米|17.32英寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    座椅深度41厘米|16.14英寸
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>选项
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    软地板或硬地板滚轮选项。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    两种座椅泡沫密度可选：中等（1.8磅/立方英尺）或高（2.8磅/立方英尺）。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    无扶手或8个位置PU扶手。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>材料
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>外壳底座滑动件
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    改性尼龙PA6/PA66涂层的铸铝。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    外壳厚度：10毫米。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    座椅
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    HD36泡沫
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>原产国
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    意大利
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>您的任务是帮助营销团队基于技术说明书创建一个产品的营销描述。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>根据两个双引号标记的技术说明书中提供的信息，编写一个产品描述。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>使用最多50个词。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>技术说明: &#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(res)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>产品描述：

我们的中世纪风格办公家具系列包括文件柜、办公桌、书柜和会议桌，外壳颜色和底座涂层可定制。座椅可选择意大利，符合合同使用资格。
</code></pre><p>虽然语言模型对长度约束的遵循不是百分之百精确，但通过迭代测试可以找到最佳的长度提示表达式，使生成文本基本符合长度要求。
因为语言模型在计算和判断文本长度时依赖于分词器，而分词器在字符统计方面不具备完美精度。</p><h4 id=优化2处理抓错文本细节>优化2：处理抓错文本细节</h4><p>根据不同目标受众关注不同的方面，输出风格和内容都适合的文本。</p><pre tabindex=0><code>	prompt := `
您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。

根据两个双引号标记的技术说明书中提供的信息，编写一个产品描述。

该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。

使用最多50个单词。

技术规格: &#34;%s&#34;
`
</code></pre><p>输出：</p><pre tabindex=0><code>产品描述：中世纪风格办公家具系列，包括文件柜、办公桌、书柜、会议桌等。可供选择多种外壳颜色和底座涂层，尺寸为宽53厘米、深51厘米、高80厘米。材料包括改性尼龙PA6/PA66涂层的铸铝和HD36泡沫。原产国为意大利。
</code></pre><p>通过修改Prompt，模型关注点变成了具体特征与技术细节。如果想要进一步展示出具体产品的ID，可以再次修改Prompt.</p><pre tabindex=0><code>	prompt := `
您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。

根据两个双引号标记的技术说明书中提供的信息，编写一个产品描述。

该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。

在描述末尾，包括技术规格中每个7个字符的产品ID.

使用最多50个单词。

技术规格: &#34;%s&#34;
`
</code></pre><p>输出：</p><pre tabindex=0><code>该产品是美丽的中世纪风格办公家具系列的一部分，包括文件柜、办公桌和书柜。外壳颜色和底座涂层可选，可选塑不带扶手，适用于家庭或商业场所。结构包括塑料涂层铝底座和气动椅子调节。尺寸为53厘米宽、51厘米深、80厘米包括改性尼龙PA6/PA66涂层的铸铝外壳底座滑动件和HD36泡沫座椅。原产国为意大利。产品ID：SWC-100。
</code></pre><p>Prompt设计是一个循序渐进的过程，需要做好多次尝试和错误的准备，通过不断调整和优化，才能找到最符合具体场景的Prompt方式。</p><h4 id=优化3添加表格描述>优化3：添加表格描述</h4><p>继续迭代优化，要求提取产品尺寸信息并组织成表格，并指定表格的列、表名和格式；再将所有内容格式化为可以在网页使用的HTML。</p><pre tabindex=0><code>	prompt := `
您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。

根据两个双引号标记的技术说明书中提供的信息，编写一个产品描述。

该描述面向家具零售商，因此应具有技术性质，并侧重于产品的材料构造。

在描述末尾，包括技术规格中每个7个字符的产品ID。

在描述之后，包括一个表格，提供产品的尺寸。表格应该有两列。第一列包括尺寸的名称。第二列只包括英寸的测量值。

给表格命名为&#34;产品尺寸&#34;。

将所有内容格式化为可用于网站的HTML格式。将描述放在&lt;div&gt;元素中。

技术规格: &#34;%s&#34;
`
</code></pre><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-html data-lang=html><span style=display:flex><span><span style=color:#ff79c6>&lt;!DOCTYPE html&gt;</span>
</span></span><span style=display:flex><span>&lt;<span style=color:#ff79c6>html</span> <span style=color:#50fa7b>lang</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;en&#34;</span>&gt;
</span></span><span style=display:flex><span>&lt;<span style=color:#ff79c6>head</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>meta</span> <span style=color:#50fa7b>charset</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;UTF-8&#34;</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>meta</span> <span style=color:#50fa7b>name</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;viewport&#34;</span> <span style=color:#50fa7b>content</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;width=device-width, initial-scale=1.0&#34;</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>title</span>&gt;Product Description&lt;/<span style=color:#ff79c6>title</span>&gt;
</span></span><span style=display:flex><span>&lt;/<span style=color:#ff79c6>head</span>&gt;
</span></span><span style=display:flex><span>&lt;<span style=color:#ff79c6>body</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>div</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#ff79c6>h2</span>&gt;产品描述&lt;/<span style=color:#ff79c6>h2</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#ff79c6>p</span>&gt;我们隆重推出美丽的中世纪风格办公家具系列，包括文件柜、办公桌、书柜、会议桌等。您可以根据饰。底座涂层选项为不锈钢、哑光黑色、光泽白色或铬，而椅子可选择带扶手或不带扶手。这些家具适用于家庭或商业场所，并且符合合同使用资格。&lt;/<span style=color:#ff79c6>p</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#ff79c6>p</span>&gt;产品的结构特点包括五个轮子的塑料涂层铝底座和气动椅子调节功能，方便升降。&lt;/<span style=color:#ff79c6>p</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#ff79c6>h3</span>&gt;材料构造&lt;/<span style=color:#ff79c6>h3</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#ff79c6>p</span>&gt;外壳底座滑动件由改性尼龙PA6/PA66涂层的铸铝制成，外壳厚度为10毫米。而座椅使用了HD36泡沫材料。&lt;/<span style=color:#ff79c6>p</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#ff79c6>p</span>&gt;产品ID: 5623874&lt;/<span style=color:#ff79c6>p</span>&gt;
</span></span><span style=display:flex><span>    &lt;/<span style=color:#ff79c6>div</span>&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>h2</span>&gt;产品尺寸&lt;/<span style=color:#ff79c6>h2</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>table</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#ff79c6>td</span>&gt;宽度&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#ff79c6>td</span>&gt;20.87英寸&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>        &lt;/<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#ff79c6>td</span>&gt;深度&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#ff79c6>td</span>&gt;20.08英寸&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>        &lt;/<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#ff79c6>td</span>&gt;高度&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#ff79c6>td</span>&gt;31.50英寸&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>        &lt;/<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#ff79c6>td</span>&gt;座椅高度&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#ff79c6>td</span>&gt;17.32英寸&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>        &lt;/<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#ff79c6>td</span>&gt;座椅深度&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#ff79c6>td</span>&gt;16.14英寸&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>        &lt;/<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>    &lt;/<span style=color:#ff79c6>table</span>&gt;
</span></span><span style=display:flex><span>&lt;/<span style=color:#ff79c6>body</span>&gt;
</span></span><span style=display:flex><span>&lt;/<span style=color:#ff79c6>html</span>&gt;
</span></span></code></pre></div><p>浏览器查看</p><pre tabindex=0><code>产品描述
我们隆重推出美丽的中世纪风格办公家具系列，包括文件柜、办公桌、书柜、会议桌等。您可以根据饰。底座涂层选项为不锈钢、哑光黑色、光泽白色或铬，而椅子可选择带扶手或不带扶手。这些家具适用于家庭或商业场所，并且符合合同使用资格。

产品的结构特点包括五个轮子的塑料涂层铝底座和气动椅子调节功能，方便升降。

材料构造
外壳底座滑动件由改性尼龙PA6/PA66涂层的铸铝制成，外壳厚度为10毫米。而座椅使用了HD36泡沫材料。

产品ID: 5623874

产品尺寸
宽度	20.87英寸
深度	20.08英寸
高度	31.50英寸
座椅高度	17.32英寸
座椅深度	16.14英寸
</code></pre><h3 id=总结>总结</h3><p>Prompt的核心是掌握Prompt的迭代开发和优化技巧，通过不断调整试错，最终找到可靠适用的Prompt形式才是Prompt设计的正确方法。</p><h3 id=英文原版>英文原版</h3><p>产品说明书：</p><pre tabindex=0><code>text := `
OVERVIEW
- Part of a beautiful family of mid-century inspired office furniture, 
including filing cabinets, desks, bookcases, meeting tables, and more.
- Several options of shell color and base finishes.
- Available with plastic back and front upholstery (SWC-100) 
or full upholstery (SWC-110) in 10 fabric and 6 leather options.
- Base finish options are: stainless steel, matte black, 
gloss white, or chrome.
- Chair is available with or without armrests.
- Suitable for home or business settings.
- Qualified for contract use.

CONSTRUCTION
- 5-wheel plastic coated aluminum base.
- Pneumatic chair adjust for easy raise/lower action.

DIMENSIONS
- WIDTH 53 CM | 20.87”
- DEPTH 51 CM | 20.08”
- HEIGHT 80 CM | 31.50”
- SEAT HEIGHT 44 CM | 17.32”
- SEAT DEPTH 41 CM | 16.14”

OPTIONS
- Soft or hard-floor caster options.
- Two choices of seat foam densities: 
medium (1.8 lb/ft3) or high (2.8 lb/ft3)
- Armless or 8 position PU armrests 

MATERIALS
SHELL BASE GLIDER
- Cast Aluminum with modified nylon PA6/PA66 coating.
- Shell thickness: 10 mm.
SEAT
- HD36 foam

COUNTRY OF ORIGIN
- Italy
`
</code></pre><p>1.1 英文初始提示</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Your task is to help a marketing team create a 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>description for a retail website of a product based 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>on a technical fact sheet.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Write a product description based on the information 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>provided in the technical specifications delimited by 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>two double quotes.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Technical specifications: &#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(res)
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>Introducing our new mid-century inspired office chair, the perfect addition to any home or business setting. This chair is part of a beautiful family of office furniture, offering a range of options for shell color and base finishes to suit your personal style.

Choose from plastic back and front upholstery or full upholstery in a variety of fabric and leather options. The chair is also available with or without armrests, allowing you to customize it to your specific needs.

Constructed with a 5-wheel plastic coated aluminum base and a pneumatic chair adjust, this chair offers easy raise/lower action for maximum comfort. With dimensions of 53 cm in width, 51 cm in depth, and a seat height of 44 cm, this chair is both stylish and practical.

You also have the option to choose between soft or hard-floor casters and two choices of seat foam densities. The materials used are of the highest quality and the chair is qualified for contract use, making it a durable and long-lasting investment.

Designed and crafted in Italy, this office chair embodies both style and functionality, making it the perfect addition to any workspace. Elevate your office space with our mid-century inspired office chair today.
</code></pre><p>1.2 限制生成长度</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Your task is to help a marketing team create a 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>description for a retail website of a product based 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>on a technical fact sheet.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Write a product description based on the information 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>provided in the technical specifications delimited by 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>two double quotes.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Use at most 50 words.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Technical specifications: &#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(res)
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>Elevate your office space with our mid-century inspired office chair. With multiple shell color and base finish options, as well as upholstery choices in fabric or leather, you can customize it to fit your style. Designed for both home and business settings, this chair is perfect for any environment. Made in Italy with high-quality materials, this chair is a stylish and functional addition to any workspace.
</code></pre><p>1.3 处理抓错文本细节</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Your task is to help a marketing team create a 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>description for a retail website of a product based 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>on a technical fact sheet.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Write a product description based on the information 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>provided in the technical specifications delimited by 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>two double quotes.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>The description is intended for furniture retailers, 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>so should be technical in nature and focus on the 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>materials the product is constructed from.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Use at most 50 words.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Technical specifications: &#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(res)
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>Introducing the SWC-100 and SWC-110 office chairs, part of our mid-century inspired furniture collection. Choose from a variety of shell colors and base finishes, with options for upholstery in fabric or leather. Constructed with a 5-wheel plastic-coated aluminum base and high-quality foam, these chairs are perfect for any home or business setting. Made in Italy.
</code></pre><p>在描述末尾包含 7个字符的产品ID</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Your task is to help a marketing team create a 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>description for a retail website of a product based 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>on a technical fact sheet.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Write a product description based on the information 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>provided in the technical specifications delimited by 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>two double quotes.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>The description is intended for furniture retailers, 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>so should be technical in nature and focus on the 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>materials the product is constructed from.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>At the end of the description, include every 7-character 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Product ID in the technical specification.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Use at most 50 words.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Technical specifications: &#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(res)
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>Introducing our mid-century inspired office chair, available in a variety of shell colors and base finishes. With options for plastic or full upholstery in fabric or leather, and a range of armrest and caster choices, this chair is perfect for any home or business setting. Constructed with a 5-wheel plastic coated aluminum base and high-quality foam, this chair offers both style and durability. Made in Italy.

Product IDs: SWC-100, SWC-110
</code></pre><p>1.4 英文添加表格描述</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Your task is to help a marketing team create a 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>description for a retail website of a product based 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>on a technical fact sheet.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Write a product description based on the information 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>provided in the technical specifications delimited by 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>two double quotes.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>The description is intended for furniture retailers, 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>so should be technical in nature and focus on the 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>materials the product is constructed from.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>At the end of the description, include every 7-character 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Product ID in the technical specification.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>After the description, include a table that gives the 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>product&#39;s dimensions. The table should have two columns.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>In the first column include the name of the dimension. 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>In the second column include the measurements in inches only.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Give the table the title &#39;Product Dimensions&#39;.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Format everything as HTML that can be used in a website. 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Place the description in a &lt;div&gt; element.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Technical specifications: &#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(res)
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-html data-lang=html><span style=display:flex><span>&lt;<span style=color:#ff79c6>div</span>&gt;
</span></span><span style=display:flex><span>  &lt;<span style=color:#ff79c6>p</span>&gt;This mid-century inspired office chair is the perfect addition to any home or business setting. The chair is available with various options for shell color, base finishes, and upholstery, making it easy to find the perfect match for any decor. The 5-wheel plastic coated aluminum base provides stability and maneuverability, while the pneumatic chair adjustment allows for easy raise/lower action. The seat is constructed with HD36 foam for ultimate comfort, and the shell base glider is made of cast aluminum with a modified nylon PA6/PA66 coating. With its stylish design and high-quality construction, this chair is ideal for any office space or meeting room.&lt;/<span style=color:#ff79c6>p</span>&gt;
</span></span><span style=display:flex><span>  &lt;<span style=color:#ff79c6>p</span>&gt;Product ID: SWC-100, SWC-110&lt;/<span style=color:#ff79c6>p</span>&gt;
</span></span><span style=display:flex><span>  &lt;<span style=color:#ff79c6>table</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>caption</span>&gt;Product Dimensions&lt;/<span style=color:#ff79c6>caption</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#ff79c6>th</span>&gt;WIDTH&lt;/<span style=color:#ff79c6>th</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#ff79c6>td</span>&gt;20.87 inches&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>    &lt;/<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#ff79c6>th</span>&gt;DEPTH&lt;/<span style=color:#ff79c6>th</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#ff79c6>td</span>&gt;20.08 inches&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>    &lt;/<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#ff79c6>th</span>&gt;HEIGHT&lt;/<span style=color:#ff79c6>th</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#ff79c6>td</span>&gt;31.50 inches&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>    &lt;/<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#ff79c6>th</span>&gt;SEAT HEIGHT&lt;/<span style=color:#ff79c6>th</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#ff79c6>td</span>&gt;17.32 inches&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>    &lt;/<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#ff79c6>th</span>&gt;SEAT DEPTH&lt;/<span style=color:#ff79c6>th</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#ff79c6>td</span>&gt;16.14 inches&lt;/<span style=color:#ff79c6>td</span>&gt;
</span></span><span style=display:flex><span>    &lt;/<span style=color:#ff79c6>tr</span>&gt;
</span></span><span style=display:flex><span>  &lt;/<span style=color:#ff79c6>table</span>&gt;
</span></span><span style=display:flex><span>&lt;/<span style=color:#ff79c6>div</span>&gt;
</span></span></code></pre></div><h2 id=文本概括summarizing>文本概括Summarizing</h2><h3 id=单一文本概括>单一文本概括</h3><p>我们提供一段在线商品评价作为示例，可能来自于一个在线购物平台，例如亚马逊、淘宝、京东等。
评价者为一款熊猫公仔进行了点评，评价内容包括商品的质量、大小、价格和物流速度等因素，以及他的女儿对该商品的喜爱程度</p><p>text := <code>这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。 公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说， 它有点小，我感觉在别的地方用同样的价钱能买到更大的。 快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。</code></p><h4 id=限制输出文本长度>限制输出文本长度</h4><p>尝试将文本的长度限制在30个字以内</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说，
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>它有点小，我感觉在别的地方用同样的价钱能买到更大的。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>您的任务是从电子商务网站上生成一个产品评论的简短摘要。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>请对两个双引号之间的评论文本进行概括，最多30个字。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>评论: &#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(res)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>可爱软萌的熊猫公仔，面部表情和善，快递提前一天到货，值得购买。
</span></span></code></pre></div><h4 id=设置关键角度侧重>设置关键角度侧重</h4><p>在某些情况下，我们会针对不同的业务场景对文本的侧重会有所不同。通过增强输入提示（Prompt），来强调我们对某一特定视角的重视</p><p>1.侧重于快递服务</p><pre tabindex=0><code>    text := `
这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。
公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说，
它有点小，我感觉在别的地方用同样的价钱能买到更大的。
快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。
`
	prompt := `
您的任务是从电子商务网站上生成一个产品评论的简短摘要。

请对两个双引号之间的评论文本进行概括，最多30个字，并且侧重在快递服务上。

评论: &#34;%s&#34;
`
</code></pre><p>输出：</p><pre tabindex=0><code>快递提前一天送到，公仔有点小，性价比一般。
</code></pre><p>从输出结果可以看出，以快递效率侧重文本开头。</p><p>2.侧重于价格与质量</p><pre tabindex=0><code>    text := `
这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。
公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说，
它有点小，我感觉在别的地方用同样的价钱能买到更大的。
快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。
`
	prompt := `
您的任务是从电子商务网站上生成一个产品评论的简短摘要。

请对两个双引号之间的评论文本进行概括，最多30个字，并且侧重在产品价格和质量上。

评论: &#34;%s&#34;
</code></pre><p>输出：</p><pre tabindex=0><code>熊猫公仔贵但可爱，尺寸小一些。质量好，面部表情和善。快递提前到货。
</code></pre><p>从输出结果来看，确实侧重了价格和质量。</p><h4 id=关键信息提取>关键信息提取</h4><p>如果我们只想要提取某一角度的信息，并过滤掉其他所有信息，则可以要求LLM进行文本提取(Extract)而非概括(Summarize)</p><pre tabindex=0><code>    text := `
这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。
公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说，
它有点小，我感觉在别的地方用同样的价钱能买到更大的。
快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。
`
	prompt := `
您的任务是从电子商务网站上的产品评论中提取相关信息。

请对两个双引号之间的评论文本中提取产品运输相关的信息，最多30个字。

评论: &#34;%s&#34;
`
</code></pre><p>输出：</p><pre tabindex=0><code>快递比预期提前了一天到货。
</code></pre><h3 id=同时概括多条文本>同时概括多条文本</h3><p>在实际工作中，往往要处理大量的评论文本，下面示例展示将多条用户评论集中在一起，利用for循环和文本概括提示词，将评论概括在20个词以内，并按顺序打印。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text1 <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>这个熊猫公仔是我给女儿的生日礼物，她很喜欢，去哪都带着。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>公仔很软，超级可爱，面部表情也很和善。但是相比于价钱来说，
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>它有点小，我感觉在别的地方用同样的价钱能买到更大的。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>快递比预期提前了一天到货，所以在送给女儿之前，我自己玩了会。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#6272a4>// 评论一盏落地灯
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>	text2 <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>我需要一盏漂亮的卧室灯，这款灯不仅具备额外的储物功能，价格也并不算太高。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>收货速度非常快，仅用了两天的时间就送到了。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>不过，在运输过程中，灯的拉线出了问题，幸好，公司很乐意寄送了一根全新的灯线。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>新的灯线也很快就送到手了，只用了几天的时间。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>装配非常容易。然而，之后我发现有一个零件丢失了，于是我联系了客服，他们迅速地给我寄来了缺失的零件！
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>对我来说，这是一家非常关心客户和产品的优秀公司。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#6272a4>// 评论一把电动牙刷
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>	text3 <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>我的牙科卫生员推荐了电动牙刷，所以我就买了这款。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>到目前为止，电池续航表现相当不错。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>初次充电后，我在第一周一直将充电器插着，为的是对电池进行条件养护。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>过去的3周里，我每天早晚都使用它刷牙，但电池依然维持着原来的充电状态。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>不过，牙刷头太小了。我见过比这个牙刷头还大的婴儿牙刷。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>我希望牙刷头更大一些，带有不同长度的刷毛，
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>这样可以更好地清洁牙齿间的空隙，但这款牙刷做不到。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>总的来说，如果你能以50美元左右的价格购买到这款牙刷，那是一个不错的交易。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>制造商的替换刷头相当昂贵，但你可以购买价格更为合理的通用刷头。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>这款牙刷让我感觉就像每天都去了一次牙医，我的牙齿感觉非常干净！
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#6272a4>// 评论一台搅拌机
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>	text4 <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>在11月份期间，这个17件套装还在季节性促销中，售价约为49美元，打了五折左右。可是由于某种原因（我们可以称之为价格上涨），到了12月的第二周，所有的价格都上涨了，
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>同样的套装价格涨到了70-89美元不等。而11件套装的价格也从之前的29美元上涨了约10美元。看起来还算不错，但是如果你仔细看底座，刀片锁定的部分看起来没有前几年版本的那么漂亮。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>然而，我打算非常小心地使用它（例如，我会先在搅拌机中研磨豆类、冰块、大米等坚硬的食物，然后再将它们研磨成所需的粒度，接着切换到打蛋器刀片以获得更细的面粉，如果我需要制作更细腻/少果肉的食物）。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>在制作冰沙时，我会将要使用的水果和蔬菜切成细小块并冷冻（如果使用菠菜，我会先轻微煮熟菠菜，然后冷冻，直到使用时准备食用。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>如果要制作冰糕，我会使用一个小到中号的食物加工器），这样你就可以避免添加过多的冰块。大约一年后，电机开始发出奇怪的声音。我打电话给客户服务，但保修期已经过期了，
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>所以我只好购买了另一台。值得注意的是，这类产品的整体质量在过去几年里有所下降，所以他们在一定程度上依靠品牌认知和消费者忠诚来维持销售。在大约两天内，我收到了新的搅拌机。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>您的任务是从电子商务网站上的产品评论中提取相关信息。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>请对两个双引号之间的评论文本进行概括，最多20个词汇。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>评论: &#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>for</span> i, text <span style=color:#ff79c6>:=</span> <span style=color:#ff79c6>range</span> []<span style=color:#8be9fd>string</span>{text1, text2, text3, text4} {
</span></span><span style=display:flex><span>		client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>		resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, text))
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>			fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>			<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>		res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;评论%d: %s\n&#34;</span>, i<span style=color:#ff79c6>+</span><span style=color:#bd93f9>1</span>, res)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>评论1: 概括：熊猫公仔生日礼物，女儿喜欢，软可爱，面部表情和善，价钱小，快递提前到货。
评论2: 评论总结: 漂亮卧室灯，带储物功能，价格适中。快速配送，良好售后服务。易装配，客服及时处理问题。
评论3: 推荐的电动牙刷，续航不错，但牙刷头太小，价格合理，给牙齿清洁感觉。
评论4: 评论内容概括：购买17件套装搅拌机，价格先打折再上涨，质量下降，使用注意事项，售后服务需改进。
</code></pre><h2 id=推断inferring>推断Inferring</h2><p>这一章让你了解如何从产品评价和新闻文章中推导出情感和主题，包括了标签提取、实体提取、以及理解文本的情感等等</p><h3 id=情感推断>情感推断</h3><h4 id=情感倾向分析>情感倾向分析</h4><p>如何对评论进行情感二分类（正面/负面），让系统自动解析这条评论的情感倾向</p><pre tabindex=0><code>text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`

prompt := `
以下用两个双引号分隔的产品评论的情感是什么？

评论文本: &#34;%s&#34;
`
</code></pre><p>输出：</p><pre tabindex=0><code>积极的情感。
</code></pre><p>可以使用更简介的输出，比如用一个单词回答</p><pre tabindex=0><code>text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`
	prompt := `
以下用两个双引号分隔的产品评论的情感是什么？

用一个单词回答： 正面或负面

评论文本: &#34;%s&#34;
`
</code></pre><h4 id=识别情感类型>识别情感类型</h4><p>让模型能够识别出评论作者所表达的情感，将这些情感整理为一个不超过五项的列表</p><pre tabindex=0><code>    text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`
	prompt := `
识别以下评论的作者表达的情感。包含不超过五个项目。将答案格式化为以逗号分隔的单词列表。

评论文本: &#34;%s&#34;
`
</code></pre><p>输出：</p><pre tabindex=0><code>满意,感激,信任,高兴,愉快
</code></pre><h4 id=识别愤怒>识别愤怒</h4><p>洞察到愤怒情绪至关重要</p><pre tabindex=0><code>    text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`
	prompt := `
以下评论的作者是否表达了愤怒？评论用两个双引号分隔。给出是或否的答案。

评论文本: &#34;%s&#34;
`
</code></pre><p>输出：</p><pre tabindex=0><code>否
</code></pre><h3 id=信息提取>信息提取</h3><h4 id=商品信息提取>商品信息提取</h4><p>信息提取能够帮助我们从文本中抽取特定、关注的信息</p><p>示例：要求模型返回一个json对象，其中key是商品和品牌</p><pre tabindex=0><code>	text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`
    prompt := `
从评论文本中识别以下项目：
- 评论者购买的物品
- 制造该物品的公司

评论文本用两个双引号分隔。将你的响应格式化为以&#34;物品&#34;和&#34;品牌&#34;为键的json对象。
如果信息不存在，请使用&#34;未知&#34;作为值。
让你的回应尽可能简短。

评论文本: &#34;%s&#34;
`
</code></pre><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;物品&#34;</span>: <span style=color:#f1fa8c>&#34;卧室灯&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>&#34;品牌&#34;</span>: <span style=color:#f1fa8c>&#34;Lumina&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h4 id=综合情感推断和信息提取>综合情感推断和信息提取</h4><p>设计一个单一的prompt，来同时提取所有这些信息</p><pre tabindex=0><code>text := `
我需要一盏漂亮的卧室灯，这款灯具有额外的储物功能，价格也不算太高。\
我很快就收到了它。在运输过程中，我们的灯绳断了，但是公司很乐意寄送了一个新的。\
几天后就收到了。这款灯很容易组装。我发现少了一个零件，于是联系了他们的客服，他们很快就给我寄来了缺失的零件！\
在我看来，Lumina 是一家非常关心顾客和产品的优秀公司！
`
	prompt := `
从评论文本中识别以下项目：
- 情绪（正面或负面）
- 审稿人是否表达了愤怒？（是或否）
- 评论者购买的物品
- 制造该物品的公司

评论用两个双引号分隔。将你的响应格式化为JSON对象，以&#34;情感倾向&#34;、&#34;是否生气&#34;、&#34;物品类型&#34;和&#34;品牌&#34;作为键。
如果信息不存在，请使用&#34;未知&#34;作为值。
让你的回应尽可能简短。
将&#34;是否生气&#34;值格式化为布尔值。

评论文本: &#34;%s&#34;
`
</code></pre><p>输出：</p><pre tabindex=0><code>{
    &#34;情感倾向&#34;: &#34;正面&#34;,
    &#34;是否生气&#34;: false,
    &#34;物品类型&#34;: &#34;卧室灯&#34;,
    &#34;品牌&#34;: &#34;Lumina&#34;
}
</code></pre><h3 id=主题推断>主题推断</h3><p>根据一段长文本，判断这段文本的主旨，涉及了哪些主题</p><pre tabindex=0><code>text := `
在政府最近进行的一项调查中，要求公共部门的员工对他们所在部门的满意度进行评分。
调查结果显示，NASA是最受欢迎的部门，满意度为95％。

一位NASA员工John Smith对这一发现发表了评论，他表示：
我对NASA排名第一并不感到惊讶。这是一个与了不起的人们和令人难以置信的机会共事的好地方。我为成为这样一个创新组织的一员感到自豪。

NASA的管理团队也对这一结果表示欢迎，主管Tom Johnson表示：
我们很高兴听到我们的员工对NASA的工作感到满意。
我们拥有一支才华横溢、忠诚敬业的团队，他们为实现我们的目标不懈努力，看到他们的辛勤工作得到回报是太棒了。

调查还显示，社会保障管理局的满意度最低，只有45％的员工表示他们对工作满意。
政府承诺解决调查中员工提出的问题，并努力提高所有部门的工作满意度。
`
</code></pre><h4 id=推断讨论主题>推断讨论主题</h4><pre tabindex=0><code>prompt := `
将以下两个双引号分隔的给定文本中讨论的五个主题，每个主题用1-2个词概括，输出一个可解析的Python语言列表，每个元素是一个字符串，展示了一个主题。

给定文本: &#34;%s&#34;
`
</code></pre><p>输出：</p><pre tabindex=0><code>[&#34;NASA&#34;, &#34;员工满意度调查&#34;, &#34;John Smith评论&#34;, &#34;Tom Johnson评论&#34;, &#34;社会保障管理局&#34;]
</code></pre><h4 id=为特定主题制作新闻提醒>为特定主题制作新闻提醒</h4><pre tabindex=0><code>rompt := `
判断主题列表中的每一项是否是以下给定文本中的一个话题，

以列表的形式给出答案，每个元素是一个Json对象，键为对应主题，值为对应的0或1。

主题列表：美国航空航天局、当地政府、工程、员工满意度、联邦政府

给定文本: &#34;%s&#34;
`
</code></pre><p>输出：</p><pre tabindex=0><code>[
    {&#34;美国航空航天局&#34;: 1},
    {&#34;当地政府&#34;: 1},
    {&#34;工程&#34;: 0},
    {&#34;员工满意度&#34;: 1},
    {&#34;联邦政府&#34;: 1}
]
</code></pre><p>在机器学习领域这种称为零样本学习，没有提供任何带标签的训练数据，凭借Prompt，就能判定哪些主题被包含。</p><h2 id=文本转换>文本转换</h2><p>文本扩展是大语言模型的一个重要应用方向，输入简短文本，生成更加丰富的长文。</p><h3 id=定制客户邮件>定制客户邮件</h3><p>根据客户的评价和其中的情感倾向，使用语言模型针对性生成回复邮件。先输入客户的评论文本和对应的情感分析结果(正面或者负面)。
然后构造一个Prompt，要求大语言模型基于这些信息来生成一封定制的回复电子邮件。</p><p>示例：首先明确大语言模型的身份是客户服务AI助手；它任务是为客户发送电子邮件回复；
然后在三个反引号间给出具体的客户评论；最后要求语言模型根据这条反馈邮件生成一封回复，以感谢客户的评价</p><pre tabindex=0><code>text := `
他们在11月份的季节性销售期间以约49美元的价格出售17件套装，折扣约为一半。\
但由于某些原因（可能是价格欺诈），到了12月第二周，同样的套装价格全都涨到了70美元到89美元不等。\
11件套装的价格也上涨了大约10美元左右。\
虽然外观看起来还可以，但基座上锁定刀片的部分看起来不如几年前的早期版本那么好。\
不过我打算非常温柔地使用它，例如，\
我会先在搅拌机中将像豆子、冰、米饭等硬物研磨，然后再制成所需的份量，\
切换到打蛋器制作更细的面粉，或者在制作冰沙时先使用交叉切割刀片，然后使用平面刀片制作更细/不粘的效果。\
制作冰沙时，特别提示：\
将水果和蔬菜切碎并冷冻（如果使用菠菜，则轻轻煮软菠菜，然后冷冻直到使用；\
如果制作果酱，则使用小到中号的食品处理器），这样可以避免在制作冰沙时添加太多冰块。\
大约一年后，电机发出奇怪的噪音，我打电话给客服，但保修已经过期了，所以我不得不再买一个。\
总的来说，这些产品的总体质量已经下降，因此它们依靠品牌认可和消费者忠诚度来维持销售。\
货物在两天内到达。
`
	prompt := `
你是一位客户服务的AI助手。
你的任务是给一位重要客户发送邮件回复。
根据客户通过两个双引号分隔的评价，生成回复以感谢客户的评价。提醒模型使用评价中的具体细节
用简明而专业的语气写信。作为&#34;AI客户代理&#34;签署电子邮件。
客户评论: &#34;%s&#34;
评论情感：消极的。
`
</code></pre><p>输出：</p><pre tabindex=0><code>尊敬的客户，

非常感谢您的反馈，我们对您在11月份的购买经历感到十分抱歉。我们一直致力于提供高品质的产品，但很明显您对最近购买的套装不满意。关于您提到的品质问题和售后服务经验，我们会将您的意见反馈给相关部门，以便改进和提高我们的产品和服务质量。

同时，我们也感谢您提供了关于产品使用和维护的建议。您的经验分享对其他客户来说也将非常有帮助。我们将注意到您提到的制作冰沙的技巧，以及保持电机使用寿命的方法。

如果您有任何进一步的问题或需求，请随时与我们联系。我们会尽力为您提供更好的服务和支持。

再次感谢您对我们的支持和反馈。

祝您生活愉快！

AI客户代理
</code></pre><h3 id=引入温度系数>引入温度系数</h3><p>大语言模型中的温度参数可以控制生成文本的随机性和多样性，temperature 的值越大，语言模型输出的多样性越大；
temperature的值越小，输出越倾向高概率的文本。一般需要可预测、可靠的输出，就把temperature设置为0。</p><p>示例：针对同一段来信，我们提醒语言模型使用用户来信中的详细信息，并设置一个较高的temperature ，运行两次，比较他们的结果。</p><p>第一次输出：</p><pre tabindex=0><code>尊敬的客户，

非常感谢您对我们产品的详细评价。我们对您在购买过程中遇到的问题感到抱歉，我们会认真考虑您提出的问题，并努力改进产品质量和服务。

对于价格上涨和产品质量下降的问题，我们深表歉意。我们会进一步调查此事，并确保未来的销售活动和产品质量能够得到改善。同时，我们也会加强售后服务，以确保客户在使用产品时能够得到及时的支持和帮助。

如果您需要进一步的帮助或有任何其他问题，请随时联系我们的客户服务团队。我们将竭诚为您提供支持并解决您的问题。

再次感谢您的宝贵意见，期待未来能够为您提供更好的产品和服务。

祝您生活愉快！

AI客户代理
</code></pre><p>第二次输出：</p><pre tabindex=0><code>尊敬的客户，

感谢您给出的详细评价，我们对您的反馈感到非常抱歉。我们对您遇到的问题感到遗憾，我们将会进一步改进我们的产品质量和服务。

请您联系我们的客户服务部门，以便我们可以进一步了解您的情况并为您提供帮助。我们将竭尽全力解决您的问题，并确保您的满意度。

再次感谢您的反馈，我们期待能够为您提供更好的服务和产品体验。

祝您一切顺利，

AI客户代理
</code></pre><p>温度（temperature）参数可以控制语言模型生成文本的随机性。</p><h2 id=聊天机器人>聊天机器人</h2><p>大语言模型让构建定制的聊天机器人，只需要很少的工作量。类似ChatGPT这样的聊天模型实际上是一系列消息作为输入，
并返回一个模型生成的消息作为输出。</p><h3 id=给定身份>给定身份</h3><p>要区分系统消息、用户消息、助手消息；系统消息有助于设置助手的行为和角色，并作为对话的高级指示。ChatGPT的系统消息是屏蔽了的，为了不让请求称为对话的一部分，
引导助手并指导其回应。举个例子：在ChatGPT网页中，你的消息就称为用户消息，ChatGPT的消息就称为助手消息。但是在构建聊天机器人时，发送了系统消息后，你可以作为用户，
也可以在用户和助手之间切换，从而提供对话上下文。</p><p>新增个函数定义，可以接收消息列表，这些消息来自不同的角色</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> (c <span style=color:#ff79c6>*</span>openAIClient) <span style=color:#50fa7b>CreateChatCompletionWithMessage</span>(ctx context.Context, messages []openai.ChatCompletionMessage) (openai.ChatCompletionResponse, <span style=color:#8be9fd>error</span>) {
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>return</span> c.Client.<span style=color:#50fa7b>CreateChatCompletion</span>(ctx, openai.ChatCompletionRequest{
</span></span><span style=display:flex><span>		Model:       openai.GPT3Dot5Turbo,
</span></span><span style=display:flex><span>		Messages:    messages,
</span></span><span style=display:flex><span>		Temperature: <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>	})
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h4 id=讲笑话>讲笑话</h4><p>通过系统消息来定义：&ldquo;你是一个说话像莎士比亚的助手。&ldquo;这是我们向助手描述它应该如何表现的方式。</p><p>然后，第一个用户消息：&ldquo;给我讲个笑话。&rdquo;</p><p>接下来以助手身份给出回复：&ldquo;为什么鸡会过马路？&rdquo;</p><p>最后发送用户消息是：&ldquo;我不知道。&rdquo;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	messages <span style=color:#ff79c6>:=</span> []openai.ChatCompletionMessage{
</span></span><span style=display:flex><span>		{
</span></span><span style=display:flex><span>			Role:    openai.ChatMessageRoleSystem,
</span></span><span style=display:flex><span>			Content: <span style=color:#f1fa8c>&#34;你是一个像莎士比亚一样说话的助手。&#34;</span>,
</span></span><span style=display:flex><span>		},
</span></span><span style=display:flex><span>		{
</span></span><span style=display:flex><span>			Role:    openai.ChatMessageRoleUser,
</span></span><span style=display:flex><span>			Content: <span style=color:#f1fa8c>&#34;给我讲个笑话&#34;</span>,
</span></span><span style=display:flex><span>		},
</span></span><span style=display:flex><span>		{
</span></span><span style=display:flex><span>			Role:    openai.ChatMessageRoleAssistant,
</span></span><span style=display:flex><span>			Content: <span style=color:#f1fa8c>&#34;鸡为什么过马路&#34;</span>,
</span></span><span style=display:flex><span>		},
</span></span><span style=display:flex><span>		{
</span></span><span style=display:flex><span>			Role:    openai.ChatMessageRoleUser,
</span></span><span style=display:flex><span>			Content: <span style=color:#f1fa8c>&#34;我不知道&#34;</span>,
</span></span><span style=display:flex><span>		},
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletionWithMessage</span>(context.<span style=color:#50fa7b>TODO</span>(), messages)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(res)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>因为它想证明自己并不只是个胆小鬼！哈哈哈！
</code></pre><h4 id=友好的聊天机器人>友好的聊天机器人</h4><p>系统消息定义：&ldquo;你是一个友好的聊天机器人&rdquo;，第一个用户消息：&ldquo;嗨，我叫lsa。&rdquo;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>    client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	messages <span style=color:#ff79c6>:=</span> []openai.ChatCompletionMessage{
</span></span><span style=display:flex><span>		{
</span></span><span style=display:flex><span>			Role:    openai.ChatMessageRoleSystem,
</span></span><span style=display:flex><span>			Content: <span style=color:#f1fa8c>&#34;你是个友好的聊天机器人&#34;</span>,
</span></span><span style=display:flex><span>		},
</span></span><span style=display:flex><span>		{
</span></span><span style=display:flex><span>			Role:    openai.ChatMessageRoleUser,
</span></span><span style=display:flex><span>			Content: <span style=color:#f1fa8c>&#34;Hi，我是Isa&#34;</span>,
</span></span><span style=display:flex><span>		},
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletionWithMessage</span>(context.<span style=color:#50fa7b>TODO</span>(), messages)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(res)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>你好Isa，很高兴认识你！有什么可以帮助你的吗？
</code></pre><h3 id=构建上下文>构建上下文</h3><p>系统消息来定义：“你是一个友好的聊天机器人”，第一个用户消息：“是的，你能提醒我我的名字是什么吗？”</p><pre tabindex=0><code>    messages := []openai.ChatCompletionMessage{
		{
			Role:    openai.ChatMessageRoleSystem,
			Content: &#34;你是个友好的聊天机器人。&#34;,
		},
		{
			Role:    openai.ChatMessageRoleUser,
			Content: &#34;是的，你能提醒我我的名字是什么吗？&#34;,
		},
	}
</code></pre><p>输出：</p><pre tabindex=0><code>当然！您的名字是...嗯...抱歉，我不知道您的名字。您能告诉我一下您的名字吗？我会牢记在心的！
</code></pre><p>每次与语言模型的交互都互相独立，这意味着我们必须提供所有相关的消息，以便模型在当前对话中进行引用。
如果想让模型引用或"记住"对话的早期部分，则必须在模型的输入中提供早期的交流。我们将其称为上下文 (context)</p><pre tabindex=0><code>    messages := []openai.ChatCompletionMessage{
		{
			Role:    openai.ChatMessageRoleSystem,
			Content: &#34;你是个友好的聊天机器人。&#34;,
		},
		{
			Role:    openai.ChatMessageRoleUser,
			Content: &#34;Hi，我是Isa&#34;,
		},
		{
			Role:    openai.ChatMessageRoleAssistant,
			Content: &#34;Hi Isa！很高兴认识你。今天有什么可以帮到你的吗？&#34;,
		},
		{
			Role:    openai.ChatMessageRoleUser,
			Content: &#34;是的，你可以提醒我，我的名字是什么？&#34;,
		},
	}
</code></pre><p>输出：</p><pre tabindex=0><code>当然，你的名字是Isa。如果你需要我提醒你任何事情，随时告诉我哦！有什么其他问题我可以帮你解决吗？
</code></pre><p>模型有了上下文，模型就能够做出回应。</p><h3 id=订餐机器人>订餐机器人</h3><p>如何构建一个"点餐助手机器人&rdquo;，这个机器人将被设计为自动收集用户信息，并接收来自披萨店的订单。</p><h4 id=构建机器人>构建机器人</h4><p>新增一个函数，这个函数从我们构建的用户界面中收集prompt，然后添加到上下文context中，并在每次调用模型时使用这个上下文。
模型的返回也会被添加到上下文中</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	messages <span style=color:#ff79c6>:=</span> []openai.ChatCompletionMessage{
</span></span><span style=display:flex><span>		{
</span></span><span style=display:flex><span>			Role: openai.ChatMessageRoleSystem,
</span></span><span style=display:flex><span>			Content: <span style=color:#f1fa8c>`你是订餐机器人，为披萨餐厅自动收集订单信息。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>你要首先问候顾客。然后等待用户回复收集订单信息。收集完信息需确认顾客是否还需要添加其他内容。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>最后需要询问是否自取或外送，如果是外送，你要询问地址。最后告诉顾客订单总金额，并送上祝福。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>请确保明确所有选项、附加项和尺寸，以便从菜单中识别出该项唯一的内容。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>你的回应应该以简短、非常随意和友好的风格呈现。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>菜单包括：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>菜品价格：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>意式辣香肠披萨（大、中、小） 12.95、10.00、7.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>芝士披萨（大、中、小） 10.95、9.25、6.50
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>茄子披萨（大、中、小） 11.95、9.75、6.75
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>薯条（大、小） 4.50、3.50
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>希腊沙拉 7.25
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>配料：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>奶酪 2.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>蘑菇 1.50
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>香肠 3.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>加拿大熏肉 3.50
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>AI酱 1.50
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>辣椒 1.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>饮料：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>可乐（大、中、小） 3.00、2.00、1.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>雪碧（大、中、小） 3.00、2.00、1.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>瓶装水 5.00`</span>,
</span></span><span style=display:flex><span>		},
</span></span><span style=display:flex><span>		{
</span></span><span style=display:flex><span>			Role:    openai.ChatMessageRoleAssistant,
</span></span><span style=display:flex><span>			Content: <span style=color:#f1fa8c>&#34;你好！欢迎来到披萨餐厅！您想点什么呢？&#34;</span>,
</span></span><span style=display:flex><span>		},
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;%s: %s\n&#34;</span>, messages[<span style=color:#bd93f9>1</span>].Role, messages[<span style=color:#bd93f9>1</span>].Content)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>for</span> {
</span></span><span style=display:flex><span>		<span style=color:#8be9fd;font-style:italic>var</span> s <span style=color:#8be9fd>string</span>
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;user: &#34;</span>)
</span></span><span style=display:flex><span>		_, err <span style=color:#ff79c6>:=</span> fmt.<span style=color:#50fa7b>Scanf</span>(<span style=color:#f1fa8c>&#34;%s&#34;</span>, <span style=color:#ff79c6>&amp;</span>s)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>			<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>if</span> s <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;end&#34;</span> {
</span></span><span style=display:flex><span>			<span style=color:#ff79c6>break</span>
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		messages = <span style=color:#8be9fd;font-style:italic>append</span>(messages, openai.ChatCompletionMessage{
</span></span><span style=display:flex><span>			Role:    openai.ChatMessageRoleUser,
</span></span><span style=display:flex><span>			Content: s,
</span></span><span style=display:flex><span>		})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletionWithMessage</span>(context.<span style=color:#50fa7b>TODO</span>(), messages)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>			fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>			<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;%s: %s\n&#34;</span>, resp.Choices[<span style=color:#bd93f9>0</span>].Message.Role, resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>		messages = <span style=color:#8be9fd;font-style:italic>append</span>(messages, resp.Choices[<span style=color:#bd93f9>0</span>].Message)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>assistant: 你好！欢迎来到披萨餐厅！您想点什么呢？
user: 一杯大可乐
assistant: 好的，您想点什么披萨吗？还是需要其他食物或者饮料？
user: 一份大芝士披萨，配料加蘑菇，还有一份沙拉
assistant: 好的，一份大芝士披萨，配料加蘑菇，还有一份希腊沙拉。您需要外卖还是自取？
user: 自取吧
assistant: 好的，您的订单是一份大芝士披萨（配料加蘑菇）和一份希腊沙拉，自取。总金额是 21.20 元。感谢您的订购！祝您用餐愉快！如果还需要帮助，请随时告诉我。
user: 你可以展示金额是怎么算的吗
assistant: 当然！大芝士披萨 10.95 元 + 配料蘑菇 1.50 元 + 希腊沙拉 7.25 元 = 21.20 元。希望这样清楚明了！祝您用餐愉快！
user: 谢谢
assistant: 不客气！如果您有任何其他问题或需求，请随时告诉我。祝您有美好的一天！
user: end
</code></pre><h4 id=创建json摘要>创建JSON摘要</h4><p>要求模型创建一个JSON摘要，方便我们发送给订单系统；在上下文的基础上追加另一个系统消息，定义json概要的格式</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	messages <span style=color:#ff79c6>:=</span> []openai.ChatCompletionMessage{
</span></span><span style=display:flex><span>		{
</span></span><span style=display:flex><span>			Role: openai.ChatMessageRoleSystem,
</span></span><span style=display:flex><span>			Content: <span style=color:#f1fa8c>`你是订餐机器人，为披萨餐厅自动收集订单信息。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>你要首先问候顾客。然后等待用户回复收集订单信息。收集完信息需确认顾客是否还需要添加其他内容。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>最后需要询问是否自取或外送，如果是外送，你要询问地址。最后告诉顾客订单总金额，并送上祝福。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>请确保明确所有选项、附加项和尺寸，以便从菜单中识别出该项唯一的内容。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>你的回应应该以简短、非常随意和友好的风格呈现。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>菜单包括：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>菜品价格：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>意式辣香肠披萨（大、中、小） 12.95、10.00、7.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>芝士披萨（大、中、小） 10.95、9.25、6.50
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>茄子披萨（大、中、小） 11.95、9.75、6.75
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>薯条（大、小） 4.50、3.50
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>希腊沙拉 7.25
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>配料：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>奶酪 2.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>蘑菇 1.50
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>香肠 3.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>加拿大熏肉 3.50
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>AI酱 1.50
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>辣椒 1.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>饮料：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>可乐（大、中、小） 3.00、2.00、1.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>雪碧（大、中、小） 3.00、2.00、1.00
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>瓶装水 5.00`</span>,
</span></span><span style=display:flex><span>		},
</span></span><span style=display:flex><span>		{
</span></span><span style=display:flex><span>			Role:    openai.ChatMessageRoleAssistant,
</span></span><span style=display:flex><span>			Content: <span style=color:#f1fa8c>&#34;你好！欢迎来到披萨餐厅！您想点什么呢？&#34;</span>,
</span></span><span style=display:flex><span>		},
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;%s: %s\n&#34;</span>, messages[<span style=color:#bd93f9>1</span>].Role, messages[<span style=color:#bd93f9>1</span>].Content)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>for</span> {
</span></span><span style=display:flex><span>		<span style=color:#8be9fd;font-style:italic>var</span> s <span style=color:#8be9fd>string</span>
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;user: &#34;</span>)
</span></span><span style=display:flex><span>		_, err <span style=color:#ff79c6>:=</span> fmt.<span style=color:#50fa7b>Scanf</span>(<span style=color:#f1fa8c>&#34;%s&#34;</span>, <span style=color:#ff79c6>&amp;</span>s)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>			<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>if</span> s <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;end&#34;</span> {
</span></span><span style=display:flex><span>			<span style=color:#ff79c6>break</span>
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		messages = <span style=color:#8be9fd;font-style:italic>append</span>(messages, openai.ChatCompletionMessage{
</span></span><span style=display:flex><span>			Role:    openai.ChatMessageRoleUser,
</span></span><span style=display:flex><span>			Content: s,
</span></span><span style=display:flex><span>		})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletionWithMessage</span>(context.<span style=color:#50fa7b>TODO</span>(), messages)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>			fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>			<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;%s: %s\n&#34;</span>, resp.Choices[<span style=color:#bd93f9>0</span>].Message.Role, resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>		messages = <span style=color:#8be9fd;font-style:italic>append</span>(messages, resp.Choices[<span style=color:#bd93f9>0</span>].Message)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	messages = <span style=color:#8be9fd;font-style:italic>append</span>(messages, openai.ChatCompletionMessage{
</span></span><span style=display:flex><span>		Role: openai.ChatMessageRoleSystem,
</span></span><span style=display:flex><span>		Content: <span style=color:#f1fa8c>`创建上一个食品订单的json摘要。\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>逐项列出每件商品的价格，字段应该是 1) 披萨，包括大小 2) 配料列表 3) 饮料列表，包括大小 4) 配菜列表包括大小 5) 总价
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>你应该给我返回一个可解析的json对象，包括上述字段`</span>,
</span></span><span style=display:flex><span>	})
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletionWithMessage</span>(context.<span style=color:#50fa7b>TODO</span>(), messages)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>assistant: 你好！欢迎来到披萨餐厅！您想点什么呢？
user: 一杯大可乐
assistant: 好的！您还需要点其他菜品吗？如果需要，请告诉我您的选择。
user: 一份大芝士披萨，配料加蘑菇，还有一份沙拉
assistant: 好的，一份大芝士披萨加蘑菇配料和一份希腊沙拉。请问您是选择自取还是外送呢？如果是外送，请告诉我您的地址。
user: 自取吧
assistant: 好的，您点的菜品是一份大芝士披萨（10.95）加蘑菇配料（1.50）和一份希腊沙拉（7.25），再加一杯大可乐（3.00），总金额为22.70。祝您用餐愉快！如果您有其他需要，请随时告诉我。
user: 你可以展示金额是怎么算的吗
assistant: 当然可以！大芝士披萨（10.95）+ 蘑菇配料（1.50）+ 希腊沙拉（7.25）+ 大可乐（3.00）= 22.70。祝您用餐愉快！如果您有其他需要，请随时告诉我。
user: end
{
    &#34;披萨&#34;: {
        &#34;名称&#34;: &#34;大芝士披萨&#34;,
        &#34;价格&#34;: 10.95
    },
    &#34;配料列表&#34;: [
        {
            &#34;名称&#34;: &#34;蘑菇&#34;,
            &#34;价格&#34;: 1.50
        }
    ],
    &#34;饮料列表&#34;: [
        {
            &#34;名称&#34;: &#34;大可乐&#34;,
            &#34;价格&#34;: 3.00
        }
    ],
    &#34;配菜列表&#34;: [
        {
            &#34;名称&#34;: &#34;希腊沙拉&#34;,
            &#34;价格&#34;: 7.25
        }
    ],
    &#34;总价&#34;: 22.70
}
</code></pre><p>订餐聊天机器人已经能够正常运行，可以自定义机器人的系统消息，改变它的行为，扮演各种不同的角色</p><h1 id=llm大模型部署>LLM大模型部署</h1><h2 id=ollama>Ollama</h2><h3 id=简介>简介</h3><p>Ollama实现一行命令在本地轻松部署大语言模型, 是一个开源框架，专门设计用于在本地运行大型语言模型。从而简化了在本地运行大型模型的过程。Ollama基于llama.cpp之上做了很多封装抽象。</p><p>llama.cpp项目是开发者Georgi Gerganov基于Meta开放的LLaMA模型（简易Python代码示例）手撸的纯C/C++版本，用于模型推理。其可执行的模型文件格式为GGUF，GGUF(GPT-Generated Unified Format)格式是用于存储大型模型预训练结果的，相较于Hugging Face和torch的bin文件，它采用了紧凑的二进制编码格式、优化的数据结构以及内存映射等技术，提供了更高效的数据存储和访问方式。GGUF也是由Georgi Gerganov发明的。</p><h3 id=安装>安装</h3><p>部署指引：<a href=https://ollama.com/download>https://ollama.com/download</a></p><h3 id=本地gguf模型导入>本地GGUF模型导入</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># vim Modelfile</span>
</span></span><span style=display:flex><span>FROM ./test-33b.Q4_0.gguf
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># ollama create example -f Modelfile</span>
</span></span></code></pre></div><h3 id=参数调优>参数调优</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># vim /etc/systemd/system/ollama.service</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Unit<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Description</span><span style=color:#ff79c6>=</span>Ollama Service
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>After</span><span style=color:#ff79c6>=</span>network-online.target
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Service<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>ExecStart</span><span style=color:#ff79c6>=</span>/usr/bin/ollama serve
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>User</span><span style=color:#ff79c6>=</span>ollama
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Group</span><span style=color:#ff79c6>=</span>ollama
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Restart</span><span style=color:#ff79c6>=</span>always
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>RestartSec</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>3</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 开启debug模式，可以看到更多输出日志</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Environment</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;OLLAMA_DEBUG=1&#34;</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Environment</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;OLLAMA_HOST=0.0.0.0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 绑定GPU卡，通过nvidia-smi -L可以查到，多卡使用逗号隔开</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Environment</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;CUDA_VISIBLE_DEVICES=GPU-18089521-10dd-9d6a-88df-cabf9d427d43&#34;</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 最大显存使用，单位byte</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Environment</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;OLLAMA_MAX_VRAM=23192823398&#34;</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 模型加载到显存中的过期时间</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Environment</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;OLLAMA_KEEP_ALIVE=5m&#34;</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 动态库，强制使用GPU，默认情况下在GPU不足的情况下会使用cpu</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Environment</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;OLLAMA_LLM_LIBRARY=cuda_v11&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Install<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>WantedBy</span><span style=color:#ff79c6>=</span>default.target
</span></span></code></pre></div><h3 id=ollama第三方生态>Ollama第三方生态</h3><ul><li>Cody：VSCode编程插件，支持对接Ollama，<a href=https://github.com/sourcegraph/cody>https://github.com/sourcegraph/cody</a></li><li>open-webui：Ollama WebUI，<a href=https://github.com/open-webui/open-webui>https://github.com/open-webui/open-webui</a></li><li>openui：AI网页生成，支持对接Ollama，<a href=https://github.com/wandb/openui>https://github.com/wandb/openui</a></li></ul><h2 id=企业级分布式推理框架xinference>企业级分布式推理框架Xinference</h2><p><a href=https://inference.readthedocs.io/zh-cn/latest/getting_started/using_xinference.html>https://inference.readthedocs.io/zh-cn/latest/getting_started/using_xinference.html</a></p><h2 id=麒麟v10系统llm大模型部署>麒麟v10系统LLM大模型部署</h2><h3 id=准备工作>准备工作</h3><h4 id=硬件环境>硬件环境</h4><table><thead><tr><th>类型</th><th>规格</th></tr></thead><tbody><tr><td>CPU</td><td>24核 AMD EPYC 7402</td></tr><tr><td>内存</td><td>32GB</td></tr><tr><td>GPU</td><td>2 * NVIDIA GeForce RTX 4090 显存24GB</td></tr><tr><td>操作系统</td><td>银河麒麟v10</td></tr><tr><td>架构</td><td>x86_64</td></tr><tr><td>内核版本</td><td>4.19.90-23.43.v2101.ky10.x86_64</td></tr><tr><td>GPU驱动版本</td><td>535.146.02</td></tr><tr><td>CUDA版本</td><td>12.2</td></tr></tbody></table><h4 id=下载huggingface_hub工具>下载huggingface_hub工具</h4><p>HuggingFace Hub是一个用于分享和获取自然语言处理（NLP）模型和相关资源的平台；类似代码界的GitHub。</p><p>pip3安装huggingface_hub工具</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># pip3 install -U huggingface_hub</span>
</span></span></code></pre></div><p>验证huggingface-cli</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># huggingface-cli env</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Copy-and-paste the text below in your GitHub issue.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>- huggingface_hub version: 0.20.3
</span></span><span style=display:flex><span>- Platform: Linux-4.19.90-23.43.v2101.ky10.x86_64-x86_64-with-glibc2.28
</span></span><span style=display:flex><span>- Python version: 3.11.7
</span></span><span style=display:flex><span>- Running in iPython ?: No
</span></span><span style=display:flex><span>- Running in notebook ?: No
</span></span><span style=display:flex><span>- Running in Google Colab ?: No
</span></span><span style=display:flex><span>- Token path ?: /root/.cache/huggingface/token
</span></span><span style=display:flex><span>- Has saved token ?: False
</span></span><span style=display:flex><span>- Configured git credential helpers:
</span></span><span style=display:flex><span>- FastAI: N/A
</span></span><span style=display:flex><span>- Tensorflow: N/A
</span></span><span style=display:flex><span>- Torch: 2.1.2
</span></span><span style=display:flex><span>- Jinja2: 3.1.3
</span></span><span style=display:flex><span>- Graphviz: N/A
</span></span><span style=display:flex><span>- Pydot: N/A
</span></span><span style=display:flex><span>- Pillow: 10.2.0
</span></span><span style=display:flex><span>- hf_transfer: N/A
</span></span><span style=display:flex><span>- gradio: N/A
</span></span><span style=display:flex><span>- tensorboard: N/A
</span></span><span style=display:flex><span>- numpy: 1.26.3
</span></span><span style=display:flex><span>- pydantic: 1.10.13
</span></span><span style=display:flex><span>- aiohttp: N/A
</span></span><span style=display:flex><span>- ENDPOINT: https://huggingface.co
</span></span><span style=display:flex><span>- HF_HUB_CACHE: /root/.cache/huggingface/hub
</span></span><span style=display:flex><span>- HF_ASSETS_CACHE: /root/.cache/huggingface/assets
</span></span><span style=display:flex><span>- HF_TOKEN_PATH: /root/.cache/huggingface/token
</span></span><span style=display:flex><span>- HF_HUB_OFFLINE: False
</span></span><span style=display:flex><span>- HF_HUB_DISABLE_TELEMETRY: False
</span></span><span style=display:flex><span>- HF_HUB_DISABLE_PROGRESS_BARS: None
</span></span><span style=display:flex><span>- HF_HUB_DISABLE_SYMLINKS_WARNING: False
</span></span><span style=display:flex><span>- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False
</span></span><span style=display:flex><span>- HF_HUB_DISABLE_IMPLICIT_TOKEN: False
</span></span><span style=display:flex><span>- HF_HUB_ENABLE_HF_TRANSFER: False
</span></span><span style=display:flex><span>- HF_HUB_ETAG_TIMEOUT: <span style=color:#bd93f9>10</span>
</span></span><span style=display:flex><span>- HF_HUB_DOWNLOAD_TIMEOUT: <span style=color:#bd93f9>10</span>
</span></span></code></pre></div><p>修改HF环境变量，使用国内镜像地址<code>https://hf-mirror.com</code>，加速下载模型</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># export HF_ENDPOINT=https://hf-mirror.com</span>
</span></span></code></pre></div><h4 id=第三方工具hfd>第三方工具hfd</h4><p>使用<a href=https://hf-mirror.com/>https://hf-mirror.com/</a>提供的hfd专用下载工具，它是基于成熟工具git+aria2，可以做到稳定下载不断线，测下来比huggingface-cli稳定。</p><p>下载hfd工具</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>wget -c https://hf-mirror.com/hfd/hfd.sh
</span></span><span style=display:flex><span>chmod a+x hfd.sh
</span></span></code></pre></div><p>下载模型示例</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>HF_ENDPOINT</span><span style=color:#ff79c6>=</span>https://hf-mirror.com ./hfd.sh &lt;模型&gt; --tool aria2c -x <span style=color:#bd93f9>4</span>
</span></span></code></pre></div><p>下载数据集示例</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>HF_ENDPOINT</span><span style=color:#ff79c6>=</span>https://hf-mirror.com ./hfd.sh &lt;数据集&gt; --dataset --tool aria2c -x <span style=color:#bd93f9>4</span>
</span></span></code></pre></div><h4 id=下载模型>下载模型</h4><p>下载模型，以通义千问14b-chat模型为例</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># 创建模型目录，用于存放下载的模型文件</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># mkdir -p /opt/ice/models/qwen-14b-chat </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># huggingface-cli download \</span>
</span></span><span style=display:flex><span>--resume-download --local-dir-use-symlinks False <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--local-dir /opt/ice/models/qwen-14b-chat <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>qwen/qwen-14b-chat
</span></span><span style=display:flex><span>Consider using <span style=color:#f1fa8c>`</span>hf_transfer<span style=color:#f1fa8c>`</span> <span style=color:#ff79c6>for</span> faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer <span style=color:#ff79c6>for</span> more details.
</span></span><span style=display:flex><span>Fetching <span style=color:#bd93f9>37</span> files:   0%|                                                                                                                              | 0/37 <span style=color:#ff79c6>[</span>00:00&lt;?, ?it/s<span style=color:#ff79c6>]</span>downloading https://hf-mirror.com/qwen/qwen-14b-chat/resolve/cdaff792392504e679496a9f386acf3c1e4333a5/.gitattributes to /root/.cache/huggingface/hub/models--qwen--qwen-14b-chat/blobs/a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete
</span></span><span style=display:flex><span>downloading https://hf-mirror.com/qwen/qwen-14b-chat/resolve/cdaff792392504e679496a9f386acf3c1e4333a5/LICENSE to /root/.cache/huggingface/hub/models--qwen--qwen-14b-chat/blobs/5be33384d19169a98eee863ff09c74eb32e37696.incomplete
</span></span></code></pre></div><p>下载需要登录的模型(Gated Model)，添加参数<code>--token hf_***</code>参数，hf_***是access token，token从这里获取<code>https://huggingface.co/settings/tokens</code></p><h4 id=配置nvidia-docker-runtime>配置nvidia docker runtime</h4><p>下载nvidia runtime和工具包，下面启动大模型是采用docker方式来启动的，原生docker runtime不支持GPU</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost<span style=color:#ff79c6>]</span><span style=color:#6272a4># yum install -y nvidia-container-toolkit nvidia-container-runtime</span>
</span></span></code></pre></div><p>生成nvidia runtime的docker配置</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost<span style=color:#ff79c6>]</span><span style=color:#6272a4># nvidia-ctk runtime configure --runtime=docker</span>
</span></span><span style=display:flex><span>INFO<span style=color:#ff79c6>[</span>0000<span style=color:#ff79c6>]</span> Loading docker config from /etc/docker/daemon.json
</span></span><span style=display:flex><span>INFO<span style=color:#ff79c6>[</span>0000<span style=color:#ff79c6>]</span> Config file does not exist, creating new one
</span></span><span style=display:flex><span>INFO<span style=color:#ff79c6>[</span>0000<span style=color:#ff79c6>]</span> Wrote updated config to /etc/docker/daemon.json
</span></span><span style=display:flex><span>INFO<span style=color:#ff79c6>[</span>0000<span style=color:#ff79c6>]</span> It is recommended that the docker daemon be restarted.
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost qwen-14b<span style=color:#ff79c6>]</span><span style=color:#6272a4># cat /etc/docker/daemon.json</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;runtimes&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;nvidia&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;args&#34;</span>: <span style=color:#ff79c6>[]</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;path&#34;</span>: <span style=color:#f1fa8c>&#34;nvidia-container-runtime&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost<span style=color:#ff79c6>]</span><span style=color:#6272a4># systemctl restart docker</span>
</span></span></code></pre></div><p>验证nvidia runtime是否安装成功</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost<span style=color:#ff79c6>]</span><span style=color:#6272a4># docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi</span>
</span></span></code></pre></div><h3 id=非vllm方式运行大模型>非vLLM方式运行大模型</h3><h4 id=下载api-for-open-llm>下载api-for-open-llm</h4><p>api-for-open-llm是开源大模型的统一后端接口，与OpenAI的响应保持一致。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># git clone https://github.com/xusenlinzy/api-for-open-llm.git</span>
</span></span></code></pre></div><h4 id=api-for-open-llm配置参数>api-for-open-llm配置参数</h4><p>拷贝一份配置</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># cd api-for-open-llm/</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># cp .env.example .env</span>
</span></span></code></pre></div><p>编辑参数</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost api-for-open-llm<span style=color:#ff79c6>]</span><span style=color:#6272a4># vim .env</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>PORT</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>8000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># model related</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>MODEL_NAME</span><span style=color:#ff79c6>=</span>qwen      <span style=color:#6272a4># 模型名</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>MODEL_PATH</span><span style=color:#ff79c6>=</span>/opt/ice/models/qwen-14b-chat <span style=color:#6272a4># 下载好的模型文件路径</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>EMBEDDING_NAME</span><span style=color:#ff79c6>=</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>ADAPTER_MODEL_PATH</span><span style=color:#ff79c6>=</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>QUANTIZE</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>16</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>CONTEXT_LEN</span><span style=color:#ff79c6>=</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>LOAD_IN_8BIT</span><span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>false</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>LOAD_IN_4BIT</span><span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>false</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>USING_PTUNING_V2</span><span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>false</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>STREAM_INTERVERL</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>PROMPT_NAME</span><span style=color:#ff79c6>=</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># device related</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>DEVICE</span><span style=color:#ff79c6>=</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># &#34;auto&#34;, &#34;cuda:0&#34;, &#34;cuda:1&#34;, ...</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>DEVICE_MAP</span><span style=color:#ff79c6>=</span>auto
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>GPUS</span><span style=color:#ff79c6>=</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>NUM_GPUs</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>DTYPE</span><span style=color:#ff79c6>=</span>half
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># api related</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>API_PREFIX</span><span style=color:#ff79c6>=</span>/v1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>USE_STREAMER_V2</span><span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>false</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>ENGINE</span><span style=color:#ff79c6>=</span>default
</span></span></code></pre></div><h4 id=api-for-open-llm加载模型>api-for-open-llm加载模型</h4><p>api-for-open-llm启动方式有docker和本地方式，推荐使用docker</p><p>使用docker方式的话，需要先构建llm-api镜像</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost api-for-open-llm<span style=color:#ff79c6>]</span><span style=color:#6272a4># docker build -f docker/Dockerfile -t llm-api:pytorch .</span>
</span></span></code></pre></div><p>docker启动</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost api-for-open-llm<span style=color:#ff79c6>]</span><span style=color:#6272a4># docker run -it -d \</span>
</span></span><span style=display:flex><span>--gpus all <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--ipc<span style=color:#ff79c6>=</span>host -p 8000:8000 <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--name<span style=color:#ff79c6>=</span>llm-api --ulimit <span style=color:#8be9fd;font-style:italic>memlock</span><span style=color:#ff79c6>=</span>-1 <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--ulimit <span style=color:#8be9fd;font-style:italic>stack</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>67108864</span> <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>-v <span style=color:#f1fa8c>`</span><span style=color:#8be9fd;font-style:italic>pwd</span><span style=color:#f1fa8c>`</span>:/workspace -v /opt/ice/models/qwen-14b-chat:/opt/ice/models/qwen-14b-chat <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>llm-api:pytorch <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>python api/server.py
</span></span></code></pre></div><p>查看docker实例启动日志，验证模型是否启动成功</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost api-for-open-llm<span style=color:#ff79c6>]</span><span style=color:#6272a4># docker logs -f llm-api</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>=============</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>==</span> <span style=color:#8be9fd;font-style:italic>PyTorch</span> <span style=color:#ff79c6>==</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>=============</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NVIDIA Release 23.10 <span style=color:#ff79c6>(</span>build 71422337<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>PyTorch Version 2.1.0a0+32f93b1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Container image Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> 2023, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> 2014-2023 Facebook Inc.
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> 2011-2014 Idiap Research Institute <span style=color:#ff79c6>(</span>Ronan Collobert<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> 2012-2014 Deepmind Technologies    <span style=color:#ff79c6>(</span>Koray Kavukcuoglu<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> 2011-2012 NEC Laboratories America <span style=color:#ff79c6>(</span>Koray Kavukcuoglu<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> 2011-2013 NYU                      <span style=color:#ff79c6>(</span>Clement Farabet<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> 2006-2010 NEC Laboratories America <span style=color:#ff79c6>(</span>Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> <span style=color:#bd93f9>2006</span>      Idiap Research Institute <span style=color:#ff79c6>(</span>Samy Bengio<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> 2001-2004 Idiap Research Institute <span style=color:#ff79c6>(</span>Ronan Collobert, Samy Bengio, Johnny Mariethoz<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> <span style=color:#bd93f9>2015</span>      Google Inc.
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> <span style=color:#bd93f9>2015</span>      Yangqing Jia
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> 2013-2016 The Caffe contributors
</span></span><span style=display:flex><span>All rights reserved.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Various files include modifications <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> NVIDIA CORPORATION &amp; AFFILIATES.  All rights reserved.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>This container image and its contents are governed by the NVIDIA Deep Learning Container License.
</span></span><span style=display:flex><span>By pulling and using the container, you accept the terms and conditions of this license:
</span></span><span style=display:flex><span>https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>2024-02-01 01:53:11.597 | DEBUG    | api.config:&lt;module&gt;:265 - SETTINGS: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;host&#34;</span>: <span style=color:#f1fa8c>&#34;0.0.0.0&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;port&#34;</span>: 8000,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;api_prefix&#34;</span>: <span style=color:#f1fa8c>&#34;/v1&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;engine&#34;</span>: <span style=color:#f1fa8c>&#34;default&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;model_name&#34;</span>: <span style=color:#f1fa8c>&#34;qwen&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;model_path&#34;</span>: <span style=color:#f1fa8c>&#34;/opt/ice/models/qwen-14b-chat&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;adapter_model_path&#34;</span>: null,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;resize_embeddings&#34;</span>: false,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;dtype&#34;</span>: <span style=color:#f1fa8c>&#34;half&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;device&#34;</span>: <span style=color:#f1fa8c>&#34;cuda&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;device_map&#34;</span>: <span style=color:#f1fa8c>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;gpus&#34;</span>: null,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;num_gpus&#34;</span>: 2,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;only_embedding&#34;</span>: false,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;embedding_name&#34;</span>: null,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;embedding_size&#34;</span>: -1,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;embedding_device&#34;</span>: <span style=color:#f1fa8c>&#34;cuda&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;quantize&#34;</span>: 16,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;load_in_8bit&#34;</span>: false,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;load_in_4bit&#34;</span>: false,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;using_ptuning_v2&#34;</span>: false,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;pre_seq_len&#34;</span>: 128,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;context_length&#34;</span>: -1,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;chat_template&#34;</span>: null,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;rope_scaling&#34;</span>: null,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;flash_attn&#34;</span>: false,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;trust_remote_code&#34;</span>: false,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;tokenize_mode&#34;</span>: <span style=color:#f1fa8c>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;tensor_parallel_size&#34;</span>: 1,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;gpu_memory_utilization&#34;</span>: 0.9,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;max_num_batched_tokens&#34;</span>: -1,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;max_num_seqs&#34;</span>: 256,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;quantization_method&#34;</span>: null,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;use_streamer_v2&#34;</span>: false,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;api_keys&#34;</span>: null,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;activate_inference&#34;</span>: true,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;interrupt_requests&#34;</span>: true,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;n_gpu_layers&#34;</span>: 0,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;main_gpu&#34;</span>: 0,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;tensor_split&#34;</span>: null,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;n_batch&#34;</span>: 512,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;n_threads&#34;</span>: 24,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;n_threads_batch&#34;</span>: 24,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;rope_scaling_type&#34;</span>: -1,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;rope_freq_base&#34;</span>: 0.0,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;rope_freq_scale&#34;</span>: 0.0,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;tgi_endpoint&#34;</span>: null,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;tei_endpoint&#34;</span>: null,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;max_concurrent_requests&#34;</span>: 256,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;max_client_batch_size&#34;</span>: <span style=color:#bd93f9>32</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>2024-02-01 01:53:22.457 | INFO     | api.adapter.patcher:patch_tokenizer:119 - Add eos token: &lt;|endoftext|&gt;
</span></span><span style=display:flex><span>2024-02-01 01:53:22.457 | INFO     | api.adapter.patcher:patch_tokenizer:126 - Add pad token: &lt;|endoftext|&gt;
</span></span><span style=display:flex><span>/root/.cache/huggingface/modules/transformers_modules/qwen-14b-chat/modeling_qwen.py:969: DeprecationWarning: The <span style=color:#f1fa8c>&#39;warn&#39;</span> method is deprecated, use <span style=color:#f1fa8c>&#39;warning&#39;</span> instead
</span></span><span style=display:flex><span>  logger.warn<span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;Try importing flash-attention for faster inference...&#34;</span><span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>Try importing flash-attention <span style=color:#ff79c6>for</span> faster inference...
</span></span><span style=display:flex><span>Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
</span></span><span style=display:flex><span>Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
</span></span><span style=display:flex><span>Loading checkpoint shards: 100% 15/15 <span style=color:#ff79c6>[</span>00:06&lt;00:00,  2.41it/s<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>2024-02-01 01:53:29.693 | INFO     | api.models:create_generate_model:61 - Using default engine
</span></span><span style=display:flex><span>2024-02-01 01:53:29.693 | INFO     | api.core.default:_check_construct_prompt:128 - Using Qwen Model <span style=color:#ff79c6>for</span> Chat!
</span></span><span style=display:flex><span>INFO:     Started server process <span style=color:#ff79c6>[</span>1<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>INFO:     Waiting <span style=color:#ff79c6>for</span> application startup.
</span></span><span style=display:flex><span>INFO:     Application startup complete.
</span></span><span style=display:flex><span>INFO:     Uvicorn running on http://0.0.0.0:8000 <span style=color:#ff79c6>(</span>Press CTRL+C to quit<span style=color:#ff79c6>)</span>
</span></span></code></pre></div><h3 id=vllm方式运行大模型>vLLM方式运行大模型</h3><p>vLLM是来自UC Berkeley的LMSYS在LLM推理方面的最新工作（发布Vicuna大模型的那个团队），最大亮点是采用Paged Attention技术，
结合Continuous Batching，极大地优化了realtime场景下的LLM serving 的 throughput 与内存使用。</p><p>除了vLLM外可以加速大模型推理，还有FlashAttention；vLLM的核心是PagedAttention，FlashAttention是一种重新排序注意力计算的算法，它利用平铺、重计算等经典技术来显著提升计算速度，并将序列长度中的内存使用实现从二次到线性减少。Flash Attention的主要目的是加速和节省内存。</p><p>FlashAttention-2需要GPU支持：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>FlashAttention-2 currently supports:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100). Support for Turing GPUs (T4, RTX 2080) is coming soon, please use FlashAttention 1.x for Turing GPUs for now.
</span></span><span style=display:flex><span>Datatype fp16 and bf16 (bf16 requires Ampere, Ada, or Hopper GPUs).
</span></span><span style=display:flex><span>All head dimensions up to 256. Head dim &gt; 192 backward requires A100/A800 or H100/H800.
</span></span></code></pre></div><h4 id=编译vllm镜像>编译vLLM镜像</h4><p>正常操作系统用<code>vllm/vllm-openai</code>这个镜像就可以了，但是通义千问模型需要额外装包，所以重新编译下vllm-openai镜像</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost<span style=color:#ff79c6>]</span><span style=color:#6272a4># vim Dockerfile</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 使用vllm/vllm-openai作为基础镜像</span>
</span></span><span style=display:flex><span>FROM vllm/vllm-openai:v0.3.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 通义千问需要额外安装包</span>
</span></span><span style=display:flex><span>RUN pip install tiktoken <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>  -i https://pypi.tuna.tsinghua.edu.cn/simple
</span></span></code></pre></div><p>docker build编译</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost<span style=color:#ff79c6>]</span><span style=color:#6272a4># docker build -f Dockerfile -t vllm/vllm-openai:kylin-v10 .</span>
</span></span></code></pre></div><h4 id=vllm加载模型>vLLM加载模型</h4><p>docker运行vLLM镜像</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost<span style=color:#ff79c6>]</span><span style=color:#6272a4># docker run -it -d \</span>
</span></span><span style=display:flex><span>--gpus all -v /opt/ice/models/qwen-14b-chat:/opt/qwen-14b-chat <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--name vllm-api <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>-p 8000:8000 --ipc<span style=color:#ff79c6>=</span>host <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>vllm/vllm-openai:kylin-v10 <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--model /opt/qwen-14b-chat <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--max-model-len <span style=color:#bd93f9>8096</span>
</span></span><span style=display:flex><span>--enforce-eager <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--trust-remote-code <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--tensor-parallel-size <span style=color:#bd93f9>2</span>
</span></span></code></pre></div><p>注：vLLM方式启动带的<code>--model</code>这个参数，调用api接口的时候也需要传入同样的值；<code>--tensor-parallel-size</code>用于在多个GPU间分配工作，<code>max-model-len</code>指定模型上下文长度，<code>--served-model-name Qwen1.5-7B-Chat</code>可以指定模型的名字，没指定的话默认就用<code>--model</code>用于模型名</p><p>查看显卡占用，模型分布在两张卡上，所谓的模型并行</p><pre tabindex=0><code>[root@localhost vllm]# nvidia-smi
Thu Feb  1 19:32:24 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4090        Off | 00000000:81:00.0 Off |                  Off |
| 55%   61C    P2             241W / 450W |  22026MiB / 24564MiB |     97%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce RTX 4090        Off | 00000000:C1:00.0 Off |                  Off |
| 68%   66C    P2             243W / 450W |  20796MiB / 24564MiB |     90%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    151961      C   python3                                   22010MiB |
|    1   N/A  N/A    154943      C   ray::RayWorkerVllm.execute_method         20780MiB |
+---------------------------------------------------------------------------------------+
</code></pre><p><code>nvidia-smi</code>的展示算比较简洁的，推荐用nvitop来看gpu使用情况，不仅有nvtop详细的展示，还拥有gpustat彩色界面展示</p><p>查看vllm-api容器日志，验证是否启动成功</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost vllm<span style=color:#ff79c6>]</span><span style=color:#6272a4># docker logs -f vllm-api</span>
</span></span><span style=display:flex><span>INFO 02-01 10:14:02 api_server.py:727<span style=color:#ff79c6>]</span> args: Namespace<span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>host</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>port</span><span style=color:#ff79c6>=</span>8000, <span style=color:#8be9fd;font-style:italic>allow_credentials</span><span style=color:#ff79c6>=</span>False, <span style=color:#8be9fd;font-style:italic>allowed_origins</span><span style=color:#ff79c6>=[</span><span style=color:#f1fa8c>&#39;*&#39;</span><span style=color:#ff79c6>]</span>, <span style=color:#8be9fd;font-style:italic>allowed_methods</span><span style=color:#ff79c6>=[</span><span style=color:#f1fa8c>&#39;*&#39;</span><span style=color:#ff79c6>]</span>, <span style=color:#8be9fd;font-style:italic>allowed_headers</span><span style=color:#ff79c6>=[</span><span style=color:#f1fa8c>&#39;*&#39;</span><span style=color:#ff79c6>]</span>, <span style=color:#8be9fd;font-style:italic>served_model_name</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>chat_template</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>response_role</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;assistant&#39;</span>, <span style=color:#8be9fd;font-style:italic>ssl_keyfile</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>ssl_certfile</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>model</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;/opt/qwen-14b-chat&#39;</span>, <span style=color:#8be9fd;font-style:italic>tokenizer</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>revision</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>tokenizer_revision</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>tokenizer_mode</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;auto&#39;</span>, <span style=color:#8be9fd;font-style:italic>trust_remote_code</span><span style=color:#ff79c6>=</span>True, <span style=color:#8be9fd;font-style:italic>download_dir</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>load_format</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;auto&#39;</span>, <span style=color:#8be9fd;font-style:italic>dtype</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;auto&#39;</span>, <span style=color:#8be9fd;font-style:italic>max_model_len</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>worker_use_ray</span><span style=color:#ff79c6>=</span>False, <span style=color:#8be9fd;font-style:italic>pipeline_parallel_size</span><span style=color:#ff79c6>=</span>1, <span style=color:#8be9fd;font-style:italic>tensor_parallel_size</span><span style=color:#ff79c6>=</span>2, <span style=color:#8be9fd;font-style:italic>max_parallel_loading_workers</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>block_size</span><span style=color:#ff79c6>=</span>16, <span style=color:#8be9fd;font-style:italic>seed</span><span style=color:#ff79c6>=</span>0, <span style=color:#8be9fd;font-style:italic>swap_space</span><span style=color:#ff79c6>=</span>4, <span style=color:#8be9fd;font-style:italic>gpu_memory_utilization</span><span style=color:#ff79c6>=</span>0.9, <span style=color:#8be9fd;font-style:italic>max_num_batched_tokens</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>max_num_seqs</span><span style=color:#ff79c6>=</span>256, <span style=color:#8be9fd;font-style:italic>max_paddings</span><span style=color:#ff79c6>=</span>256, <span style=color:#8be9fd;font-style:italic>disable_log_stats</span><span style=color:#ff79c6>=</span>False, <span style=color:#8be9fd;font-style:italic>quantization</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>enforce_eager</span><span style=color:#ff79c6>=</span>True, <span style=color:#8be9fd;font-style:italic>max_context_len_to_capture</span><span style=color:#ff79c6>=</span>8192, <span style=color:#8be9fd;font-style:italic>engine_use_ray</span><span style=color:#ff79c6>=</span>False, <span style=color:#8be9fd;font-style:italic>disable_log_requests</span><span style=color:#ff79c6>=</span>False, <span style=color:#8be9fd;font-style:italic>max_log_len</span><span style=color:#ff79c6>=</span>None<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>2024-02-01 10:14:07,259	INFO worker.py:1724 -- Started a <span style=color:#8be9fd;font-style:italic>local</span> Ray instance.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>INFO 02-01 10:14:08 llm_engine.py:70<span style=color:#ff79c6>]</span> Initializing an LLM engine with config: <span style=color:#8be9fd;font-style:italic>model</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;/opt/qwen-14b-chat&#39;</span>, <span style=color:#8be9fd;font-style:italic>tokenizer</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;/opt/qwen-14b-chat&#39;</span>, <span style=color:#8be9fd;font-style:italic>tokenizer_mode</span><span style=color:#ff79c6>=</span>auto, <span style=color:#8be9fd;font-style:italic>revision</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>tokenizer_revision</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>trust_remote_code</span><span style=color:#ff79c6>=</span>True, <span style=color:#8be9fd;font-style:italic>dtype</span><span style=color:#ff79c6>=</span>torch.float16, <span style=color:#8be9fd;font-style:italic>max_seq_len</span><span style=color:#ff79c6>=</span>2048, <span style=color:#8be9fd;font-style:italic>download_dir</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>load_format</span><span style=color:#ff79c6>=</span>auto, <span style=color:#8be9fd;font-style:italic>tensor_parallel_size</span><span style=color:#ff79c6>=</span>2, <span style=color:#8be9fd;font-style:italic>quantization</span><span style=color:#ff79c6>=</span>None, <span style=color:#8be9fd;font-style:italic>enforce_eager</span><span style=color:#ff79c6>=</span>True, <span style=color:#8be9fd;font-style:italic>seed</span><span style=color:#ff79c6>=</span>0<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>WARNING 02-01 10:14:09 tokenizer.py:62<span style=color:#ff79c6>]</span> Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
</span></span><span style=display:flex><span>INFO 02-01 10:14:23 llm_engine.py:275<span style=color:#ff79c6>]</span> <span style=color:#6272a4># GPU blocks: 981, # CPU blocks: 655</span>
</span></span><span style=display:flex><span>WARNING 02-01 10:14:25 tokenizer.py:62<span style=color:#ff79c6>]</span> Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
</span></span><span style=display:flex><span>WARNING 02-01 10:14:25 api_server.py:123<span style=color:#ff79c6>]</span> No chat template provided. Chat API will not work.
</span></span><span style=display:flex><span>INFO:     Started server process <span style=color:#ff79c6>[</span>1<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>INFO:     Waiting <span style=color:#ff79c6>for</span> application startup.
</span></span><span style=display:flex><span>INFO:     Application startup complete.
</span></span><span style=display:flex><span>INFO:     Uvicorn running on http://0.0.0.0:8000 <span style=color:#ff79c6>(</span>Press CTRL+C to quit<span style=color:#ff79c6>)</span>
</span></span></code></pre></div><h3 id=验证大模型接口>验证大模型接口</h3><p>无论是用vllm启动还是非vllm启动的大模型服务，它们的api请求参数是一致的<code>/v1/completions/</code></p><p>请求body参数解释</p><table><thead><tr><th>字段</th><th>类型</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>model</td><td>string 必填</td><td></td><td>使用的模型ID。可以使用模型API列表接口查看所有可用的模型，有关模型的描述，请参阅模型概述。</td></tr><tr><td>prompt</td><td>string或array 可选</td><td>&lt;|endoftext|></td><td>生成完成的提示，编码为字符串、字符串数组、token数组或token数组的数组。请注意， &lt;|endoftext|>是模型在训练期间看到的文档分隔符，因此，如果未指定提示，则模型将从新文档的开头生成。</td></tr><tr><td>suffix</td><td>string 可选</td><td>null</td><td>插入文本完成后出现的后缀。</td></tr><tr><td>max_tokens</td><td>nteger 可选</td><td>16</td><td>完成时要生成的最大token数量。提示的token计数加上max_tokens不能超过模型的上下文长度。大多数模型的上下文长度为2048个token（最新模型除外，支持4096个）</td></tr><tr><td>temperature</td><td>number 可选</td><td>1</td><td>使用什么样的采样温度，介于0和1之间。较高的值（如0.8）将使输出更加随机，而较低的值（例如0.2）将使其更加集中和确定。通常建议更改它或top_p，但不能同时更改两者。</td></tr><tr><td>top_p</td><td>number 可选</td><td>1</td><td>一种用温度采样的替代品，称为核采样，其中模型考虑了具有top_p概率质量的token的结果。因此，0.1意味着只考虑包含前10%概率质量的token。通常建议改变它或temperature，但不能同时更改两者。</td></tr><tr><td>n</td><td>integer 可选</td><td>1</td><td>每个提示要生成多少个完成。注意：由于此参数会生成许多完成，因此它可以快速消耗您的token配额。小心使用，并确保您对max_tokens和stop有合理的设置。</td></tr><tr><td>stream</td><td>boolean 可选</td><td>false</td><td>是否流回部分进度。如果设置，token将在可用时作为仅数据服务器发送的事件发送，流将以data:[DONE]消息终止。</td></tr><tr><td>logprobs</td><td>interger 可选</td><td>null</td><td>按可能性概率选择token的个数。例如，如果logprobs为5，API将返回5个最有可能的token的列表。API将始终返回采样token的logprob，因此响应中可能最多有logprobs+1元素。logprobs的最大值为5。</td></tr><tr><td>echo</td><td>boolean 可选</td><td>false</td><td>除了完成之外，回显提示</td></tr><tr><td>stop</td><td>string或array 可选</td><td>null</td><td>最多4个序列，API将停止生成进一步的token。返回的文本将不包含停止序列。</td></tr><tr><td>presence_penalty</td><td>number 可选</td><td>0</td><td>取值范围：-2.0~2.0。正值根据新token到目前为止是否出现在文本中来惩罚它们，这增加了模型谈论新主题的可能性。</td></tr><tr><td>best_of</td><td>interger 可选</td><td>1</td><td>在服务器端生成best_of个完成，并返回“最佳”（每个token的日志概率最高）。结果无法流式传输。与n一起使用时，best_of控制候选完成的数量，n指定要返回的数量–best_of必须大于n。注意：由于此参数会生成许多完成，因此它可以快速消耗token配额。小心使用并确保您对max_tokens和stop进行了合理的设置。</td></tr><tr><td>logit_bias</td><td>map 可选</td><td>null</td><td>修改完成时出现指定token的可能性。接受将token（由其在GPT token生成器中的token ID指定）映射到从-100到100的相关偏差值的json对象。您可以使用此token工具（适用于GPT-2和GPT-3）将文本转换为token ID。在数学上，偏差在采样之前被添加到模型生成的逻辑中。确切的效果因模型而异，但-1和1之间的值应该会降低或增加选择的可能性；-100或100这样的值应该导致相关token的禁止或独占选择。例如，可以传递｛“50256”：-100｝以防止生成&lt;|endoftext|>的token。</td></tr><tr><td>user</td><td>string 可选</td><td></td><td>代表最终用户的唯一标识符，可帮助OpenAI监控和检测滥用。</td></tr></tbody></table><p>curl发个prompt请求，查看是否正常返回</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>root@localhost ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># curl http://localhost:8000/v1/completions -H &#34;Content-Type: application/json&#34; -d &#39;{&#34;model&#34;: &#34;/opt/qwen-14b-chat&#34;,&#34;prompt&#34;: &#34;Hi,深圳排名前十的好玩的公园？&#34;,&#34;max_tokens&#34;: 1000,&#34;temperature&#34;: 0, &#34;stop&#34;: &#34;&lt;|endoftext|&gt;&#34;}&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 输出</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;id&#34;</span>: <span style=color:#f1fa8c>&#34;cmpl-dcee971a-fc6b-45f5-bd56-d7433553d9e5&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;choices&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;finish_reason&#34;</span>: <span style=color:#f1fa8c>&#34;stop&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;index&#34;</span>: 0,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;logprobs&#34;</span>: null,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;text&#34;</span>: <span style=color:#f1fa8c>&#34;深圳有哪些好玩的公园呢？公园里景色优美，空气清新，是休闲放松的好去处。\n深圳公园多，而且多有特色，比如深圳湾公园、莲花山公园、笔架山公园、洪湖公园、大梅沙海滨公园等等，都是深圳市民和游客经常去的地方。这些公园各有特色，各有特色，各有各自的特色。\n如果时间充足，可以一一游玩。如果时间紧凑，可以挑选一些有代表性的公园游玩。今天就带大家游览一下深圳十大公园。\n深圳湾公园位于深圳市南山区西部，东临深圳湾，西至白石洲，北靠福田区，南接蛇口。公园包括深圳湾栈道、深圳湾运动公园、深圳湾休闲文化公园等。深圳湾公园是深圳最浪漫的公园，最适合新人拍摄婚纱照，也适合全家出游。\n莲花山公园位于深圳市中心区北端红荔路与新洲路交汇处，占地194公顷，是深圳最大的公园。公园包括山顶广场、风筝广场、观景台、莲花山音乐厅、深圳改革开放展览馆等。莲花山公园是深圳市民最喜欢的公园之一，也是深圳最著名的公园。\n笔架山公园位于深圳市中心区，北临深南大道，西接福华三路。公园包括笔架山、笔架山公墓、笔架山儿童公园、笔架山植物园等。笔架山公园是深圳最安静的公园，最适合周末休闲。\n洪湖公园位于深圳市中心区，南临深南大道，北接红荔路，东临深南大道，西接华强北路。公园包括洪湖、洪湖公园、洪湖街、洪湖街等。洪湖公园是深圳最浪漫的公园，最适合新人拍摄婚纱照，也适合全家出游。\n大梅沙海滨公园位于深圳市盐田区东部，西临大梅沙海滨，东至盐田港。公园包括大梅沙海滨、大梅沙海滨公园、大梅沙海滨步行街等。大梅沙海滨公园是深圳最受欢迎的公园，也是深圳最著名的公园。\n深圳湾公园、莲花山公园、笔架山公园、洪湖公园、大梅沙海滨公园都是深圳市民和游客经常去的地方，也是深圳最受欢迎的公园。\n深圳湾公园位于深圳市南山区西部，东临深圳湾，西至白石洲，北靠福田区，南接蛇口。公园包括深圳湾栈道、深圳湾运动公园、深圳湾休闲文化公园等。深圳湾公园是深圳最浪漫的公园，最适合新人拍摄婚纱照，也适合全家出游。\n莲花山公园位于深圳市中心区北端红荔路与新洲路交汇处，占地194公顷，是深圳最大的公园。公园包括山顶广场、风筝广场、观景台、莲花山音乐厅、深圳改革开放展览馆等。莲花山公园是深圳市民最喜欢的公园之一，也是深圳最著名的公园。\n笔架山公园位于深圳市中心区，北临深南大道，西接福华三路。公园包括笔架山、笔架山公墓、笔架山儿童公园、笔架山植物园等。笔架山公园是深圳最安静的公园，最适合周末休闲。\n洪湖公园位于深圳市中心区，南临深南大道，北接红荔路，东临深南大道，西接华强北路。公园包括洪湖、洪湖公园、洪湖街、洪湖街等。洪湖公园是深圳最浪漫的公园，最适合新人拍摄婚纱照，也适合全家出游。\n大梅沙海滨公园位于深圳市盐田区东部，西临大梅沙海滨，东至盐田港。公园包括大梅沙海滨、大梅沙海滨公园、大梅沙海滨步行街等。大梅沙海滨公园是深圳最受欢迎的公园，也是深圳最著名的公园。\n以上就是深圳十大公园，包括深圳湾公园、莲花山公园、笔架山公园、洪湖公园、大梅沙海滨公园等。深圳十大公园各有特色，各有特色，各有各自的特色。&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;created&#34;</span>: 1706767165,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;model&#34;</span>: <span style=color:#f1fa8c>&#34;/opt/qwen-14b-chat&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;object&#34;</span>: <span style=color:#f1fa8c>&#34;text_completion&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;system_fingerprint&#34;</span>: null,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;usage&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;completion_tokens&#34;</span>: 859,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;prompt_tokens&#34;</span>: 10,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;total_tokens&#34;</span>: <span style=color:#bd93f9>869</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span></code></pre></div><h3 id=常见错误>常见错误</h3><p>问题：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 7.80 GiB total capacity; 5.21 GiB already allocated; 
</span></span><span style=display:flex><span>173.38 MiB free; 5.27 GiB reserved in total by PyTorch) 
</span></span><span style=display:flex><span>If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation. 
</span></span><span style=display:flex><span>See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
</span></span></code></pre></div><p>解决：</p><blockquote><p>最优设置策略：将max_split_size_mb设置为小于OOM发生时的显存请求大小最小值的最大整数值，
这里请求是1024MB所以可以设置为1024MB，<code>PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024</code>。</p></blockquote><h2 id=华为昇腾llm大模型部署arm架构-麒麟v10系统>华为昇腾LLM大模型部署(ARM架构 麒麟v10系统)</h2><h3 id=准备工作-1>准备工作</h3><h4 id=安装docker>安装docker</h4><p>安装docker，参考链接：<a href=https://little-star.love/posts/6da98871/>https://little-star.love/posts/6da98871/</a></p><p>麒麟v10系统aarch64通过yum安装docker的话会有不少依赖问题，所以这里通过编译好的二进制直接部署安装。</p><p>1.下载docker压缩包，拷贝二进制到<code>/usr/bin/</code>目录下</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># wget -c https://download.docker.com/linux/static/stable/aarch64/docker-20.10.9.tgz</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># tar xf docker-20.10.9.tgz</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># cp -p docker/* /usr/bin/</span>
</span></span></code></pre></div><p>2.添加docker.service systemd启动脚本</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># vim /usr/lib/systemd/system/docker.service</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Unit<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Description</span><span style=color:#ff79c6>=</span>Docker Application Container Engine
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Documentation</span><span style=color:#ff79c6>=</span>https://docs.docker.com
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>After</span><span style=color:#ff79c6>=</span>network-online.target docker.socket firewalld.service containerd.service
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Wants</span><span style=color:#ff79c6>=</span>network-online.target
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Requires</span><span style=color:#ff79c6>=</span>docker.socket containerd.service
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Service<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Type</span><span style=color:#ff79c6>=</span>notify
</span></span><span style=display:flex><span><span style=color:#6272a4># the default is not to use systemd for cgroups because the delegate issues still</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># exists and systemd currently does not support the cgroup feature set required</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># for containers run by docker</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>ExecStart</span><span style=color:#ff79c6>=</span>/usr/bin/dockerd -H fd:// --containerd<span style=color:#ff79c6>=</span>/run/containerd/containerd.sock
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>ExecReload</span><span style=color:#ff79c6>=</span>/bin/kill -s HUP <span style=color:#8be9fd;font-style:italic>$MAINPID</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>TimeoutStartSec</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>RestartSec</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Restart</span><span style=color:#ff79c6>=</span>always
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Note that StartLimit* options were moved from &#34;Service&#34; to &#34;Unit&#34; in systemd 229.</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Both the old, and new location are accepted by systemd 229 and up, so using the old location</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># to make them work for either version of systemd.</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>StartLimitBurst</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Both the old, and new name are accepted by systemd 230 and up, so using the old name to make</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># this option work for either version of systemd.</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>StartLimitInterval</span><span style=color:#ff79c6>=</span>60s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Having non-zero Limit*s causes performance problems due to accounting overhead</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># in the kernel. We recommend using cgroups to do container-local accounting.</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>LimitNOFILE</span><span style=color:#ff79c6>=</span>infinity
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>LimitNPROC</span><span style=color:#ff79c6>=</span>infinity
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>LimitCORE</span><span style=color:#ff79c6>=</span>infinity
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Comment TasksMax if your systemd version does not support it.</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Only systemd 226 and above support this option.</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>TasksMax</span><span style=color:#ff79c6>=</span>infinity
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># set delegate yes so that systemd does not reset the cgroups of docker containers</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Delegate</span><span style=color:#ff79c6>=</span>yes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># kill only the docker process, not all processes in the cgroup</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>KillMode</span><span style=color:#ff79c6>=</span>process
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>OOMScoreAdjust</span><span style=color:#ff79c6>=</span>-500
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Install<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>WantedBy</span><span style=color:#ff79c6>=</span>multi-user.target
</span></span></code></pre></div><p>3.添加docker.socket systemd启动脚本</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># vim /usr/lib/systemd/system/docker.socket</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Unit<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Description</span><span style=color:#ff79c6>=</span>Docker Socket <span style=color:#ff79c6>for</span> the API
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Socket<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># If /var/run is not implemented as a symlink to /run, you may need to</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># specify ListenStream=/var/run/docker.sock instead.</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>ListenStream</span><span style=color:#ff79c6>=</span>/run/docker.sock
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>SocketMode</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>0660</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>SocketUser</span><span style=color:#ff79c6>=</span>root
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>SocketGroup</span><span style=color:#ff79c6>=</span>docker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Install<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>WantedBy</span><span style=color:#ff79c6>=</span>sockets.target
</span></span></code></pre></div><p>4.添加containerd.service systemd启动脚本</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># vim /usr/lib/systemd/system/containerd.service</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Unit<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Description</span><span style=color:#ff79c6>=</span>containerd container runtime
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Documentation</span><span style=color:#ff79c6>=</span>https://containerd.io
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>After</span><span style=color:#ff79c6>=</span>network.target local-fs.target
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Service<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>ExecStartPre</span><span style=color:#ff79c6>=</span>-/sbin/modprobe overlay
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>ExecStart</span><span style=color:#ff79c6>=</span>/usr/bin/containerd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Type</span><span style=color:#ff79c6>=</span>notify
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Delegate</span><span style=color:#ff79c6>=</span>yes
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>KillMode</span><span style=color:#ff79c6>=</span>process
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>Restart</span><span style=color:#ff79c6>=</span>always
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>RestartSec</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>5</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Having non-zero Limit*s causes performance problems due to accounting overhead</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># in the kernel. We recommend using cgroups to do container-local accounting.</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>LimitNPROC</span><span style=color:#ff79c6>=</span>infinity
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>LimitCORE</span><span style=color:#ff79c6>=</span>infinity
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>LimitNOFILE</span><span style=color:#ff79c6>=</span>infinity
</span></span><span style=display:flex><span><span style=color:#6272a4># Comment TasksMax if your systemd version does not supports it.</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Only systemd 226 and above support this version.</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>TasksMax</span><span style=color:#ff79c6>=</span>infinity
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>OOMScoreAdjust</span><span style=color:#ff79c6>=</span>-999
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>Install<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>WantedBy</span><span style=color:#ff79c6>=</span>multi-user.target
</span></span></code></pre></div><p>5.启动docker服务</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># groupadd docker</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># systemctl daemon-reload</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># systemctl start docker</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># systemctl enable docker</span>
</span></span></code></pre></div><p>6.验证docker服务</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># docker info</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Client:
</span></span><span style=display:flex><span> Context:    default
</span></span><span style=display:flex><span> Debug Mode: <span style=color:#8be9fd;font-style:italic>false</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Server:
</span></span><span style=display:flex><span> Containers: <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>  Running: <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>  Paused: <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>  Stopped: <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span> Images: <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span> Server Version: 20.10.9
</span></span><span style=display:flex><span> Storage Driver: overlay2
</span></span><span style=display:flex><span>  Backing Filesystem: xfs
</span></span><span style=display:flex><span>  Supports d_type: <span style=color:#8be9fd;font-style:italic>true</span>
</span></span><span style=display:flex><span>  Native Overlay Diff: <span style=color:#8be9fd;font-style:italic>true</span>
</span></span><span style=display:flex><span>  userxattr: <span style=color:#8be9fd;font-style:italic>false</span>
</span></span><span style=display:flex><span> Logging Driver: json-file
</span></span><span style=display:flex><span> Cgroup Driver: cgroupfs
</span></span><span style=display:flex><span> Cgroup Version: <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span> Plugins:
</span></span><span style=display:flex><span>  Volume: <span style=color:#8be9fd;font-style:italic>local</span>
</span></span><span style=display:flex><span>  Network: bridge host ipvlan macvlan null overlay
</span></span><span style=display:flex><span>  Log: awslogs fluentd gcplogs gelf journald json-file <span style=color:#8be9fd;font-style:italic>local</span> logentries splunk syslog
</span></span><span style=display:flex><span> Swarm: inactive
</span></span><span style=display:flex><span> Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc
</span></span><span style=display:flex><span> Default Runtime: runc
</span></span><span style=display:flex><span> Init Binary: docker-init
</span></span><span style=display:flex><span> containerd version: 5b46e404f6b9f661a205e28d59c982d3634148f8
</span></span><span style=display:flex><span> runc version: v1.0.2-0-g52b36a2d
</span></span><span style=display:flex><span> init version: de40ad0
</span></span><span style=display:flex><span> Security Options:
</span></span><span style=display:flex><span>  seccomp
</span></span><span style=display:flex><span>   Profile: default
</span></span><span style=display:flex><span> Kernel Version: 4.19.90-23.8.v2101.ky10.aarch64
</span></span><span style=display:flex><span> Operating System: Kylin Linux Advanced Server V10 <span style=color:#ff79c6>(</span>Tercel<span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span> OSType: linux
</span></span><span style=display:flex><span> Architecture: aarch64
</span></span><span style=display:flex><span> CPUs: <span style=color:#bd93f9>128</span>
</span></span><span style=display:flex><span> Total Memory: 1.996TiB
</span></span><span style=display:flex><span> Name: localhost.localdomain
</span></span><span style=display:flex><span> ID: PI7U:7E6V:4CJA:YEJP:KXIU:YJ4S:24JA:CS6L:ONVR:FNMO:PCHW:RNJ7
</span></span><span style=display:flex><span> Docker Root Dir: /var/lib/docker
</span></span><span style=display:flex><span> Debug Mode: <span style=color:#8be9fd;font-style:italic>false</span>
</span></span><span style=display:flex><span> Registry: https://index.docker.io/v1/
</span></span><span style=display:flex><span> Labels:
</span></span><span style=display:flex><span> Experimental: <span style=color:#8be9fd;font-style:italic>false</span>
</span></span><span style=display:flex><span> Insecure Registries:
</span></span><span style=display:flex><span>  127.0.0.0/8
</span></span><span style=display:flex><span> Live Restore Enabled: <span style=color:#8be9fd;font-style:italic>false</span>
</span></span><span style=display:flex><span> Product License: Community Engine
</span></span></code></pre></div><p>昇腾适配问题汇总，待补充</p><h1 id=llm大模型应用>LLM大模型应用</h1><h2 id=qanything知识库>QAnything知识库</h2><p>QAnything版本：d5c59260a1af74f01a9b378b567435ab0caa40df</p><h3 id=linux环境离线安装>Linux环境离线安装</h3><p>docker、docker-compose预装好</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># 先在联网机器上下载docker镜像</span>
</span></span><span style=display:flex><span>docker pull quay.io/coreos/etcd:v3.5.5
</span></span><span style=display:flex><span>docker pull minio/minio:RELEASE.2023-03-20T20-16-18Z
</span></span><span style=display:flex><span>docker pull milvusdb/milvus:v2.3.4
</span></span><span style=display:flex><span>docker pull mysql:latest
</span></span><span style=display:flex><span>docker pull freeren/qanything:v1.2.1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 打包镜像</span>
</span></span><span style=display:flex><span>docker save quay.io/coreos/etcd:v3.5.5 minio/minio:RELEASE.2023-03-20T20-16-18Z milvusdb/milvus:v2.3.4 mysql:latest freeren/qanything:v1.2.1 -o qanything_offline.tar
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 下载QAnything代码</span>
</span></span><span style=display:flex><span>wget https://github.com/netease-youdao/QAnything/archive/refs/heads/master.zip
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 把镜像qanything_offline.tar和代码QAnything-master.zip拷贝到断网机器上</span>
</span></span><span style=display:flex><span>cp master.zip qanything_offline.tar /path/to/your/offline/machine
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 在断网机器上加载镜像</span>
</span></span><span style=display:flex><span>docker load -i qanything_offline.tar
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 解压代码，运行</span>
</span></span><span style=display:flex><span>unzip master.zip
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>cd</span> QAnything-master
</span></span><span style=display:flex><span>bash run.sh
</span></span></code></pre></div><p>脚本运行过程中，会产生交互让你输入remote远程服务器安装还是local本地化安装</p><h2 id=langchain-chatchat知识库v03>Langchain-Chatchat知识库(v0.3)</h2><p>Github链接：<a href=https://github.com/chatchat-space/Langchain-Chatchat>https://github.com/chatchat-space/Langchain-Chatchat</a></p><h3 id=搭建源码调试环境>搭建源码调试环境</h3><ol><li>安装python虚拟环境(3.8-3.11，推荐3.11)</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda create --name langchain-chatchat <span style=color:#8be9fd;font-style:italic>python</span><span style=color:#ff79c6>=</span>3.11
</span></span><span style=display:flex><span>conda activate langchain-chatchat
</span></span></code></pre></div><ol start=2><li>克隆代码，安装依赖</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># 拉取仓库</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>(</span>langchain-chatchat<span style=color:#ff79c6>)</span><span style=color:#6272a4># git clone https://github.com/chatchat-space/Langchain-Chatchat.git</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 进入目录</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>(</span>langchain-chatchat<span style=color:#ff79c6>)</span><span style=color:#6272a4># cd Langchain-Chatchat</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>(</span>langchain-chatchat<span style=color:#ff79c6>)</span><span style=color:#6272a4># pip install langchain-chatchat -U  # 0.3.0版本起，Langchain-Chatchat提供以Python库形式的安装方式</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>(</span>langchain-chatchat<span style=color:#ff79c6>)</span><span style=color:#6272a4># pip install &#34;langchain-chatchat[xinference]&#34; -U # 接入部署框架xinference</span>
</span></span></code></pre></div><ol start=3><li>设置接入模型参数</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>(</span>langchain-chatchat<span style=color:#ff79c6>)</span><span style=color:#6272a4># chatchat-config model --set_model_platforms &#34;[{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>\&#34;</span>platform_name<span style=color:#f1fa8c>\&#34;</span>: <span style=color:#f1fa8c>\&#34;</span>xinference<span style=color:#f1fa8c>\&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>\&#34;</span>platform_type<span style=color:#f1fa8c>\&#34;</span>: <span style=color:#f1fa8c>\&#34;</span>xinference<span style=color:#f1fa8c>\&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>\&#34;</span>api_base_url<span style=color:#f1fa8c>\&#34;</span>: <span style=color:#f1fa8c>\&#34;</span>http://&lt;xinference_ip&gt;:9997/v1<span style=color:#f1fa8c>\&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>\&#34;</span>api_key<span style=color:#f1fa8c>\&#34;</span>: <span style=color:#f1fa8c>\&#34;</span>EMPT<span style=color:#f1fa8c>\&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>\&#34;</span>api_concurrencies<span style=color:#f1fa8c>\&#34;</span>: 5,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>\&#34;</span>llm_models<span style=color:#f1fa8c>\&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>\&#34;</span>qwen1.5-chat<span style=color:#f1fa8c>\&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>\&#34;</span>embed_models<span style=color:#f1fa8c>\&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>\&#34;</span>bge-large-zh-v1.5<span style=color:#f1fa8c>\&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>\&#34;</span>image_models<span style=color:#f1fa8c>\&#34;</span>: <span style=color:#ff79c6>[]</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>\&#34;</span>reranking_models<span style=color:#f1fa8c>\&#34;</span>: <span style=color:#ff79c6>[]</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>\&#34;</span>speech2text_models<span style=color:#f1fa8c>\&#34;</span>: <span style=color:#ff79c6>[]</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>\&#34;</span>tts_models<span style=color:#f1fa8c>\&#34;</span>: <span style=color:#ff79c6>[]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}]</span><span style=color:#f1fa8c>&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>(langchain-chatchat)# chatchat-config model --default_llm_model qwen1.5-chat
</span></span></span></code></pre></div><p>设置的参数作用于这个文件<code>～/.chatchat/workspace/workspace_config.json</code></p><ol start=4><li>编辑server配置文件，DEFAULT_BIND_HOST设置为0.0.0.0</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>(</span>langchain-chatchat<span style=color:#ff79c6>)</span><span style=color:#6272a4># chatchat-config server --default_bind_host 0.0.0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 查看server配置是否监听在0.0.0.0上</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>(</span>langchain-chatchat<span style=color:#ff79c6>)</span><span style=color:#6272a4># chatchat-config server --show</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;HTTPX_DEFAULT_TIMEOUT&#34;</span>: 300.0,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;OPEN_CROSS_DOMAIN&#34;</span>: true,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;DEFAULT_BIND_HOST&#34;</span>: <span style=color:#f1fa8c>&#34;0.0.0.0&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;WEBUI_SERVER_PORT&#34;</span>: 8501,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;API_SERVER_PORT&#34;</span>: 7861,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;WEBUI_SERVER&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;host&#34;</span>: <span style=color:#f1fa8c>&#34;0.0.0.0&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;port&#34;</span>: <span style=color:#bd93f9>8501</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;API_SERVER&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;host&#34;</span>: <span style=color:#f1fa8c>&#34;0.0.0.0&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;port&#34;</span>: <span style=color:#bd93f9>7861</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;class_name&#34;</span>: <span style=color:#f1fa8c>&#34;ConfigServer&#34;</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span></code></pre></div><ol start=5><li>初始化数据库</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>(</span>langchain-chatchat<span style=color:#ff79c6>)</span><span style=color:#6272a4># cd libs/chatchat-server/chatchat</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>(</span>langchain-chatchat<span style=color:#ff79c6>)</span><span style=color:#6272a4># python init_database.py --recreate-vs</span>
</span></span></code></pre></div><ol start=6><li>启动服务</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>(</span>langchain-chatchat<span style=color:#ff79c6>)</span><span style=color:#6272a4># python startup.py -a</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 启动远程debug监听模式</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>(</span>langchain-chatchat<span style=color:#ff79c6>)</span><span style=color:#6272a4># python3 -m debugpy --listen 0.0.0.0:5678 --wait-for-client</span>
</span></span></code></pre></div><h3 id=源码分析结合ai编程助手辅助>源码分析(结合AI编程助手辅助)</h3><h4 id=启动函数>启动函数</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># chatchat-space/Langchain-Chatchat/startup.py</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    程序的主入口点。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    本函数初始化运行环境，包括调整当前工作目录、支持多进程、创建数据库表和启动异步事件循环。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    它为运行聊天服务器做了必要的准备。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 获取并记录当前工作目录，用于后续将该目录添加到系统路径中</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 添加这行代码</span>
</span></span><span style=display:flex><span>    cwd <span style=color:#ff79c6>=</span> os<span style=color:#ff79c6>.</span>getcwd()
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 将当前工作目录添加到系统路径中，以便能够找到本地模块</span>
</span></span><span style=display:flex><span>    sys<span style=color:#ff79c6>.</span>path<span style=color:#ff79c6>.</span>append(cwd)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 对于Windows平台，支持将当前程序打包为可执行文件</span>
</span></span><span style=display:flex><span>    multiprocessing<span style=color:#ff79c6>.</span>freeze_support()
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 输出当前工作目录，用于调试</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;cwd:&#34;</span> <span style=color:#ff79c6>+</span> cwd)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 导入模块并创建数据库表，用于存储知识库数据</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>from</span> chatchat.server.knowledge_base.migrate <span style=color:#ff79c6>import</span> create_tables
</span></span><span style=display:flex><span>    create_tables()
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 根据Python版本选择合适的异步事件循环方式</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> sys<span style=color:#ff79c6>.</span>version_info <span style=color:#ff79c6>&lt;</span> (<span style=color:#bd93f9>3</span>, <span style=color:#bd93f9>10</span>):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 在Python 3.10及以下版本，直接获取或创建一个新的事件循环</span>
</span></span><span style=display:flex><span>        loop <span style=color:#ff79c6>=</span> asyncio<span style=color:#ff79c6>.</span>get_event_loop()
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 在Python 3.10及以上版本，尝试获取当前正在运行的事件循环</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>try</span>:
</span></span><span style=display:flex><span>            loop <span style=color:#ff79c6>=</span> asyncio<span style=color:#ff79c6>.</span>get_running_loop()
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>except</span> RuntimeError:
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 如果当前没有运行的事件循环，则创建一个新的</span>
</span></span><span style=display:flex><span>            loop <span style=color:#ff79c6>=</span> asyncio<span style=color:#ff79c6>.</span>new_event_loop()
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 设置当前线程的事件循环</span>
</span></span><span style=display:flex><span>        asyncio<span style=color:#ff79c6>.</span>set_event_loop(loop)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 运行主服务器的异步任务</span>
</span></span><span style=display:flex><span>    loop<span style=color:#ff79c6>.</span>run_until_complete(start_main_server())
</span></span></code></pre></div><p>主程序</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 异步启动主服务器函数</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>async</span> <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>start_main_server</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 导入信号处理和时间模块</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>import</span> signal
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 导入配置和日志工具</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>from</span> chatchat.utils <span style=color:#ff79c6>import</span> (
</span></span><span style=display:flex><span>        get_config_dict,
</span></span><span style=display:flex><span>        get_log_file,
</span></span><span style=display:flex><span>        get_timestamp_ms,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>from</span> chatchat.configs <span style=color:#ff79c6>import</span> LOG_PATH
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 设置日志配置</span>
</span></span><span style=display:flex><span>    logging_conf <span style=color:#ff79c6>=</span> get_config_dict(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;INFO&#34;</span>,
</span></span><span style=display:flex><span>        get_log_file(
</span></span><span style=display:flex><span>            log_path<span style=color:#ff79c6>=</span>LOG_PATH, sub_dir<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;start_main_server_</span><span style=color:#f1fa8c>{</span>get_timestamp_ms()<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 设置日志文件备份数量上限</span>
</span></span><span style=display:flex><span>        <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>3</span>,
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 设置最大日志文件大小</span>
</span></span><span style=display:flex><span>        <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>3</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    logging<span style=color:#ff79c6>.</span>config<span style=color:#ff79c6>.</span>dictConfig(logging_conf)  <span style=color:#6272a4># type: ignore</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 定义信号处理函数</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>handler</span>(signalname):
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        Python 3.9 has `signal.strsignal(signalnum)` so this closure would not be needed.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        Also, 3.8 includes `signal.valid_signals()` that can be used to create a mapping for the same purpose.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>f</span>(signal_received, frame):
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>raise</span> KeyboardInterrupt(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span>signalname<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c> received&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> f
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 注册信号处理程序，用于优雅地关闭进程</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># This will be inherited by the child process if it is forked (not spawned)</span>
</span></span><span style=display:flex><span>    signal<span style=color:#ff79c6>.</span>signal(signal<span style=color:#ff79c6>.</span>SIGINT, handler(<span style=color:#f1fa8c>&#34;SIGINT&#34;</span>))
</span></span><span style=display:flex><span>    signal<span style=color:#ff79c6>.</span>signal(signal<span style=color:#ff79c6>.</span>SIGTERM, handler(<span style=color:#f1fa8c>&#34;SIGTERM&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 设置多进程启动方法</span>
</span></span><span style=display:flex><span>    mp<span style=color:#ff79c6>.</span>set_start_method(<span style=color:#f1fa8c>&#34;spawn&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化多进程管理器</span>
</span></span><span style=display:flex><span>    manager <span style=color:#ff79c6>=</span> mp<span style=color:#ff79c6>.</span>Manager()
</span></span><span style=display:flex><span>    run_mode <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 解析命令行参数</span>
</span></span><span style=display:flex><span>    args, parser <span style=color:#ff79c6>=</span> parse_args()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 根据命令行参数配置服务启动选项</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> args<span style=color:#ff79c6>.</span>all_webui:
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>api <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>api_worker <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>webui <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>elif</span> args<span style=color:#ff79c6>.</span>all_api:
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>api <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>api_worker <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>webui <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>elif</span> args<span style=color:#ff79c6>.</span>api:
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>api <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>api_worker <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>webui <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> args<span style=color:#ff79c6>.</span>lite:
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>api <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>api_worker <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>        args<span style=color:#ff79c6>.</span>webui <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 输出服务器配置信息</span>
</span></span><span style=display:flex><span>    dump_server_info(args<span style=color:#ff79c6>=</span>args)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 如果有命令行参数，输出启动日志</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>len</span>(sys<span style=color:#ff79c6>.</span>argv) <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>1</span>:
</span></span><span style=display:flex><span>        logger<span style=color:#ff79c6>.</span>info(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;正在启动服务：&#34;</span>)
</span></span><span style=display:flex><span>        logger<span style=color:#ff79c6>.</span>info(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;如需查看 llm_api 日志，请前往 </span><span style=color:#f1fa8c>{</span>LOG_PATH<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化进程字典</span>
</span></span><span style=display:flex><span>    processes <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 定义计数器函数，用于统计启动的进程数量</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>process_count</span>():
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#8be9fd;font-style:italic>len</span>(processes)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化API服务器启动事件</span>
</span></span><span style=display:flex><span>    api_started <span style=color:#ff79c6>=</span> manager<span style=color:#ff79c6>.</span>Event()
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 根据参数决定是否启动API服务器</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> args<span style=color:#ff79c6>.</span>api:
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 创建一个进行，目标是运行API服务</span>
</span></span><span style=display:flex><span>        process <span style=color:#ff79c6>=</span> Process(
</span></span><span style=display:flex><span>            target<span style=color:#ff79c6>=</span>run_api_server,
</span></span><span style=display:flex><span>            name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;API Server&#34;</span>,
</span></span><span style=display:flex><span>            kwargs<span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>dict</span>(
</span></span><span style=display:flex><span>                started_event<span style=color:#ff79c6>=</span>api_started,
</span></span><span style=display:flex><span>                run_mode<span style=color:#ff79c6>=</span>run_mode,
</span></span><span style=display:flex><span>            ),
</span></span><span style=display:flex><span>            daemon<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        processes[<span style=color:#f1fa8c>&#34;api&#34;</span>] <span style=color:#ff79c6>=</span> process
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化WebUI服务器启动事件</span>
</span></span><span style=display:flex><span>    webui_started <span style=color:#ff79c6>=</span> manager<span style=color:#ff79c6>.</span>Event()
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 根据参数决定是否启动WebUI服务器</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> args<span style=color:#ff79c6>.</span>webui:
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 创建一个进行，目标是运行WebUI服务</span>
</span></span><span style=display:flex><span>        process <span style=color:#ff79c6>=</span> Process(
</span></span><span style=display:flex><span>            target<span style=color:#ff79c6>=</span>run_webui,
</span></span><span style=display:flex><span>            name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;WEBUI Server&#34;</span>,
</span></span><span style=display:flex><span>            kwargs<span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>dict</span>(
</span></span><span style=display:flex><span>                started_event<span style=color:#ff79c6>=</span>webui_started,
</span></span><span style=display:flex><span>                run_mode<span style=color:#ff79c6>=</span>run_mode,
</span></span><span style=display:flex><span>            ),
</span></span><span style=display:flex><span>            daemon<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        processes[<span style=color:#f1fa8c>&#34;webui&#34;</span>] <span style=color:#ff79c6>=</span> process
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 如果没有要启动的进程，打印帮助信息</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> process_count() <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>:
</span></span><span style=display:flex><span>        parser<span style=color:#ff79c6>.</span>print_help()
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>try</span>:
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 启动API服务器并等待其启动完成</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> p <span style=color:#ff79c6>:=</span> processes<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;api&#34;</span>):
</span></span><span style=display:flex><span>                p<span style=color:#ff79c6>.</span>start()
</span></span><span style=display:flex><span>                p<span style=color:#ff79c6>.</span>name <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span>p<span style=color:#ff79c6>.</span>name<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c> (</span><span style=color:#f1fa8c>{</span>p<span style=color:#ff79c6>.</span>pid<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>)&#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#6272a4># 等待api.py启动完成</span>
</span></span><span style=display:flex><span>                api_started<span style=color:#ff79c6>.</span>wait()  
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 启动WebUI服务器并等待其启动完成</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> p <span style=color:#ff79c6>:=</span> processes<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;webui&#34;</span>):
</span></span><span style=display:flex><span>                p<span style=color:#ff79c6>.</span>start()
</span></span><span style=display:flex><span>                p<span style=color:#ff79c6>.</span>name <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span>p<span style=color:#ff79c6>.</span>name<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c> (</span><span style=color:#f1fa8c>{</span>p<span style=color:#ff79c6>.</span>pid<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>)&#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#6272a4># 等待webui.py启动完成</span>
</span></span><span style=display:flex><span>                webui_started<span style=color:#ff79c6>.</span>wait()  
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 输出启动后的服务器信息</span>
</span></span><span style=display:flex><span>            dump_server_info(after_start<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, args<span style=color:#ff79c6>=</span>args)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 监控进程运行，等待进程结束</span>
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 等待所有进程退出</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>while</span> processes:
</span></span><span style=display:flex><span>                <span style=color:#6272a4># 遍历processes字典的值</span>
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>for</span> p <span style=color:#ff79c6>in</span> processes<span style=color:#ff79c6>.</span>values():
</span></span><span style=display:flex><span>                    <span style=color:#6272a4># 调用join(2)方法等待每个进程运行2秒钟</span>
</span></span><span style=display:flex><span>                    p<span style=color:#ff79c6>.</span>join(<span style=color:#bd93f9>2</span>)
</span></span><span style=display:flex><span>                    <span style=color:#6272a4># 如果进程不再存活，则通过pop方法从字典中移除该进程</span>
</span></span><span style=display:flex><span>                    <span style=color:#ff79c6>if</span> <span style=color:#ff79c6>not</span> p<span style=color:#ff79c6>.</span>is_alive():
</span></span><span style=display:flex><span>                        processes<span style=color:#ff79c6>.</span>pop(p<span style=color:#ff79c6>.</span>name)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>except</span> Exception <span style=color:#ff79c6>as</span> e:
</span></span><span style=display:flex><span>            logger<span style=color:#ff79c6>.</span>error(e)
</span></span><span style=display:flex><span>            logger<span style=color:#ff79c6>.</span>warning(<span style=color:#f1fa8c>&#34;Caught KeyboardInterrupt! Setting stop event...&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>finally</span>:
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 关闭所有进程</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>for</span> p <span style=color:#ff79c6>in</span> processes<span style=color:#ff79c6>.</span>values():
</span></span><span style=display:flex><span>                logger<span style=color:#ff79c6>.</span>warning(<span style=color:#f1fa8c>&#34;Sending SIGKILL to </span><span style=color:#f1fa8c>%s</span><span style=color:#f1fa8c>&#34;</span>, p)
</span></span><span style=display:flex><span>                <span style=color:#6272a4># Queues and other inter-process communication primitives can break when</span>
</span></span><span style=display:flex><span>                <span style=color:#6272a4># process is killed, but we don&#39;t care here</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#6272a4># 如果进程对象是一个字典，遍历其值并杀死所有包含的进程</span>
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>isinstance</span>(p, <span style=color:#8be9fd;font-style:italic>dict</span>):
</span></span><span style=display:flex><span>                    <span style=color:#ff79c6>for</span> process <span style=color:#ff79c6>in</span> p<span style=color:#ff79c6>.</span>values():
</span></span><span style=display:flex><span>                        process<span style=color:#ff79c6>.</span>kill()
</span></span><span style=display:flex><span>                <span style=color:#6272a4># 如果进程对象不是一个字典，直接杀死该进程</span>
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>                    p<span style=color:#ff79c6>.</span>kill()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 再次遍历所有进程对象，检查并记录它们的最终状态</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>for</span> p <span style=color:#ff79c6>in</span> processes<span style=color:#ff79c6>.</span>values():
</span></span><span style=display:flex><span>                logger<span style=color:#ff79c6>.</span>info(<span style=color:#f1fa8c>&#34;Process status: </span><span style=color:#f1fa8c>%s</span><span style=color:#f1fa8c>&#34;</span>, p)
</span></span></code></pre></div><h4 id=核心api服务>核心API服务</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>run_api_server</span>(
</span></span><span style=display:flex><span>    started_event: mp<span style=color:#ff79c6>.</span>Event <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>, run_mode: <span style=color:#8be9fd;font-style:italic>str</span> <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    启动API服务器。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    使用Uvicorn运行ASGI应用程序。此函数配置并启动API服务器，它负责处理来自客户端的请求。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    参数:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - started_event: 一个multiprocessing.Event对象，用于在API服务器启动后通知其他进程。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - run_mode: API服务器的运行模式，用于配置应用程序的行为。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 导入必要的模块和包</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>import</span> uvicorn
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>from</span> chatchat.utils <span style=color:#ff79c6>import</span> (
</span></span><span style=display:flex><span>        get_config_dict,
</span></span><span style=display:flex><span>        get_log_file,
</span></span><span style=display:flex><span>        get_timestamp_ms,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 从配置文件中获取API服务器的配置信息</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>from</span> chatchat.configs <span style=color:#ff79c6>import</span> API_SERVER, LOG_PATH, MODEL_PLATFORMS
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 导入创建应用程序的函数</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>from</span> chatchat.server.api_server.server_app <span style=color:#ff79c6>import</span> create_app
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 导入配置HTTPX的函数</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>from</span> chatchat.server.utils <span style=color:#ff79c6>import</span> set_httpx_config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化日志记录器</span>
</span></span><span style=display:flex><span>    logger<span style=color:#ff79c6>.</span>info(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Api MODEL_PLATFORMS: </span><span style=color:#f1fa8c>{</span>MODEL_PLATFORMS<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 配置HTTPX客户端的设置，HTTPX是高性能的http客户端库</span>
</span></span><span style=display:flex><span>    set_httpx_config()
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 根据运行模式创建应用程序实例</span>
</span></span><span style=display:flex><span>    app <span style=color:#ff79c6>=</span> create_app(run_mode<span style=color:#ff79c6>=</span>run_mode)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 设置应用程序事件，如服务器启动事件</span>
</span></span><span style=display:flex><span>    _set_app_event(app, started_event)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 从配置中获取API服务器的主机和端口</span>
</span></span><span style=display:flex><span>    host <span style=color:#ff79c6>=</span> API_SERVER[<span style=color:#f1fa8c>&#34;host&#34;</span>]
</span></span><span style=display:flex><span>    port <span style=color:#ff79c6>=</span> API_SERVER[<span style=color:#f1fa8c>&#34;port&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 配置日志系统，包括日志等级、文件路径、最大文件大小和备份数量</span>
</span></span><span style=display:flex><span>    logging_conf <span style=color:#ff79c6>=</span> get_config_dict(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;INFO&#34;</span>,
</span></span><span style=display:flex><span>        get_log_file(log_path<span style=color:#ff79c6>=</span>LOG_PATH, sub_dir<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;run_api_server_</span><span style=color:#f1fa8c>{</span>get_timestamp_ms()<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>),
</span></span><span style=display:flex><span>        <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>3</span>,
</span></span><span style=display:flex><span>        <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>3</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 应用日志配置</span>
</span></span><span style=display:flex><span>    logging<span style=color:#ff79c6>.</span>config<span style=color:#ff79c6>.</span>dictConfig(logging_conf)  <span style=color:#6272a4># type: ignore</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 启动Uvicorn服务器，运行API应用程序</span>
</span></span><span style=display:flex><span>    uvicorn<span style=color:#ff79c6>.</span>run(app, host<span style=color:#ff79c6>=</span>host, port<span style=color:#ff79c6>=</span>port)
</span></span></code></pre></div><p>根据运行模式创建应用程序实例</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>create_app</span>(run_mode: <span style=color:#8be9fd;font-style:italic>str</span> <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    创建并配置FastAPI应用程序实例。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :param run_mode: 应用程序的运行模式，用于配置不同的环境设置，默认为None。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    :return: 配置好的FastAPI应用程序实例。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化FastAPI应用程序，并设置标题和版本</span>
</span></span><span style=display:flex><span>    app <span style=color:#ff79c6>=</span> FastAPI(title<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;Langchain-Chatchat API Server&#34;</span>, version<span style=color:#ff79c6>=</span>VERSION)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 离线模式配置，使应用在离线环境下仍能访问API文档</span>
</span></span><span style=display:flex><span>    MakeFastAPIOffline(app)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Add CORS middleware to allow all origins</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 在config.py中设置OPEN_DOMAIN=True，允许跨域</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># set OPEN_DOMAIN=True in config.py to allow cross-domain</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> OPEN_CROSS_DOMAIN:
</span></span><span style=display:flex><span>        app<span style=color:#ff79c6>.</span>add_middleware(
</span></span><span style=display:flex><span>            CORSMiddleware,
</span></span><span style=display:flex><span>            allow_origins<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;*&#34;</span>],
</span></span><span style=display:flex><span>            allow_credentials<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>            allow_methods<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;*&#34;</span>],
</span></span><span style=display:flex><span>            allow_headers<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;*&#34;</span>],
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 配置应用程序的根路由，重定向到文档页面</span>
</span></span><span style=display:flex><span>    @app.get(<span style=color:#f1fa8c>&#34;/&#34;</span>, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;swagger 文档&#34;</span>, include_in_schema<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>async</span> <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>document</span>():
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> RedirectResponse(url<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;/docs&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 注册各个功能模块的路由</span>
</span></span><span style=display:flex><span>    app<span style=color:#ff79c6>.</span>include_router(chat_router)
</span></span><span style=display:flex><span>    app<span style=color:#ff79c6>.</span>include_router(kb_router)
</span></span><span style=display:flex><span>    app<span style=color:#ff79c6>.</span>include_router(tool_router)
</span></span><span style=display:flex><span>    app<span style=color:#ff79c6>.</span>include_router(openai_router)
</span></span><span style=display:flex><span>    app<span style=color:#ff79c6>.</span>include_router(server_router)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 注册额外的接口路由</span>
</span></span><span style=display:flex><span>    app<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;/other/completion&#34;</span>,
</span></span><span style=display:flex><span>        tags<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;Other&#34;</span>],
</span></span><span style=display:flex><span>        summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;要求llm模型补全(通过LLMChain)&#34;</span>,
</span></span><span style=display:flex><span>    )(completion)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 配置静态文件路由，提供媒体文件访问</span>
</span></span><span style=display:flex><span>    app<span style=color:#ff79c6>.</span>mount(<span style=color:#f1fa8c>&#34;/media&#34;</span>, StaticFiles(directory<span style=color:#ff79c6>=</span>MEDIA_PATH), name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;media&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 配置项目相关图片的静态文件路由</span>
</span></span><span style=display:flex><span>    img_dir <span style=color:#ff79c6>=</span> os<span style=color:#ff79c6>.</span>path<span style=color:#ff79c6>.</span>join(CHATCHAT_ROOT, <span style=color:#f1fa8c>&#34;img&#34;</span>)
</span></span><span style=display:flex><span>    app<span style=color:#ff79c6>.</span>mount(<span style=color:#f1fa8c>&#34;/img&#34;</span>, StaticFiles(directory<span style=color:#ff79c6>=</span>img_dir), name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;img&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> app
</span></span></code></pre></div><h4 id=数据库连接会话>数据库连接会话</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> contextlib <span style=color:#ff79c6>import</span> contextmanager
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> functools <span style=color:#ff79c6>import</span> wraps
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> sqlalchemy.orm <span style=color:#ff79c6>import</span> Session
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> chatchat.server.db.base <span style=color:#ff79c6>import</span> SessionLocal
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># contextmanager可以简化yield语句来标记__enter__()和__exit__()之间的代码段</span>
</span></span><span style=display:flex><span>@contextmanager
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>session_scope</span>() <span style=color:#ff79c6>-&gt;</span> Session:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    提供一个事务性的Session上下文管理器。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    在with语句中使用时，会自动处理Session的开启、提交、回滚和关闭。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    这确保了数据库操作被正确地包含在事务中，提高了数据的一致性和完整性。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    session <span style=color:#ff79c6>=</span> SessionLocal()
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>try</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>yield</span> session
</span></span><span style=display:flex><span>        session<span style=color:#ff79c6>.</span>commit()
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>except</span>:
</span></span><span style=display:flex><span>        session<span style=color:#ff79c6>.</span>rollback()
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>raise</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>finally</span>:
</span></span><span style=display:flex><span>        session<span style=color:#ff79c6>.</span>close()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>with_session</span>(f):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    函数装饰器，用于自动管理数据库会话的生命周期。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    被装饰的函数将在一个有效的数据库会话上下文中执行。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    如果函数执行过程中发生异常，会回滚会话；否则，会提交会话。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    这简化了数据库操作的事务管理。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    @wraps(f)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>wrapper</span>(<span style=color:#ff79c6>*</span>args, <span style=color:#ff79c6>**</span>kwargs):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>with</span> session_scope() <span style=color:#ff79c6>as</span> session:
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>try</span>:
</span></span><span style=display:flex><span>                result <span style=color:#ff79c6>=</span> f(session, <span style=color:#ff79c6>*</span>args, <span style=color:#ff79c6>**</span>kwargs)
</span></span><span style=display:flex><span>                session<span style=color:#ff79c6>.</span>commit()
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>return</span> result
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>except</span>:
</span></span><span style=display:flex><span>                session<span style=color:#ff79c6>.</span>rollback()
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>raise</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> wrapper
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>get_db</span>() <span style=color:#ff79c6>-&gt;</span> SessionLocal:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    提供一个数据库会话的生成器。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    在with语句中使用时，可以确保会话在使用后正确关闭。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    这是一种资源管理的常见模式，用于避免资源泄露。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    db <span style=color:#ff79c6>=</span> SessionLocal()
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>try</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>yield</span> db
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>finally</span>:
</span></span><span style=display:flex><span>        db<span style=color:#ff79c6>.</span>close()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>get_db0</span>() <span style=color:#ff79c6>-&gt;</span> SessionLocal:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    获取一个数据库会话实例。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    与get_db()不同，这个函数直接返回一个会话实例，而不是一个生成器。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    使用者需要显式地管理会话的生命周期，确保在不需要时关闭会话。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    db <span style=color:#ff79c6>=</span> SessionLocal()
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> db
</span></span></code></pre></div><h4 id=chat聊天功能模块>chat聊天功能模块</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 初始化聊天路由，用于处理与聊天相关的API请求</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 使用前缀&#34;/chat&#34;和标签&#34;ChatChat 对话&#34;来标识这个路由的目的和功能</span>
</span></span><span style=display:flex><span>chat_router <span style=color:#ff79c6>=</span> APIRouter(prefix<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;/chat&#34;</span>, tags<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;ChatChat 对话&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义一个API端点，用于发起与llm模型的对话</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 请求方法为POST，路径为&#34;/chat&#34;</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 通过LLMChain进行模型对话</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 请求摘要说明了这个端点的主要功能</span>
</span></span><span style=display:flex><span>chat_router<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/chat&#34;</span>,
</span></span><span style=display:flex><span>    summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;与llm模型对话(通过LLMChain)&#34;</span>,
</span></span><span style=display:flex><span>)(chat)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义一个API端点，用于提交对llm模型对话的反馈</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 请求方法为POST，路径为&#34;/feedback&#34;</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 请求摘要说明了这个端点的主要功能</span>
</span></span><span style=display:flex><span>chat_router<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/feedback&#34;</span>,
</span></span><span style=display:flex><span>    summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;返回llm模型对话评分&#34;</span>,
</span></span><span style=display:flex><span>)(chat_feedback)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义一个API端点，用于处理基于文件的对话请求</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 请求方法为POST，路径为&#34;/file_chat&#34;</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 请求摘要说明了这个端点的主要功能</span>
</span></span><span style=display:flex><span>chat_router<span style=color:#ff79c6>.</span>post(<span style=color:#f1fa8c>&#34;/file_chat&#34;</span>, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;文件对话&#34;</span>)(file_chat)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 异步函数，处理聊天完成请求</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 参数 request: 请求对象</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 参数 body: 包含聊天输入信息的OpenAIChatInput对象</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 返回一个字典，与OpenAI的聊天完成响应兼容</span>
</span></span><span style=display:flex><span>@chat_router.post(<span style=color:#f1fa8c>&#34;/chat/completions&#34;</span>, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;兼容 openai 的统一 chat 接口&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#ff79c6>async</span> <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>chat_completions</span>(
</span></span><span style=display:flex><span>    request: Request,
</span></span><span style=display:flex><span>    body: OpenAIChatInput,
</span></span><span style=display:flex><span>) <span style=color:#ff79c6>-&gt;</span> Dict:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    请求参数与 openai.chat.completions.create 一致，可以通过 extra_body 传入额外参数
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    tools 和 tool_choice 可以直接传工具名称，会根据项目里包含的 tools 进行转换
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    通过不同的参数组合调用不同的 chat 功能：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - tool_choice
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        - extra_body 中包含 tool_input: 直接调用 tool_choice(tool_input)
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        - extra_body 中不包含 tool_input: 通过 agent 调用 tool_choice
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - tools: agent 对话
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - 其它：LLM 对话
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    以后还要考虑其它的组合（如文件对话）
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    返回与 openai 兼容的 Dict
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 创建OpenAI客户端，来源支持xinference、ollama、fastchat、localAI等，用于后续的API调用</span>
</span></span><span style=display:flex><span>    client <span style=color:#ff79c6>=</span> get_OpenAIClient(model_name<span style=color:#ff79c6>=</span>body<span style=color:#ff79c6>.</span>model, is_async<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 复制model_extra字段到extra字典，并从body中删除这些属性</span>
</span></span><span style=display:flex><span>    extra <span style=color:#ff79c6>=</span> {<span style=color:#ff79c6>**</span>body<span style=color:#ff79c6>.</span>model_extra} <span style=color:#ff79c6>or</span> {}
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> key <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>list</span>(extra):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>delattr</span>(body, key)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 设置全局模型名称</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>global</span> global_model_name
</span></span><span style=display:flex><span>    global_model_name <span style=color:#ff79c6>=</span> body<span style=color:#ff79c6>.</span>model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 处理body中的tool_choice和tools字段，将工具名称转换为工具配置</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>isinstance</span>(body<span style=color:#ff79c6>.</span>tool_choice, <span style=color:#8be9fd;font-style:italic>str</span>):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> t <span style=color:#ff79c6>:=</span> get_tool(body<span style=color:#ff79c6>.</span>tool_choice):
</span></span><span style=display:flex><span>            body<span style=color:#ff79c6>.</span>tool_choice <span style=color:#ff79c6>=</span> {<span style=color:#f1fa8c>&#34;function&#34;</span>: {<span style=color:#f1fa8c>&#34;name&#34;</span>: t<span style=color:#ff79c6>.</span>name}, <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;function&#34;</span>}
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>isinstance</span>(body<span style=color:#ff79c6>.</span>tools, <span style=color:#8be9fd;font-style:italic>list</span>):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(body<span style=color:#ff79c6>.</span>tools)):
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>isinstance</span>(body<span style=color:#ff79c6>.</span>tools[i], <span style=color:#8be9fd;font-style:italic>str</span>):
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>if</span> t <span style=color:#ff79c6>:=</span> get_tool(body<span style=color:#ff79c6>.</span>tools[i]):
</span></span><span style=display:flex><span>                    body<span style=color:#ff79c6>.</span>tools[i] <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>                        <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;function&#34;</span>,
</span></span><span style=display:flex><span>                        <span style=color:#f1fa8c>&#34;function&#34;</span>: {
</span></span><span style=display:flex><span>                            <span style=color:#f1fa8c>&#34;name&#34;</span>: t<span style=color:#ff79c6>.</span>name,
</span></span><span style=display:flex><span>                            <span style=color:#f1fa8c>&#34;description&#34;</span>: t<span style=color:#ff79c6>.</span>description,
</span></span><span style=display:flex><span>                            <span style=color:#f1fa8c>&#34;parameters&#34;</span>: t<span style=color:#ff79c6>.</span>args,
</span></span><span style=display:flex><span>                        },
</span></span><span style=display:flex><span>                    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 获取对话ID，用于后续的消息记录</span>
</span></span><span style=display:flex><span>    conversation_id <span style=color:#ff79c6>=</span> extra<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;conversation_id&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 如果指定了tool_choice，调用相应的工具处理函数</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># chat based on result from one choiced tool</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> body<span style=color:#ff79c6>.</span>tool_choice:
</span></span><span style=display:flex><span>        tool <span style=color:#ff79c6>=</span> get_tool(body<span style=color:#ff79c6>.</span>tool_choice[<span style=color:#f1fa8c>&#34;function&#34;</span>][<span style=color:#f1fa8c>&#34;name&#34;</span>])
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 如果没有指定其他工具，添加当前工具到tools列表</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> <span style=color:#ff79c6>not</span> body<span style=color:#ff79c6>.</span>tools:
</span></span><span style=display:flex><span>            body<span style=color:#ff79c6>.</span>tools <span style=color:#ff79c6>=</span> [
</span></span><span style=display:flex><span>                {
</span></span><span style=display:flex><span>                    <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;function&#34;</span>,
</span></span><span style=display:flex><span>                    <span style=color:#f1fa8c>&#34;function&#34;</span>: {
</span></span><span style=display:flex><span>                        <span style=color:#f1fa8c>&#34;name&#34;</span>: tool<span style=color:#ff79c6>.</span>name,
</span></span><span style=display:flex><span>                        <span style=color:#f1fa8c>&#34;description&#34;</span>: tool<span style=color:#ff79c6>.</span>description,
</span></span><span style=display:flex><span>                        <span style=color:#f1fa8c>&#34;parameters&#34;</span>: tool<span style=color:#ff79c6>.</span>args,
</span></span><span style=display:flex><span>                    },
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 如果提供了tool_input，记录消息并调用工具处理函数</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> tool_input <span style=color:#ff79c6>:=</span> extra<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;tool_input&#34;</span>):
</span></span><span style=display:flex><span>            message_id <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>                add_message_to_db(
</span></span><span style=display:flex><span>                    chat_type<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;tool_call&#34;</span>,
</span></span><span style=display:flex><span>                    query<span style=color:#ff79c6>=</span>body<span style=color:#ff79c6>.</span>messages[<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>][<span style=color:#f1fa8c>&#34;content&#34;</span>],
</span></span><span style=display:flex><span>                    conversation_id<span style=color:#ff79c6>=</span>conversation_id,
</span></span><span style=display:flex><span>                )
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>if</span> conversation_id
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>else</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            tool_result <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>await</span> tool<span style=color:#ff79c6>.</span>ainvoke(tool_input)
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 使用模板生成最终的回复内容</span>
</span></span><span style=display:flex><span>            prompt_template <span style=color:#ff79c6>=</span> PromptTemplate<span style=color:#ff79c6>.</span>from_template(
</span></span><span style=display:flex><span>                get_prompt_template(<span style=color:#f1fa8c>&#34;llm_model&#34;</span>, <span style=color:#f1fa8c>&#34;rag&#34;</span>), template_format<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;jinja2&#34;</span>
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            body<span style=color:#ff79c6>.</span>messages[<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>][<span style=color:#f1fa8c>&#34;content&#34;</span>] <span style=color:#ff79c6>=</span> prompt_template<span style=color:#ff79c6>.</span>format(
</span></span><span style=display:flex><span>                context<span style=color:#ff79c6>=</span>tool_result, question<span style=color:#ff79c6>=</span>body<span style=color:#ff79c6>.</span>messages[<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>][<span style=color:#f1fa8c>&#34;content&#34;</span>]
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 删除tools和tool_choice字段，以避免不必要的处理</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>del</span> body<span style=color:#ff79c6>.</span>tools
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>del</span> body<span style=color:#ff79c6>.</span>tool_choice
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 构建响应头信息</span>
</span></span><span style=display:flex><span>            extra_json <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;message_id&#34;</span>: message_id,
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;status&#34;</span>: <span style=color:#ff79c6>None</span>,
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>            header <span style=color:#ff79c6>=</span> [
</span></span><span style=display:flex><span>                {
</span></span><span style=display:flex><span>                    <span style=color:#ff79c6>**</span>extra_json,
</span></span><span style=display:flex><span>                    <span style=color:#f1fa8c>&#34;content&#34;</span>: <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span>tool_result<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>,
</span></span><span style=display:flex><span>                    <span style=color:#f1fa8c>&#34;tool_output&#34;</span>: tool_result<span style=color:#ff79c6>.</span>data,
</span></span><span style=display:flex><span>                    <span style=color:#f1fa8c>&#34;is_ref&#34;</span>: <span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 发起OpenAI聊天完成请求</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>return</span> <span style=color:#ff79c6>await</span> openai_request(
</span></span><span style=display:flex><span>                client<span style=color:#ff79c6>.</span>chat<span style=color:#ff79c6>.</span>completions<span style=color:#ff79c6>.</span>create,
</span></span><span style=display:flex><span>                body,
</span></span><span style=display:flex><span>                extra_json<span style=color:#ff79c6>=</span>extra_json,
</span></span><span style=display:flex><span>                header<span style=color:#ff79c6>=</span>header,
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 如果指定了tools列表，调用agent处理函数</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># agent chat with tool calls</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> body<span style=color:#ff79c6>.</span>tools:
</span></span><span style=display:flex><span>        message_id <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>            add_message_to_db(
</span></span><span style=display:flex><span>                chat_type<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;agent_chat&#34;</span>,
</span></span><span style=display:flex><span>                query<span style=color:#ff79c6>=</span>body<span style=color:#ff79c6>.</span>messages[<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>][<span style=color:#f1fa8c>&#34;content&#34;</span>],
</span></span><span style=display:flex><span>                conversation_id<span style=color:#ff79c6>=</span>conversation_id,
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> conversation_id
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>else</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 配置聊天模型和工具配置</span>
</span></span><span style=display:flex><span>        chat_model_config <span style=color:#ff79c6>=</span> {}  <span style=color:#6272a4># TODO: 前端支持配置模型</span>
</span></span><span style=display:flex><span>        tool_names <span style=color:#ff79c6>=</span> [x[<span style=color:#f1fa8c>&#34;function&#34;</span>][<span style=color:#f1fa8c>&#34;name&#34;</span>] <span style=color:#ff79c6>for</span> x <span style=color:#ff79c6>in</span> body<span style=color:#ff79c6>.</span>tools]
</span></span><span style=display:flex><span>        tool_config <span style=color:#ff79c6>=</span> {name: get_tool_config(name) <span style=color:#ff79c6>for</span> name <span style=color:#ff79c6>in</span> tool_names}
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 调用agent处理函数，获取最终回复</span>
</span></span><span style=display:flex><span>        result <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>await</span> chat(
</span></span><span style=display:flex><span>            query<span style=color:#ff79c6>=</span>body<span style=color:#ff79c6>.</span>messages[<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>][<span style=color:#f1fa8c>&#34;content&#34;</span>],
</span></span><span style=display:flex><span>            metadata<span style=color:#ff79c6>=</span>extra<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;metadata&#34;</span>, {}),
</span></span><span style=display:flex><span>            conversation_id<span style=color:#ff79c6>=</span>extra<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;conversation_id&#34;</span>, <span style=color:#f1fa8c>&#34;&#34;</span>),
</span></span><span style=display:flex><span>            message_id<span style=color:#ff79c6>=</span>message_id,
</span></span><span style=display:flex><span>            history_len<span style=color:#ff79c6>=-</span><span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>            history<span style=color:#ff79c6>=</span>body<span style=color:#ff79c6>.</span>messages[:<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>],
</span></span><span style=display:flex><span>            stream<span style=color:#ff79c6>=</span>body<span style=color:#ff79c6>.</span>stream,
</span></span><span style=display:flex><span>            chat_model_config<span style=color:#ff79c6>=</span>extra<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;chat_model_config&#34;</span>, chat_model_config),
</span></span><span style=display:flex><span>            tool_config<span style=color:#ff79c6>=</span>extra<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;tool_config&#34;</span>, tool_config),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> result
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>else</span>:  <span style=color:#6272a4># 如果没有指定工具，直接使用LLM模型处理</span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># LLM chat directly</span>
</span></span><span style=display:flex><span>        message_id <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>            add_message_to_db(
</span></span><span style=display:flex><span>                chat_type<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;llm_chat&#34;</span>,
</span></span><span style=display:flex><span>                query<span style=color:#ff79c6>=</span>body<span style=color:#ff79c6>.</span>messages[<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>][<span style=color:#f1fa8c>&#34;content&#34;</span>],
</span></span><span style=display:flex><span>                conversation_id<span style=color:#ff79c6>=</span>conversation_id,
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> conversation_id
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>else</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 构建响应头信息</span>
</span></span><span style=display:flex><span>        extra_json <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;message_id&#34;</span>: message_id,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;status&#34;</span>: <span style=color:#ff79c6>None</span>,
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 发起OpenAI聊天完成请求</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#ff79c6>await</span> openai_request(
</span></span><span style=display:flex><span>            client<span style=color:#ff79c6>.</span>chat<span style=color:#ff79c6>.</span>completions<span style=color:#ff79c6>.</span>create, body, extra_json<span style=color:#ff79c6>=</span>extra_json
</span></span><span style=display:flex><span>        )
</span></span></code></pre></div><h4 id=kb知识库功能模块>kb知识库功能模块</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 初始化知识库路由，用于管理知识库的相关API</span>
</span></span><span style=display:flex><span>kb_router <span style=color:#ff79c6>=</span> APIRouter(prefix<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;/knowledge_base&#34;</span>, tags<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;Knowledge Base Management&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 获取知识库列表的API</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>get(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/list_knowledge_bases&#34;</span>, response_model<span style=color:#ff79c6>=</span>ListResponse, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;获取知识库列表&#34;</span>
</span></span><span style=display:flex><span>)(list_kbs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 创建知识库的API</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/create_knowledge_base&#34;</span>, response_model<span style=color:#ff79c6>=</span>BaseResponse, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;创建知识库&#34;</span>
</span></span><span style=display:flex><span>)(create_kb)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 删除知识库的API</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/delete_knowledge_base&#34;</span>, response_model<span style=color:#ff79c6>=</span>BaseResponse, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;删除知识库&#34;</span>
</span></span><span style=display:flex><span>)(delete_kb)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 获取知识库内文件列表的API</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>get(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/list_files&#34;</span>, response_model<span style=color:#ff79c6>=</span>ListResponse, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;获取知识库内的文件列表&#34;</span>
</span></span><span style=display:flex><span>)(list_files)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 搜索知识库的API</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>post(<span style=color:#f1fa8c>&#34;/search_docs&#34;</span>, response_model<span style=color:#ff79c6>=</span>List[<span style=color:#8be9fd;font-style:italic>dict</span>], summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;搜索知识库&#34;</span>)(
</span></span><span style=display:flex><span>    search_docs
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 上传文件到知识库并进行向量化的API</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/upload_docs&#34;</span>,
</span></span><span style=display:flex><span>    response_model<span style=color:#ff79c6>=</span>BaseResponse,
</span></span><span style=display:flex><span>    summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;上传文件到知识库，并/或进行向量化&#34;</span>,
</span></span><span style=display:flex><span>)(upload_docs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 删除知识库内指定文件的API</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/delete_docs&#34;</span>, response_model<span style=color:#ff79c6>=</span>BaseResponse, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;删除知识库内指定文件&#34;</span>
</span></span><span style=display:flex><span>)(delete_docs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 更新知识库介绍的API</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>post(<span style=color:#f1fa8c>&#34;/update_info&#34;</span>, response_model<span style=color:#ff79c6>=</span>BaseResponse, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;更新知识库介绍&#34;</span>)(
</span></span><span style=display:flex><span>    update_info
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 更新现有文件到知识库的API</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/update_docs&#34;</span>, response_model<span style=color:#ff79c6>=</span>BaseResponse, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;更新现有文件到知识库&#34;</span>
</span></span><span style=display:flex><span>)(update_docs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 下载知识库文件的API</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;/download_doc&#34;</span>, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;下载对应的知识文件&#34;</span>)(download_doc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 重建向量库的API</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/recreate_vector_store&#34;</span>, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;根据content中文档重建向量库，流式输出处理进度。&#34;</span>
</span></span><span style=display:flex><span>)(recreate_vector_store)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 上传临时文件的API，用于文件对话</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>post(<span style=color:#f1fa8c>&#34;/upload_temp_docs&#34;</span>, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;上传文件到临时目录，用于文件对话。&#34;</span>)(
</span></span><span style=display:flex><span>    upload_temp_docs
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 初始化摘要路由，用于管理知识库摘要的相关API</span>
</span></span><span style=display:flex><span>summary_router <span style=color:#ff79c6>=</span> APIRouter(prefix<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;/kb_summary_api&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 根据文件名称生成摘要的API</span>
</span></span><span style=display:flex><span>summary_router<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/summary_file_to_vector_store&#34;</span>, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;单个知识库根据文件名称摘要&#34;</span>
</span></span><span style=display:flex><span>)(summary_file_to_vector_store)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 根据doc_ids生成摘要的API</span>
</span></span><span style=display:flex><span>summary_router<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/summary_doc_ids_to_vector_store&#34;</span>,
</span></span><span style=display:flex><span>    summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;单个知识库根据doc_ids摘要&#34;</span>,
</span></span><span style=display:flex><span>    response_model<span style=color:#ff79c6>=</span>BaseResponse,
</span></span><span style=display:flex><span>)(summary_doc_ids_to_vector_store)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 重建单个知识库文件摘要的API</span>
</span></span><span style=display:flex><span>summary_router<span style=color:#ff79c6>.</span>post(<span style=color:#f1fa8c>&#34;/recreate_summary_vector_store&#34;</span>, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;重建单个知识库文件摘要&#34;</span>)(
</span></span><span style=display:flex><span>    recreate_summary_vector_store
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 将摘要路由包含进知识库路由中</span>
</span></span><span style=display:flex><span>kb_router<span style=color:#ff79c6>.</span>include_router(summary_router)
</span></span></code></pre></div><h4 id=openai功能模块>openai功能模块</h4><p>这里定义了兼容OpenAI API的接口，封装调用了OpenAI SDK</p><h4 id=tool工具功能模块>tool工具功能模块</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 初始化工具路由，用于集中管理工具相关的API</span>
</span></span><span style=display:flex><span>tool_router <span style=color:#ff79c6>=</span> APIRouter(prefix<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;/tools&#34;</span>, tags<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;Toolkits&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 描述：列出所有可用的工具及其配置信息</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 请求方法：GET</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 请求路径：/tools</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 响应模型：BaseResponse</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 返回数据格式：包含所有工具信息的字典，每个工具以其名称为键，值为包含工具标题、描述、参数、配置等信息的字典</span>
</span></span><span style=display:flex><span>@tool_router.get(<span style=color:#f1fa8c>&#34;&#34;</span>, response_model<span style=color:#ff79c6>=</span>BaseResponse)
</span></span><span style=display:flex><span><span style=color:#ff79c6>async</span> <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>list_tools</span>():
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    获取所有工具的信息列表。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    返回:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - data: 包含所有工具详细信息的字典。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    tools <span style=color:#ff79c6>=</span> get_tool()
</span></span><span style=display:flex><span>    data <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>        t<span style=color:#ff79c6>.</span>name: {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;name&#34;</span>: t<span style=color:#ff79c6>.</span>name,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;title&#34;</span>: t<span style=color:#ff79c6>.</span>title,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;description&#34;</span>: t<span style=color:#ff79c6>.</span>description,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;args&#34;</span>: t<span style=color:#ff79c6>.</span>args,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;config&#34;</span>: get_tool_config(t<span style=color:#ff79c6>.</span>name),
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> t <span style=color:#ff79c6>in</span> tools<span style=color:#ff79c6>.</span>values()
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> {<span style=color:#f1fa8c>&#34;data&#34;</span>: data}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 描述：调用指定的工具进行处理</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 请求方法：POST</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 请求路径：/tools/call</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 请求参数：name - 工具名称, tool_input - 工具输入参数</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 响应模型：BaseResponse</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 返回数据格式：调用成功返回工具处理结果，调用失败返回错误信息</span>
</span></span><span style=display:flex><span>@tool_router.post(<span style=color:#f1fa8c>&#34;/call&#34;</span>, response_model<span style=color:#ff79c6>=</span>BaseResponse)
</span></span><span style=display:flex><span><span style=color:#ff79c6>async</span> <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>call_tool</span>(
</span></span><span style=display:flex><span>    name: <span style=color:#8be9fd;font-style:italic>str</span> <span style=color:#ff79c6>=</span> Body(examples<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;calculate&#34;</span>]),
</span></span><span style=display:flex><span>    tool_input: <span style=color:#8be9fd;font-style:italic>dict</span> <span style=color:#ff79c6>=</span> Body({}, examples<span style=color:#ff79c6>=</span>[{<span style=color:#f1fa8c>&#34;text&#34;</span>: <span style=color:#f1fa8c>&#34;3+5/2&#34;</span>}]),
</span></span><span style=display:flex><span>):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    调用指定的工具进行处理。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    参数:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - name: 工具的名称。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - tool_input: 提供给工具的输入参数。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    返回:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - data: 工具处理的结果。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 尝试获取指定名称的工具</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> tool <span style=color:#ff79c6>:=</span> get_tool(name):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>try</span>:
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 异步调用工具的处理方法</span>
</span></span><span style=display:flex><span>            result <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>await</span> tool<span style=color:#ff79c6>.</span>ainvoke(tool_input)
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>return</span> {<span style=color:#f1fa8c>&#34;data&#34;</span>: result}
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>except</span> Exception:
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 记录调用工具失败的错误日志</span>
</span></span><span style=display:flex><span>            msg <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;failed to call tool &#39;</span><span style=color:#f1fa8c>{</span>name<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#39;&#34;</span>
</span></span><span style=display:flex><span>            logger<span style=color:#ff79c6>.</span>error(msg, exc_info<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>            <span style=color:#6272a4># 返回调用失败的响应</span>
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>return</span> {<span style=color:#f1fa8c>&#34;code&#34;</span>: <span style=color:#bd93f9>500</span>, <span style=color:#f1fa8c>&#34;msg&#34;</span>: msg}
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 返回找不到指定工具的响应</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> {<span style=color:#f1fa8c>&#34;code&#34;</span>: <span style=color:#bd93f9>500</span>, <span style=color:#f1fa8c>&#34;msg&#34;</span>: <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;no tool named &#39;</span><span style=color:#f1fa8c>{</span>name<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#39;&#34;</span>}
</span></span></code></pre></div><h4 id=server功能模块>server功能模块</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> typing <span style=color:#ff79c6>import</span> Literal
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> fastapi <span style=color:#ff79c6>import</span> APIRouter, Body
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> chatchat.server.utils <span style=color:#ff79c6>import</span> get_prompt_template, get_server_configs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 初始化一个APIRouter，用于处理与服务器状态相关的路由</span>
</span></span><span style=display:flex><span>server_router <span style=color:#ff79c6>=</span> APIRouter(prefix<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;/server&#34;</span>, tags<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;Server State&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义路由规则，用于获取服务器的原始配置信息</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 服务器相关接口</span>
</span></span><span style=display:flex><span>server_router<span style=color:#ff79c6>.</span>post(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;/configs&#34;</span>,
</span></span><span style=display:flex><span>    summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;获取服务器原始配置信息&#34;</span>,
</span></span><span style=display:flex><span>)(get_server_configs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 定义路由规则，用于根据类型和名称获取服务器的prompt模板</span>
</span></span><span style=display:flex><span>@server_router.post(<span style=color:#f1fa8c>&#34;/get_prompt_template&#34;</span>, summary<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;获取服务区配置的prompt模板&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>get_server_prompt_template</span>(
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 模板类型，可选值为&#34;llm_chat&#34;或&#34;knowledge_base_chat&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>type</span>: Literal[<span style=color:#f1fa8c>&#34;llm_chat&#34;</span>, <span style=color:#f1fa8c>&#34;knowledge_base_chat&#34;</span>] <span style=color:#ff79c6>=</span> Body(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;llm_chat&#34;</span>, description<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;模板类型，可选值：llm_chat，knowledge_base_chat&#34;</span>
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 模板名称，默认为&#34;default&#34;</span>
</span></span><span style=display:flex><span>    name: <span style=color:#8be9fd;font-style:italic>str</span> <span style=color:#ff79c6>=</span> Body(<span style=color:#f1fa8c>&#34;default&#34;</span>, description<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;模板名称&#34;</span>),
</span></span><span style=display:flex><span>) <span style=color:#ff79c6>-&gt;</span> <span style=color:#8be9fd;font-style:italic>str</span>:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    根据提供的模板类型和名称，获取相应的prompt模板。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    参数:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - type: 模板类型，可以是&#34;llm_chat&#34;或&#34;knowledge_base_chat&#34;。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - name: 模板名称。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    返回:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    - 与指定类型和名称匹配的prompt模板字符串。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> get_prompt_template(<span style=color:#8be9fd;font-style:italic>type</span><span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>type</span>, name<span style=color:#ff79c6>=</span>name)
</span></span></code></pre></div><h4 id=webui服务>WebUI服务</h4><h2 id=maxkb知识库>MaxKB知识库</h2><p>Github链接：<a href=https://github.com/1Panel-dev/MaxKB>https://github.com/1Panel-dev/MaxKB</a></p><h2 id=copilot-云桌面>Copilot+ 云桌面</h2><h3 id=puter>puter</h3><p><a href=https://github.com/HeyPuter/puter>puter WebOS</a>：Puter是一个先进的开源桌面环境，运行在浏览器中，旨在具备丰富的功能、异常快速和高度可扩展性。</p><h3 id=neko>neko</h3><p><a href=https://github.com/m1k1o/neko>虚拟浏览器</a> Neko是一款在 Docker 中运行、采用 WebRTC 技术的自托管虚拟浏览器，功能强大。它允许在虚拟环境中运行全功能浏览器，保障安全私密的上网体验，支持多用户同时访问。可用于多种场景，如团队协作、个人隐私保护、举办活动等。</p><p>部署neko</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/m1k1o/neko.git
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>cd</span> neko/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 使用chromium的镜像，并设置http代理，没有代理的话就移除对应配置</span>
</span></span><span style=display:flex><span>root@llm-test:~/neko# vim docker-compose.yaml 
</span></span><span style=display:flex><span>version: <span style=color:#f1fa8c>&#34;3.4&#34;</span>
</span></span><span style=display:flex><span>services:
</span></span><span style=display:flex><span>  neko:
</span></span><span style=display:flex><span>    image: <span style=color:#f1fa8c>&#34;m1k1o/neko:chromium&#34;</span>
</span></span><span style=display:flex><span>    restart: <span style=color:#f1fa8c>&#34;unless-stopped&#34;</span>
</span></span><span style=display:flex><span>    shm_size: <span style=color:#f1fa8c>&#34;8gb&#34;</span>
</span></span><span style=display:flex><span>    cpus: <span style=color:#bd93f9>4</span>
</span></span><span style=display:flex><span>    cap_add:
</span></span><span style=display:flex><span>    - SYS_ADMIN
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>      - <span style=color:#f1fa8c>&#34;8080:8080&#34;</span>
</span></span><span style=display:flex><span>      - <span style=color:#f1fa8c>&#34;52000-52100:52000-52100/udp&#34;</span>
</span></span><span style=display:flex><span>    environment:
</span></span><span style=display:flex><span>      HTTP_PROXY: http://&lt;你的代理地址&gt;:31475
</span></span><span style=display:flex><span>      HTTPS_PROXY: http://&lt;你的代理地址&gt;:31475
</span></span><span style=display:flex><span>      NO_PROXY: localhost,127.0.0.1
</span></span><span style=display:flex><span>      NEKO_SCREEN: 1920x1080@30
</span></span><span style=display:flex><span>      NEKO_PASSWORD: neko
</span></span><span style=display:flex><span>      NEKO_PASSWORD_ADMIN: admin
</span></span><span style=display:flex><span>      NEKO_EPR: 52000-52100
</span></span><span style=display:flex><span>      NEKO_ICELITE: <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>      NEKO_NAT1TO1: &lt;你的内网IP&gt;   <span style=color:#6272a4># 部署在内网环境的话，要设置这个映射IP</span>
</span></span></code></pre></div><p>启动neko容器</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker-compose up -d
</span></span></code></pre></div><p>Chromium下载安装扩展支持，创建<code>policies.json</code>文件</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@llm-test:~/neko# vim .docker/chromium/policies.json
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;AutofillAddressEnabled&#34;</span>: false,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;AutofillCreditCardEnabled&#34;</span>: false,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;BrowserSignin&#34;</span>: 0,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;DefaultNotificationsSetting&#34;</span>: 2,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;DeveloperToolsAvailability&#34;</span>: 1,   <span style=color:#6272a4># 1表示无论在什么情况下，都可以访问开发者工具和控制台</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;EditBookmarksEnabled&#34;</span>: false,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;FullscreenAllowed&#34;</span>: true,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;IncognitoModeAvailability&#34;</span>: 1,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;SyncDisabled&#34;</span>: true,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;AutoplayAllowed&#34;</span>: true,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;BrowserAddPersonEnabled&#34;</span>: false,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;BrowserGuestModeEnabled&#34;</span>: false,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;DefaultPopupsSetting&#34;</span>: 2,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;DownloadRestrictions&#34;</span>: 0,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;VideoCaptureAllowed&#34;</span>: true,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;AllowFileSelectionDialogs&#34;</span>: true,   <span style=color:#6272a4># 允许文件选择对话框加载私有extension</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;PromptForDownloadLocation&#34;</span>: false,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;BookmarkBarEnabled&#34;</span>: false,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;PasswordManagerEnabled&#34;</span>: false,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;BrowserLabsEnabled&#34;</span>: false,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;URLAllowlist&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;file:///home/neko/Downloads&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;file://*&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;chrome://policy&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;ExtensionInstallForcelist&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;cjpalhdlnbpafiamejdnhcphjbkeiagm;https://clients2.google.com/service/update2/crx&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;mnjggcdmjocbbbhaepdhchncahnbgone;https://clients2.google.com/service/update2/crx&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;ExtensionInstallAllowlist&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;cjpalhdlnbpafiamejdnhcphjbkeiagm&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;mnjggcdmjocbbbhaepdhchncahnbgone&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;padekgcemlokbadohgkifijomclgjgif&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span></code></pre></div><p>拷贝policies.json至docker目录下，并重启docker容器</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker cp policies.json &lt;neko-docker-id&gt;:/etc/chromium/policies/managed/policies.json
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>docker restart &lt;neko-docker-id&gt;
</span></span></code></pre></div><p>部署失败排错参考：https://neko.m1k1o.net/#/getting-started/troubleshooting</p><h2 id=pdf转markdown>PDF转markdown</h2><p><a href=https://github.com/VikParuchuri/marker>marker</a></p><h1 id=llm大模型评测>LLM大模型评测</h1><p><a href=https://ai-bot.cn/>https://ai-bot.cn/</a>，这个网站类似AI工具的链接导航，有MMLU、Open LLM Leaderboard、OpenCompass等；其中中文大模型评测的话，推荐OpenCompass，国产大模型阿里通义千问在其github展示的评测表现也是基于OpenCompass.</p><p>FastChat也提供一个工具<a href=https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge>llm_judge</a></p><p>编程类模型评估标准参考：</p><ul><li>主要HumanEval/Babelcode指标，得分越高越好</li><li>模型参数量不宜过大，过大增加精调和部署成本</li><li>最好具备一定中文能力，但是编码能力优先级更高</li></ul><p><img src=https://raw.githubusercontent.com/yaoice/blog-images/main/2024-01-18-llm_develop_readnotes/20240118-code-model.png alt></p><h2 id=opencompass>OpenCompass</h2><h3 id=简介-1>简介</h3><p>OpenCompass司南2.0是大模型评测体系，主要由三大核心模块构建而成：CompassKit、CompassHub以及CompassRank组成。</p><ul><li>CompassRank是一个排行榜体系，包含开源基准测试项目、私有基准测试</li><li>CompassHub是一个基准测试资源导航平台</li><li>CompassKit是一系列专为大型语言模型和大型视觉-语言模型打造的强大评估工具合集</li></ul><p>OpenCompass是面向大模型评测的一站式平台。其主要特点如下：</p><ul><li>开源可复现：提供公平、公开、可复现的大模型评测方案</li><li>全面的能力维度：五大维度设计，提供 70+ 个数据集约 40 万题的的模型评测方案，全面评估模型能力</li><li>丰富的模型支持：已支持 20+ HuggingFace 及 API 模型</li><li>分布式高效评测：一行命令实现任务分割和分布式评测，数小时即可完成千亿模型全量评测</li><li>多样化评测范式：支持零样本、小样本及思维链评测，结合标准型或对话型提示词模板，轻松激发各种模型最大性能</li><li>灵活化拓展：想增加新模型或数据集？想要自定义更高级的任务分割策略，甚至接入新的集群管理系统？OpenCompass 的一切均可轻松扩展！</li></ul><h3 id=原理>原理</h3><p>OpenCompass原理图：
<img src=https://raw.githubusercontent.com/yaoice/blog-images/main/2024-01-18-llm_develop_readnotes/20240118-opencompass-principle.png alt=20240118-opencompass-principle></p><h3 id=实践>实践</h3><p>安装opencompass</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/open-compass/opencompass opencompass
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>cd</span> opencompass
</span></span><span style=display:flex><span>pip install -e 
</span></span><span style=display:flex><span><span style=color:#6272a4># 如果需要使用各个API模型，`pip install -r requirements/api.txt`安装API模型的相关依赖</span>
</span></span></code></pre></div><p>下载数据集</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>wget https://github.com/open-compass/opencompass/releases/download/0.2.2.rc1/OpenCompassData-core-20240207.zip
</span></span><span style=display:flex><span>unzip OpenCompassData-core-20240207.zip
</span></span></code></pre></div><p>评测</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4>#官方测试用例llama_7b</span>
</span></span><span style=display:flex><span>python run.py --models hf_llama_7b --datasets mmlu_ppl ceval_ppl 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4>#评测本地微调后的模型phi-3-mini-4k</span>
</span></span><span style=display:flex><span>python run.py --datasets ceval_gen <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--hf-path /root/phi-3-mini-4k-with-lora/ <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--model-kwargs <span style=color:#8be9fd;font-style:italic>device_map</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;auto&#39;</span> <span style=color:#8be9fd;font-style:italic>trust_remote_code</span><span style=color:#ff79c6>=</span>True <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--max-seq-len <span style=color:#bd93f9>2048</span> <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--batch-size <span style=color:#bd93f9>8</span> <span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span>--num-gpus <span style=color:#bd93f9>1</span>
</span></span></code></pre></div><p>评测结果输出</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>dataset                                         version    metric    mode    _hf
</span></span><span style=display:flex><span>----------------------------------------------  ---------  --------  ------  -----
</span></span><span style=display:flex><span>ceval-computer_network                          db9ce2     accuracy  gen     47.37
</span></span><span style=display:flex><span>ceval-operating_system                          1c2571     accuracy  gen     57.89
</span></span><span style=display:flex><span>ceval-computer_architecture                     a74dad     accuracy  gen     42.86
</span></span><span style=display:flex><span>ceval-college_programming                       4ca32a     accuracy  gen     54.05
</span></span><span style=display:flex><span>ceval-college_physics                           963fa8     accuracy  gen     42.11
</span></span><span style=display:flex><span>ceval-college_chemistry                         e78857     accuracy  gen     29.17
</span></span><span style=display:flex><span>ceval-advanced_mathematics                      ce03e2     accuracy  gen     36.84
</span></span><span style=display:flex><span>ceval-probability_and_statistics                -          -         -       -
</span></span><span style=display:flex><span>ceval-discrete_mathematics                      -          -         -       -
</span></span><span style=display:flex><span>ceval-electrical_engineer                       -          -         -       -
</span></span><span style=display:flex><span>ceval-metrology_engineer                        -          -         -       -
</span></span><span style=display:flex><span>ceval-high_school_mathematics                   -          -         -       -
</span></span><span style=display:flex><span>ceval-high_school_physics                       -          -         -       -
</span></span><span style=display:flex><span>ceval-high_school_chemistry                     -          -         -       -
</span></span><span style=display:flex><span>ceval-high_school_biology                       -          -         -       -
</span></span><span style=display:flex><span>ceval-middle_school_mathematics                 -          -         -       -
</span></span><span style=display:flex><span>ceval-middle_school_biology                     -          -         -       -
</span></span><span style=display:flex><span>ceval-middle_school_physics                     -          -         -       -
</span></span><span style=display:flex><span>ceval-middle_school_chemistry                   -          -         -       -
</span></span><span style=display:flex><span>ceval-veterinary_medicine                       -          -         -       -
</span></span><span style=display:flex><span>ceval-college_economics                         -          -         -       -
</span></span><span style=display:flex><span>ceval-business_administration                   -          -         -       -
</span></span><span style=display:flex><span>ceval-marxism                                   -          -         -       -
</span></span><span style=display:flex><span>ceval-mao_zedong_thought                        -          -         -       -
</span></span><span style=display:flex><span>ceval-education_science                         -          -         -       -
</span></span><span style=display:flex><span>ceval-teacher_qualification                     -          -         -       -
</span></span><span style=display:flex><span>ceval-high_school_politics                      -          -         -       -
</span></span><span style=display:flex><span>ceval-high_school_geography                     -          -         -       -
</span></span><span style=display:flex><span>ceval-middle_school_politics                    -          -         -       -
</span></span><span style=display:flex><span>ceval-middle_school_geography                   -          -         -       -
</span></span><span style=display:flex><span>ceval-modern_chinese_history                    -          -         -       -
</span></span><span style=display:flex><span>ceval-ideological_and_moral_cultivation         -          -         -       -
</span></span><span style=display:flex><span>ceval-logic                                     -          -         -       -
</span></span><span style=display:flex><span>ceval-law                                       -          -         -       -
</span></span><span style=display:flex><span>ceval-chinese_language_and_literature           -          -         -       -
</span></span><span style=display:flex><span>ceval-art_studies                               -          -         -       -
</span></span><span style=display:flex><span>ceval-professional_tour_guide                   -          -         -       -
</span></span><span style=display:flex><span>ceval-legal_professional                        -          -         -       -
</span></span><span style=display:flex><span>ceval-high_school_chinese                       -          -         -       -
</span></span><span style=display:flex><span>ceval-high_school_history                       -          -         -       -
</span></span><span style=display:flex><span>ceval-middle_school_history                     -          -         -       -
</span></span><span style=display:flex><span>ceval-civil_servant                             -          -         -       -
</span></span><span style=display:flex><span>ceval-sports_science                            -          -         -       -
</span></span><span style=display:flex><span>ceval-plant_protection                          -          -         -       -
</span></span><span style=display:flex><span>ceval-basic_medicine                            -          -         -       -
</span></span><span style=display:flex><span>ceval-clinical_medicine                         -          -         -       -
</span></span><span style=display:flex><span>ceval-urban_and_rural_planner                   -          -         -       -
</span></span><span style=display:flex><span>ceval-accountant                                -          -         -       -
</span></span><span style=display:flex><span>ceval-fire_engineer                             -          -         -       -
</span></span><span style=display:flex><span>ceval-environmental_impact_assessment_engineer  -          -         -       -
</span></span><span style=display:flex><span>ceval-tax_accountant                            -          -         -       -
</span></span><span style=display:flex><span>ceval-physician                                 -          -         -       -
</span></span><span style=display:flex><span>05/26 15:16:32 - OpenCompass - INFO - write summary to /root/opencompass/outputs/default/20240526_151246/summary/summary_20240526_151246.txt
</span></span><span style=display:flex><span>05/26 15:16:32 - OpenCompass - INFO - write csv to /root/opencompass/outputs/default/20240526_151246/summary/summary_20240526_151246.csv
</span></span></code></pre></div><h3 id=自定义数据集>自定义数据集</h3><ul><li>数据集格式：支持 .jsonl 和 .csv 两种格式的数据集</li><li>任务类型：包括选择题 (mcq) 和问答 (qa) 两种</li></ul><p>选择题的.jsonl格式：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{<span style=color:#ff79c6>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;165+833+650+615=&#34;</span>, <span style=color:#ff79c6>&#34;A&#34;</span>: <span style=color:#f1fa8c>&#34;2258&#34;</span>, <span style=color:#ff79c6>&#34;B&#34;</span>: <span style=color:#f1fa8c>&#34;2263&#34;</span>, <span style=color:#ff79c6>&#34;C&#34;</span>: <span style=color:#f1fa8c>&#34;2281&#34;</span>, <span style=color:#ff79c6>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;B&#34;</span>}
</span></span><span style=display:flex><span>{<span style=color:#ff79c6>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;368+959+918+653+978=&#34;</span>, <span style=color:#ff79c6>&#34;A&#34;</span>: <span style=color:#f1fa8c>&#34;3876&#34;</span>, <span style=color:#ff79c6>&#34;B&#34;</span>: <span style=color:#f1fa8c>&#34;3878&#34;</span>, <span style=color:#ff79c6>&#34;C&#34;</span>: <span style=color:#f1fa8c>&#34;3880&#34;</span>, <span style=color:#ff79c6>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;A&#34;</span>}
</span></span><span style=display:flex><span>{<span style=color:#ff79c6>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;776+208+589+882+571+996+515+726=&#34;</span>, <span style=color:#ff79c6>&#34;A&#34;</span>: <span style=color:#f1fa8c>&#34;5213&#34;</span>, <span style=color:#ff79c6>&#34;B&#34;</span>: <span style=color:#f1fa8c>&#34;5263&#34;</span>, <span style=color:#ff79c6>&#34;C&#34;</span>: <span style=color:#f1fa8c>&#34;5383&#34;</span>, <span style=color:#ff79c6>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;B&#34;</span>}
</span></span><span style=display:flex><span>{<span style=color:#ff79c6>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;803+862+815+100+409+758+262+169=&#34;</span>, <span style=color:#ff79c6>&#34;A&#34;</span>: <span style=color:#f1fa8c>&#34;4098&#34;</span>, <span style=color:#ff79c6>&#34;B&#34;</span>: <span style=color:#f1fa8c>&#34;4128&#34;</span>, <span style=color:#ff79c6>&#34;C&#34;</span>: <span style=color:#f1fa8c>&#34;4178&#34;</span>, <span style=color:#ff79c6>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;C&#34;</span>}
</span></span></code></pre></div><p>选择题的.csv格式：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>question,A,B,C,answer
</span></span><span style=display:flex><span>127+545+588+620+556+199=,2632,2635,2645,B
</span></span><span style=display:flex><span>735+603+102+335+605=,2376,2380,2410,B
</span></span><span style=display:flex><span>506+346+920+451+910+142+659+850=,4766,4774,4784,C
</span></span><span style=display:flex><span>504+811+870+445=,2615,2630,2750,B
</span></span></code></pre></div><p>问答的.jsonl格式：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{<span style=color:#ff79c6>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;752+361+181+933+235+986=&#34;</span>, <span style=color:#ff79c6>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;3448&#34;</span>}
</span></span><span style=display:flex><span>{<span style=color:#ff79c6>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;712+165+223+711=&#34;</span>, <span style=color:#ff79c6>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;1811&#34;</span>}
</span></span><span style=display:flex><span>{<span style=color:#ff79c6>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;921+975+888+539=&#34;</span>, <span style=color:#ff79c6>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;3323&#34;</span>}
</span></span><span style=display:flex><span>{<span style=color:#ff79c6>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;752+321+388+643+568+982+468+397=&#34;</span>, <span style=color:#ff79c6>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;4519&#34;</span>}
</span></span></code></pre></div><p>问答的.csv格式：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>question,answer
</span></span><span style=display:flex><span>123+147+874+850+915+163+291+604=,3967
</span></span><span style=display:flex><span>149+646+241+898+822+386=,3142
</span></span><span style=display:flex><span>332+424+582+962+735+798+653+214=,4700
</span></span><span style=display:flex><span>649+215+412+495+220+738+989+452=,4170
</span></span></code></pre></div><p>加载自定义数据集使用<code>--custom-dataset-path</code>参数来加载</p><h1 id=llm大模型量化>LLM大模型量化</h1><p>什么是量化? 举个例子来解释量化，模型使用16位浮点数作为权重进行训练，可以将其缩小到4位整数以进行推理，而不会失去太多的功率，收益是会节省大量的GPU计算资源。</p><h2 id=llamacpp>llama.cpp</h2><p>llama.cpp主要解决的是推理过程中的性能问题。主要有两点优化：</p><ul><li>llama.cpp使用的是C/C++语言写的机器学习张量库ggml</li><li>llama.cpp提供了模型量化的工具</li></ul><p>llama.cpp量化出来的模型格式是GGUF，准备来说GGUF是一种模型量化方法，除了GGUF，还有GPTQ、AWQ等。</p><ul><li>GPTQ：通过最小化该权重的均方误差将所有权重压缩到4位，在推理过程中，它将动态地将其权重去量化为float16，以提高性能，同时保持低内存。如果没有高性能的GPU，建议切换为GGUF，支持在cpu上运行。</li><li>GGUF：允许使用CPU来运行模型，也可以把某些层使用GPU来提高速度</li><li>AWQ：激活感知权重量化，权重并非同等重要，在量化过程中会跳过一小部分权重，这有助于减轻量化损失。</li></ul><p>使用llama.cpp/main可直接加载GGUF格式的模型</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./llama.cpp/main -m model-unsloth.Q4_K_M.gguf -p <span style=color:#f1fa8c>&#34;Building a website can be done in 10 simple steps:\nStep 1:&#34;</span> -n <span style=color:#bd93f9>400</span> -e
</span></span></code></pre></div><p>使用llama.cpp转换格式，HF转换为16bit GGUF格式</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python llama.cpp/convert-hf-to-gguf.py --outfile phi-3-mini-4k-lora.fp16.gguf --outtype f16 phi-3-mini-4k-with-lora/
</span></span></code></pre></div><p>使用llama.cpp把16bit量化为4bit GGUF</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./llama.cpp/quantize phi-3-mini-4k-lora.fp16.gguf phi-3-mini-4k-lora.Q4_K_M.gguf Q4_K_M
</span></span></code></pre></div><h2 id=gptq>GPTQ</h2><p>安装auto-gptq库，GPTQ量化仅适用于文本模型</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install auto-gptq
</span></span><span style=display:flex><span>pip install --upgrade accelerate optimum transformers
</span></span></code></pre></div><p>GPTQ量化模型</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> AutoModelForCausalLM, AutoTokenizer, GPTQConfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_id <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;microsoft/Phi-3-mini-4k-instruct&#34;</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#ff79c6>=</span> AutoTokenizer<span style=color:#ff79c6>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span><span style=color:#6272a4># 创建GPTQ配置，需要传入的参数：bits的数量、校准量化的数据集、模型的分词器</span>
</span></span><span style=display:flex><span>gptq_config <span style=color:#ff79c6>=</span> GPTQConfig(bits<span style=color:#ff79c6>=</span><span style=color:#bd93f9>4</span>, dataset <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;c4&#34;</span>, tokenizer<span style=color:#ff79c6>=</span>tokenizer)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 也可以传入自定义数据集，官方强烈建议使用GPTQ论文中提供的数据集</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># dataset = [&#34;auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.&#34;]</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># gptq_config = GPTQConfig(bits=4, dataset = dataset, tokenizer=tokenizer)</span>
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> AutoModelForCausalLM<span style=color:#ff79c6>.</span>from_pretrained(model_id, quantization_config<span style=color:#ff79c6>=</span>gptq_config)
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>save_pretrained(<span style=color:#f1fa8c>&#34;Phi-3-mini-4k-instruct-GPTQ-Int4&#34;</span>)
</span></span><span style=display:flex><span>tokenizer<span style=color:#ff79c6>.</span>save_pretrained(<span style=color:#f1fa8c>&#34;Phi-3-mini-4k-instruct-GPTQ-Int4&#34;</span>)
</span></span></code></pre></div><h2 id=awq>AWQ</h2><p>使用AWQ需要访问NVIDIA GPU。目前不支持CPU推理。目前与Transformers的集成仅适用于使用autoawq、llm-awq量化后的模型。</p><p>安装autoawq库</p><pre tabindex=0><code>pip install -q transformers accelerate autoawq
</code></pre><p>使用autoawq进行量化，使用colab一直卡住？下次换成本地量化待验证中</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> awq <span style=color:#ff79c6>import</span> AutoAWQForCausalLM
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4>#model_path = &#34;facebook/opt-125m&#34;</span>
</span></span><span style=display:flex><span>model_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;Qwen/Qwen1.5-0.5B-Chat&#34;</span>
</span></span><span style=display:flex><span>quant_path <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;Qwen1.5-0.5B-Chat-AWQ&#34;</span>
</span></span><span style=display:flex><span>quant_config <span style=color:#ff79c6>=</span> {<span style=color:#f1fa8c>&#34;zero_point&#34;</span>: <span style=color:#ff79c6>True</span>, <span style=color:#f1fa8c>&#34;q_group_size&#34;</span>: <span style=color:#bd93f9>128</span>, <span style=color:#f1fa8c>&#34;w_bit&#34;</span>: <span style=color:#bd93f9>4</span>, <span style=color:#f1fa8c>&#34;version&#34;</span>:<span style=color:#f1fa8c>&#34;GEMM&#34;</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Load model</span>
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> AutoAWQForCausalLM<span style=color:#ff79c6>.</span>from_pretrained(model_path, device_map<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;auto&#34;</span>, safetensors<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#ff79c6>=</span> AutoTokenizer<span style=color:#ff79c6>.</span>from_pretrained(model_path, trust_remote_code<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Quantize</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>quantize(tokenizer, quant_config<span style=color:#ff79c6>=</span>quant_config)
</span></span></code></pre></div><p>执行quantize的函数时出现<code>Token indices sequence length is longer than the specified maximum sequence length for this model (909 > 512).</code>的提示，这个提示并非是错误；此外并非所有的模型都支持autoawq，autoawq支持的模型列表参考：<code>https://github.com/casper-hansen/AutoAWQ/blob/main/awq/models/auto.py#L7</code></p><p>兼容transformers，并保存模型</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> AwqConfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># modify the config file so that it is compatible with transformers integration</span>
</span></span><span style=display:flex><span>quantization_config <span style=color:#ff79c6>=</span> AwqConfig(
</span></span><span style=display:flex><span>    bits<span style=color:#ff79c6>=</span>quant_config[<span style=color:#f1fa8c>&#34;w_bit&#34;</span>],
</span></span><span style=display:flex><span>    group_size<span style=color:#ff79c6>=</span>quant_config[<span style=color:#f1fa8c>&#34;q_group_size&#34;</span>],
</span></span><span style=display:flex><span>    zero_point<span style=color:#ff79c6>=</span>quant_config[<span style=color:#f1fa8c>&#34;zero_point&#34;</span>],
</span></span><span style=display:flex><span>    version<span style=color:#ff79c6>=</span>quant_config[<span style=color:#f1fa8c>&#34;version&#34;</span>]<span style=color:#ff79c6>.</span>lower(),
</span></span><span style=display:flex><span>)<span style=color:#ff79c6>.</span>to_dict()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># the pretrained transformers model is stored in the model attribute + we need to pass a dict</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>model<span style=color:#ff79c6>.</span>config<span style=color:#ff79c6>.</span>quantization_config <span style=color:#ff79c6>=</span> quantization_config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># save model weights</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>save_quantized(quant_path)
</span></span><span style=display:flex><span>tokenizer<span style=color:#ff79c6>.</span>save_pretrained(quant_path)
</span></span></code></pre></div><h2 id=bitsandbytes>BitSandBytes</h2><p>Transformers与bitsandbytes做了紧密集成，bitsandbytes集成支持8bit和4bit精度数据类型，用于加载大模型，这种方式可以节省内存</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># </span>
</span></span><span style=display:flex><span>quantization_config <span style=color:#ff79c6>=</span> BitsAndBytesConfig(load_in_4bit<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> AutoModelForCausalLM<span style=color:#ff79c6>.</span>from_pretrained(<span style=color:#f1fa8c>&#34;microsoft/Phi-3-mini-4k-instruct&#34;</span>, 
</span></span><span style=display:flex><span>    trust_remote_code<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, 
</span></span><span style=display:flex><span>    device_map<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>    quantization_config<span style=color:#ff79c6>=</span>quantization_config, torch_dtype<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>)
</span></span></code></pre></div><p>load_in_4bit应用4位动态量化来极大地减少资源需求，也支持load_in_8bit</p><h1 id=llm大模型微调>LLM大模型微调</h1><p>为什么要进行大模型微调？大模型通常是针对通用场景来训练的，针对垂直领域的特定场景可能并不适用。所以需要对大模型参数进行微调，以便在特定场景下的效果更好。具体体现在这几个方面：提高准确性、提高效率、减少训练的数据需求。</p><h2 id=什么是transformer>什么是Transformer</h2><blockquote><p>Transformer是一种深度学习模型，最初由Google在2017年在论文《Attention is All You Need》中提出，主要用于自然语言处理任务，如机器翻译。它彻底改变了机器翻译领域的架构，突破了之前基于循环神经网络（RNN）和编码-解码模型（如LSTM和GRU）的局限性。</p></blockquote><blockquote><p>Transformer模型的核心特点是使用自注意力机制
（Self-Attention），这使得模型能够同时关注输入序列中的所有位置，而不仅仅是前一个或后一个位置，从而极大地提高了处理长序列数据的能力。它不再需要按序列顺序传递信息，而是对每个位置的输入进行独立的处理，降低了计算复杂度。</p></blockquote><p>Transformer模型通常由以下组件组成：</p><ol><li><strong>Encoder</strong>: 用来处理输入序列，将输入映射到一个高维空间，并产生一组表示（也称为编码器输出）。</li><li><strong>Decoder</strong>: 根据编码器输出和目标序列生成解码输出，用于机器翻译等任务。</li><li><strong>Multi-Head Attention</strong>: 在Encoder和Decoder中，使用多头自注意力来增强模型对输入序列的理解。</li><li><strong>Positional Encoding</strong>: 为输入序列添加位置信息，使得模型能够理解序列中的相对位置。</li><li><strong>Feed-Forward Networks (FFN)</strong>: 一个线性层后接一个激活函数（如ReLU），用于进一步处理自注意力和位置编码后的输出。</li></ol><p>由于其优秀的性能和对序列数据的强大处理能力，Transformer后来被广泛应用于语音识别、文本生成、问答系统等多个自然语言处理任务中。</p><p>BERT与Transformer的区别：</p><ul><li>BERT模型是Transformer的一种变体，采用双向编码器结构，而Transformer模型通常使用编码器-解码器结构。</li><li>BERT模型的预训练过程也与之有不同，采用了Masked Language Modeling和下一句预测任务。</li></ul><h2 id=qloralora微调技术>QLoRA/LoRA微调技术</h2><p>知乎有篇文章对大模型微调技术QLoRA、LoRA的介绍：<a href=https://zhuanlan.zhihu.com/p/671089942>https://zhuanlan.zhihu.com/p/671089942</a>，使用<a href=https://kimi.moonshot.cn/>https://kimi.moonshot.cn/</a>可以快速解读论文</p><p>LoRA和QLoRA是两种用于大型预训练模型(如Transformer模型)微调的技术：</p><p>LoRA（Low-Rank Adaptation）</p><ul><li>低秩矩阵近似：LoRA的核心思想是使用低秩矩阵来近似原始模型中的权重矩阵。低秩矩阵具有较少的参数，因此可以减少微调过程中的计算和存储需求。</li><li>微调过程：在微调阶段，LoRA不是直接更新原始模型的权重，而是更新这些低秩矩阵。这样可以在不显著增加模型大小和计算复杂度的情况下，对模型进行有效的微调。</li><li>适应性：通过这种方式，LoRA允许模型在保持预训练知识的同时，学习特定任务的特征，从而提高微调后模型在下游任务上的性能。</li></ul><p>QLoRA（Quantized Low-Rank Adaptation）</p><ul><li>量化：QLoRA是LoRA的扩展，它在低秩矩阵近似的基础上引入了量化技术。量化是一种压缩技术，可以进一步减少模型的参数数量和计算需求。</li><li>低秩量化矩阵：在QLoRA中，微调涉及的是一组量化后的低秩矩阵。这些矩阵在微调过程中被更新，以适应特定任务的需求。</li><li>效率与性能：QLoRA旨在实现与LoRA相似的适应性，同时通过量化进一步降低模型的内存占用和加速推理过程，使得模型更适合在资源受限的环境下部署。</li><li>应用场景：这些技术尤其适用于大型模型，如NLP领域的Transformer模型，它们在预训练阶段已经学习了大量的语言知识。通过LoRA或QLoRA微调，这些模型可以更有效地适应特定的下游任务，如文本分类、机器翻译或问答系统，而无需从头开始训练整个模型。</li></ul><p>LoRA和QLoRA通过减少微调过程中的参数数量和计算量，使得大型模型的微调变得更加高效，同时尽可能保留模型在预训练阶段获得的知识。这些技术对于推动大型模型在实际应用中的部署具有重要意义。</p><h2 id=llm微调项目unsloth>LLM微调项目unsloth</h2><p><a href=https://github.com/unslothai/unsloth>unsloth</a>是通过QLoRA、LoRA技术来加速大模型微调，能够提升2～5倍的速度并且减少80%的内存占用</p><p>主要特性：</p><ul><li>所有内核均采用OpenAI的Triton语言编写。采用手动反向传播引擎。</li><li>无精度损失，所有方法都是精确的。</li><li>支持自2018年起支持NVIDIA GPU。最低CUDA能力7.0（V100、T4、Titan V、RTX 20、30、40x、A100、H100、L40 等）GTX 1070、1080可以工作，但速度很慢。</li><li>在Linux和Windows的WSL上可以工作。</li><li>通过<a href=https://github.com/TimDettmers/bitsandbytes>bitsandbytes</a>支持4位和16位QLoRA/LoRA微调。</li><li>它的开源版本训练速度提高5倍，(Unsloth Pro)[https://unsloth.ai/]版本，训练速度可提高30倍</li></ul><h3 id=本地微调llama-3-8b>本地微调llama-3-8b</h3><p>参考youtube上的视频教程：<a href="https://www.youtube.com/watch?v=LPmI-Ok5fUc">https://www.youtube.com/watch?v=LPmI-Ok5fUc</a>，这里选用Meta开源的llama3-8B模型为例进行微调，llama3在各种基准测试中表现出色，属于先天优势。</p><h4 id=安装依赖>安装依赖</h4><p>安装nvidia驱动(这里用的是NVIDIA GeForce RTX 3090)、cuda</p><ul><li>nvidia驱动下载：<a href=https://www.nvidia.com/Download/index.aspx>https://www.nvidia.com/Download/index.aspx</a></li><li>cuda下载：<a href=https://developer.nvidia.com/cuda-toolkit-archive>https://developer.nvidia.com/cuda-toolkit-archive</a></li></ul><p>cuda的脚本执行完后，会在终端输出一些待操作的信息，需要设置环境变量才生效，以cuda 12.2版本为例</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>export</span> <span style=color:#8be9fd;font-style:italic>PATH</span><span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>$PATH</span>:/usr/local/cuda-12.2/bin
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>export</span> <span style=color:#8be9fd;font-style:italic>LD_LIBRARY_PATH</span><span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>$LD_LIBRARY_PATH</span>:/usr/local/cuda-12.2/lib64
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># nvcc -V</span>
</span></span><span style=display:flex><span>nvcc: NVIDIA <span style=color:#ff79c6>(</span>R<span style=color:#ff79c6>)</span> Cuda compiler driver
</span></span><span style=display:flex><span>Copyright <span style=color:#ff79c6>(</span>c<span style=color:#ff79c6>)</span> 2005-2023 NVIDIA Corporation
</span></span><span style=display:flex><span>Built on Tue_Jun_13_19:16:58_PDT_2023
</span></span><span style=display:flex><span>Cuda compilation tools, release 12.2, V12.2.91
</span></span><span style=display:flex><span>Build cuda_12.2.r12.2/compiler.32965470_0
</span></span></code></pre></div><h4 id=安装微调库>安装微调库</h4><p>使用conda安装微调环境</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda create --name unsloth_env <span style=color:#8be9fd;font-style:italic>python</span><span style=color:#ff79c6>=</span>3.10
</span></span><span style=display:flex><span>conda activate unsloth_env
</span></span><span style=display:flex><span>conda install pytorch-cuda<span style=color:#ff79c6>=</span>12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers
</span></span><span style=display:flex><span>pip install <span style=color:#f1fa8c>&#34;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git&#34;</span>
</span></span><span style=display:flex><span>pip install --no-deps trl peft accelerate bitsandbytes
</span></span></code></pre></div><h4 id=安装jupyter可选>安装Jupyter(可选)</h4><p>安装jupyter，也可以使用ipython在终端实现交互式python效果</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda install -c anaconda ipykernel jupyter chardet
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 在ipykernel中添加虚拟环境，以便在jupyter notebook中使用conda虚拟环境</span>
</span></span><span style=display:flex><span>python -m ipykernel install --user --name<span style=color:#ff79c6>=</span>unsloth_env
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># 查看jupyter kernel列表，是否生效</span>
</span></span><span style=display:flex><span>jupyter kernelspec list
</span></span><span style=display:flex><span>0.00s - Debugger warning: It seems that frozen modules are being used, which may
</span></span><span style=display:flex><span>0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules<span style=color:#ff79c6>=</span>off
</span></span><span style=display:flex><span>0.00s - to python to disable frozen modules.
</span></span><span style=display:flex><span>0.00s - Note: Debugging will proceed. Set <span style=color:#8be9fd;font-style:italic>PYDEVD_DISABLE_FILE_VALIDATION</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span> to disable this validation.
</span></span><span style=display:flex><span>Available kernels:
</span></span><span style=display:flex><span>  unsloth_env    /root/.local/share/jupyter/kernels/unsloth_env
</span></span><span style=display:flex><span>  python3        /root/anaconda3/share/jupyter/kernels/python3
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># 启动jupyter，默认监听在&lt;server-ip&gt;:8888上</span>
</span></span><span style=display:flex><span>jupyter notebook --ip<span style=color:#ff79c6>=</span>&lt;server-ip&gt; --allow-root
</span></span></code></pre></div><h4 id=加载模型>加载模型</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 加载模型</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> unsloth <span style=color:#ff79c6>import</span> FastLanguageModel
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> torch
</span></span><span style=display:flex><span>max_seq_length <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2048</span> <span style=color:#6272a4># Choose any! We auto support RoPE Scaling internally!</span>
</span></span><span style=display:flex><span>dtype <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span> <span style=color:#6272a4># None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+</span>
</span></span><span style=display:flex><span>load_in_4bit <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span> <span style=color:#6272a4># Use 4bit quantization to reduce memory usage. Can be False.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model, tokenizer <span style=color:#ff79c6>=</span> FastLanguageModel<span style=color:#ff79c6>.</span>from_pretrained(
</span></span><span style=display:flex><span>    model_name <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;unsloth/llama-3-8b-bnb-4bit&#34;</span>, <span style=color:#6272a4># HuggingFace上的模型路径，HuggingFace上unsloth也提供基础的instruct量化4位模型，要使用替换相应路径即可</span>
</span></span><span style=display:flex><span>    max_seq_length <span style=color:#ff79c6>=</span> max_seq_length,
</span></span><span style=display:flex><span>    dtype <span style=color:#ff79c6>=</span> dtype,
</span></span><span style=display:flex><span>    load_in_4bit <span style=color:#ff79c6>=</span> load_in_4bit,
</span></span><span style=display:flex><span>    <span style=color:#6272a4># token = &#34;hf_...&#34;, # use one if using gated models like meta-llama/Llama-2-7b-hf</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h4 id=微调前测试>微调前测试</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 微调前测试</span>
</span></span><span style=display:flex><span>alpaca_prompt <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Instruction:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Input:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Response:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>&#34;&#34;&#34;</span> <span style=color:#6272a4># 定义prompt结构</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>FastLanguageModel<span style=color:#ff79c6>.</span>for_inference(model) <span style=color:#6272a4># Enable native 2x faster inference</span>
</span></span><span style=display:flex><span>inputs <span style=color:#ff79c6>=</span> tokenizer(
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>    alpaca_prompt<span style=color:#ff79c6>.</span>format(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;请使用中文回答问题&#34;</span>, <span style=color:#6272a4># instruction</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;海绵宝宝的书法是不是叫做海绵体?&#34;</span>, <span style=color:#6272a4># input</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;</span>, <span style=color:#6272a4># output - leave this blank for generation!</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>], return_tensors <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;pt&#34;</span>)<span style=color:#ff79c6>.</span>to(<span style=color:#f1fa8c>&#34;cuda&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> TextStreamer  <span style=color:#6272a4># TextStreamer可以实时输出</span>
</span></span><span style=display:flex><span>text_streamer <span style=color:#ff79c6>=</span> TextStreamer(tokenizer)
</span></span><span style=display:flex><span>_ <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>generate(<span style=color:#ff79c6>**</span>inputs, streamer <span style=color:#ff79c6>=</span> text_streamer, max_new_tokens <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>128</span>)
</span></span></code></pre></div><h4 id=准备微调数据集>准备微调数据集</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 准备微调数据集</span>
</span></span><span style=display:flex><span>EOS_TOKEN <span style=color:#ff79c6>=</span> tokenizer<span style=color:#ff79c6>.</span>eos_token <span style=color:#6272a4># 必须添加EOS_TOKEN，是一个特殊标记，用于表示句子或序列的结束</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>formatting_prompts_func</span>(examples):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 跟数据集的格式结构一致</span>
</span></span><span style=display:flex><span>    instructions <span style=color:#ff79c6>=</span> examples[<span style=color:#f1fa8c>&#34;instruction&#34;</span>]
</span></span><span style=display:flex><span>    inputs       <span style=color:#ff79c6>=</span> examples[<span style=color:#f1fa8c>&#34;input&#34;</span>]
</span></span><span style=display:flex><span>    outputs      <span style=color:#ff79c6>=</span> examples[<span style=color:#f1fa8c>&#34;output&#34;</span>]
</span></span><span style=display:flex><span>    texts <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> instruction, <span style=color:#8be9fd;font-style:italic>input</span>, output <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>zip</span>(instructions, inputs, outputs):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 必须添加EOS_TOKEN, 否则无限生成</span>
</span></span><span style=display:flex><span>        text <span style=color:#ff79c6>=</span> alpaca_prompt<span style=color:#ff79c6>.</span>format(instruction, <span style=color:#8be9fd;font-style:italic>input</span>, output) <span style=color:#ff79c6>+</span> EOS_TOKEN
</span></span><span style=display:flex><span>        texts<span style=color:#ff79c6>.</span>append(text)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> { <span style=color:#f1fa8c>&#34;text&#34;</span> : texts}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> datasets <span style=color:#ff79c6>import</span> load_dataset
</span></span><span style=display:flex><span><span style=color:#6272a4># 选用中文数据集，内容来自于百度贴吧-弱智吧</span>
</span></span><span style=display:flex><span>dataset <span style=color:#ff79c6>=</span> load_dataset(<span style=color:#f1fa8c>&#34;kigner/ruozhiba-llama3&#34;</span>, split <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;train&#34;</span>)  <span style=color:#6272a4># 从HuggingFace上下载数据集，也支持加载本地的数据集</span>
</span></span><span style=display:flex><span>dataset <span style=color:#ff79c6>=</span> dataset<span style=color:#ff79c6>.</span>map(formatting_prompts_func, batched <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>)
</span></span></code></pre></div><h4 id=设置微调参数>设置微调参数</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 设置微调参数，微调方法选择LoRA</span>
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> FastLanguageModel<span style=color:#ff79c6>.</span>get_peft_model(
</span></span><span style=display:flex><span>    model,
</span></span><span style=display:flex><span>    r <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>16</span>, <span style=color:#6272a4># 建议8, 16, 32, 64, 128</span>
</span></span><span style=display:flex><span>    target_modules <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#34;q_proj&#34;</span>, <span style=color:#f1fa8c>&#34;k_proj&#34;</span>, <span style=color:#f1fa8c>&#34;v_proj&#34;</span>, <span style=color:#f1fa8c>&#34;o_proj&#34;</span>,
</span></span><span style=display:flex><span>                      <span style=color:#f1fa8c>&#34;gate_proj&#34;</span>, <span style=color:#f1fa8c>&#34;up_proj&#34;</span>, <span style=color:#f1fa8c>&#34;down_proj&#34;</span>,],
</span></span><span style=display:flex><span>    lora_alpha <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>16</span>,
</span></span><span style=display:flex><span>    lora_dropout <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>, <span style=color:#6272a4># Supports any, but = 0 is optimized</span>
</span></span><span style=display:flex><span>    bias <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;none&#34;</span>,    <span style=color:#6272a4># Supports any, but = &#34;none&#34; is optimized</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># [NEW] &#34;unsloth&#34; uses 30% less VRAM, fits 2x larger batch sizes!</span>
</span></span><span style=display:flex><span>    use_gradient_checkpointing <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;unsloth&#34;</span>, <span style=color:#6272a4># 梯度检查点，True or &#34;unsloth&#34; for very long context</span>
</span></span><span style=display:flex><span>    random_state <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>3407</span>,
</span></span><span style=display:flex><span>    use_rslora <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>,  <span style=color:#6272a4># We support rank stabilized LoRA</span>
</span></span><span style=display:flex><span>    loftq_config <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>, <span style=color:#6272a4># </span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> trl <span style=color:#ff79c6>import</span> SFTTrainer
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> TrainingArguments
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 使用SFT微调，unsloth支持SFT、DPO微调</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 更多详细介绍: https://huggingface.co/docs/trl/main/en/index</span>
</span></span><span style=display:flex><span>trainer <span style=color:#ff79c6>=</span> SFTTrainer(
</span></span><span style=display:flex><span>    model <span style=color:#ff79c6>=</span> model,
</span></span><span style=display:flex><span>    tokenizer <span style=color:#ff79c6>=</span> tokenizer,
</span></span><span style=display:flex><span>    train_dataset <span style=color:#ff79c6>=</span> dataset,
</span></span><span style=display:flex><span>    dataset_text_field <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;text&#34;</span>,  <span style=color:#6272a4># 跟数据集定义的map函数formatting_prompts_func返回的key一致</span>
</span></span><span style=display:flex><span>    max_seq_length <span style=color:#ff79c6>=</span> max_seq_length,
</span></span><span style=display:flex><span>    dataset_num_proc <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2</span>,
</span></span><span style=display:flex><span>    packing <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>, <span style=color:#6272a4># Can make training 5x faster for short sequences.</span>
</span></span><span style=display:flex><span>    args <span style=color:#ff79c6>=</span> TrainingArguments(
</span></span><span style=display:flex><span>        per_device_train_batch_size <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2</span>,  <span style=color:#6272a4># 控制训练的批量大小</span>
</span></span><span style=display:flex><span>        gradient_accumulation_steps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>4</span>, <span style=color:#6272a4># 梯度累积技术通过分割数据为更小的子批量来实现模拟大批量训练的效果。如果将per_device_train_batch_size设为4且gradient_accumulation_steps设为2，则最终的总批量大小实际上是8（4乘以2）</span>
</span></span><span style=display:flex><span>        warmup_steps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>5</span>,   <span style=color:#6272a4># 是针对学习率learning rate优化的一种策略，在训练初期使用较小的学习率（从0开始），在一定步数（比如1000步）内逐渐提高到正常大小(设置的learning_rate)，避免模型过早进入局部最优而过拟合；在训练后期再慢慢将学习率降低到 0，避免后期训练还出现较大的参数变化</span>
</span></span><span style=display:flex><span>        max_steps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>60</span>,     <span style=color:#6272a4># 微调步数，影响微调后模型的输出质量，-1为无限制，根据数据集大小来</span>
</span></span><span style=display:flex><span>        learning_rate <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2e-4</span>,   <span style=color:#6272a4># 学习率是决定模型在训练期间如何更新其权重的关键超参数。</span>
</span></span><span style=display:flex><span>        fp16 <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>not</span> torch<span style=color:#ff79c6>.</span>cuda<span style=color:#ff79c6>.</span>is_bf16_supported(),
</span></span><span style=display:flex><span>        bf16 <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>cuda<span style=color:#ff79c6>.</span>is_bf16_supported(), <span style=color:#6272a4># bfloat16的设计允许处理更广泛的数值范围，而不会显著牺牲计算精度</span>
</span></span><span style=display:flex><span>        logging_steps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>        optim <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;adamw_8bit&#34;</span>,  <span style=color:#6272a4># 优化器的作用在于引导模型训练过程，通过最小化误差或提升准确性来进行微调。8位分页AdamW与LoRA可以显著降低AdamW的总内存消耗</span>
</span></span><span style=display:flex><span>        weight_decay <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.01</span>, <span style=color:#6272a4># 权重衰减是一种鼓励模型维持较小权重值的技术，通过这种方式实现对模型的正则化，以避免复杂度过高的模型。有助于模型不过分依赖于任何单一的输入特征</span>
</span></span><span style=display:flex><span>        lr_scheduler_type <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;linear&#34;</span>, <span style=color:#6272a4># 最常用的还是线性两段式调整学习率</span>
</span></span><span style=display:flex><span>        seed <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>3407</span>,
</span></span><span style=display:flex><span>        output_dir <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;outputs&#34;</span>,   <span style=color:#6272a4># 模型训练的输出目录</span>
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h4 id=开始训练模型>开始训练模型</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 开始训练</span>
</span></span><span style=display:flex><span>trainer_stats <span style=color:#ff79c6>=</span> trainer<span style=color:#ff79c6>.</span>train()
</span></span></code></pre></div><h4 id=验证微调模型>验证微调模型</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>FastLanguageModel<span style=color:#ff79c6>.</span>for_inference(model) <span style=color:#6272a4># Enable native 2x faster inference</span>
</span></span><span style=display:flex><span>inputs <span style=color:#ff79c6>=</span> tokenizer(
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>    alpaca_prompt<span style=color:#ff79c6>.</span>format(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;请使用中文回答问题&#34;</span>, <span style=color:#6272a4># instruction</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;海绵宝宝的书法是不是叫做海绵体?&#34;</span>, <span style=color:#6272a4># input</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;</span>, <span style=color:#6272a4># output - leave this blank for generation!</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>], return_tensors <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;pt&#34;</span>)<span style=color:#ff79c6>.</span>to(<span style=color:#f1fa8c>&#34;cuda&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> TextStreamer  <span style=color:#6272a4># TextStreamer可以实时输出</span>
</span></span><span style=display:flex><span>text_streamer <span style=color:#ff79c6>=</span> TextStreamer(tokenizer)
</span></span><span style=display:flex><span>_ <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>generate(<span style=color:#ff79c6>**</span>inputs, streamer <span style=color:#ff79c6>=</span> text_streamer, max_new_tokens <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>128</span>)
</span></span></code></pre></div><h4 id=保存微调模型>保存微调模型</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 保存训练后的LoRA模型，并不是一个完整的模型</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>save_pretrained(<span style=color:#f1fa8c>&#34;lora_model&#34;</span>) <span style=color:#6272a4># Local saving</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#ff79c6>.</span>save_pretrained(<span style=color:#f1fa8c>&#34;lora_model&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># model.push_to_hub(&#34;your_name/lora_model&#34;, token = &#34;...&#34;) # Online saving，推送到HuggingFace上</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># tokenizer.push_to_hub(&#34;your_name/lora_model&#34;, token = &#34;...&#34;) # Online saving，推送到HuggingFace上</span>
</span></span></code></pre></div><h4 id=合并模型>合并模型</h4><p>合并模型是指将微调后的模型和原始模型进行合并，假设这里最终模型的保存格式为：4bit GGUF格式，其中经过的转换过程是：16bit HF格式 -> 16bit GGUF格式 -> 4bit GGUF格式</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 保存为HF格式，vLLM可加载使用</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>save_pretrained_merged(<span style=color:#f1fa8c>&#34;model&#34;</span>, tokenizer, save_method <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;merged_16bit&#34;</span>,)  <span style=color:#6272a4># 也可选保存为其它精度格式，如4bit </span>
</span></span><span style=display:flex><span><span style=color:#6272a4># model.push_to_hub_merged(&#34;hf/model&#34;, tokenizer, save_method = &#34;merged_16bit&#34;, token = &#34;&#34;) # 是否推送到HuggingFace上</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 保存为4bit GGUF格式，HF格式到GGUF格式的转换是集成了llama.cpp来实现的</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>save_pretrained_gguf(<span style=color:#f1fa8c>&#34;model&#34;</span>, tokenizer, quantization_method <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;q4_k_m&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># model.push_to_hub_gguf(&#34;hf/model&#34;, tokenizer, quantization_method = &#34;q4_k_m&#34;, token = &#34;&#34;) 是否推送到HuggingFace上</span>
</span></span></code></pre></div><p>如果遇到转换模型格式失败，需要重新编译下llama.cpp，然后再执行一遍程序</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/ggerganov/llama.cpp
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>cd</span> llama.cpp <span style=color:#ff79c6>&amp;&amp;</span> make clean <span style=color:#ff79c6>&amp;&amp;</span> <span style=color:#8be9fd;font-style:italic>LLAMA_CUDA</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span> make all -j
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 如果编译失败，提示缺少c++，ubuntu的话需要装下build-essential包</span>
</span></span><span style=display:flex><span>apt install build-essential -y
</span></span></code></pre></div><h4 id=本地加载gguf模型验证>本地加载GGUF模型验证</h4><p>GGUG模型可使用<a href=https://gpt4all.io/index.html>gpt4all</a>加载查看效果
<img src=https://raw.githubusercontent.com/yaoice/blog-images/main/2024-01-18-llm_develop_readnotes/20240118-gpt4all-model-verify.png alt>
可以看到除了数据集中的返回回答外，模型还会衍生发散</p><h3 id=云端colab微调phi-3-mini-4k>云端colab微调phi-3-mini-4k</h3><p>参考youtube上的视频教程：<a href="https://www.youtube.com/watch?v=JIaWgRStuXA&amp;t=9s">https://www.youtube.com/watch?v=JIaWgRStuXA&amp;t=9s</a></p><p>Colab是Google Cloud的一项托管式Jupyter笔记本服务，有免费额度提供使用，运行代码执行程序时请修改运行时类型为T4 GPU. phi-3-mini是微软开源的3.8B个参数的小模型，提供两个版本：4k和128k token的上下文长度。</p><h4 id=安装微调库-1>安装微调库</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>%%capture
</span></span><span style=display:flex><span><span style=color:#6272a4># Installs Unsloth, Xformers (Flash Attention) and all other packages!</span>
</span></span><span style=display:flex><span>!pip install <span style=color:#f1fa8c>&#34;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git&#34;</span>
</span></span><span style=display:flex><span>!pip install --no-deps <span style=color:#f1fa8c>&#34;xformers&lt;0.0.26&#34;</span> trl peft accelerate bitsandbytes
</span></span></code></pre></div><p><code>%%capture</code>是Colab的Magic命令，用来阻止特定单元格的输出；<code>!</code>符号后面可以接shell命令</p><h4 id=加载模型-1>加载模型</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> unsloth <span style=color:#ff79c6>import</span> FastLanguageModel
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> torch
</span></span><span style=display:flex><span>max_seq_length <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2048</span> <span style=color:#6272a4># Choose any! We auto support RoPE Scaling internally!</span>
</span></span><span style=display:flex><span>dtype <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span> <span style=color:#6272a4># None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+</span>
</span></span><span style=display:flex><span>load_in_4bit <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span> <span style=color:#6272a4># Use 4bit quantization to reduce memory usage. Can be False.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model, tokenizer <span style=color:#ff79c6>=</span> FastLanguageModel<span style=color:#ff79c6>.</span>from_pretrained(
</span></span><span style=display:flex><span>    model_name <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;microsoft/Phi-3-mini-4k-instruct&#34;</span>,
</span></span><span style=display:flex><span>    max_seq_length <span style=color:#ff79c6>=</span> max_seq_length,
</span></span><span style=display:flex><span>    dtype <span style=color:#ff79c6>=</span> dtype,
</span></span><span style=display:flex><span>    load_in_4bit <span style=color:#ff79c6>=</span> load_in_4bit,
</span></span><span style=display:flex><span>    <span style=color:#6272a4># token = &#34;hf_...&#34;, # use one if using gated models like meta-llama/Llama-2-7b-hf</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h4 id=微调前测试-1>微调前测试</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 微调前测试</span>
</span></span><span style=display:flex><span>alpaca_prompt <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Instruction:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Input:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Response:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>&#34;&#34;&#34;</span> <span style=color:#6272a4># 定义prompt结构</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>FastLanguageModel<span style=color:#ff79c6>.</span>for_inference(model) <span style=color:#6272a4># Enable native 2x faster inference</span>
</span></span><span style=display:flex><span>inputs <span style=color:#ff79c6>=</span> tokenizer(
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>    alpaca_prompt<span style=color:#ff79c6>.</span>format(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;请使用中文回答问题&#34;</span>, <span style=color:#6272a4># instruction</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;海绵宝宝的书法是不是叫做海绵体?&#34;</span>, <span style=color:#6272a4># input</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;</span>, <span style=color:#6272a4># output - leave this blank for generation!</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>], return_tensors <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;pt&#34;</span>)<span style=color:#ff79c6>.</span>to(<span style=color:#f1fa8c>&#34;cuda&#34;</span>)   <span style=color:#6272a4># tokenizer可以返回实际输入到模型的张量(tensor)，张量实际上是多维数组，pt是PyTorch，tf是TensorFlow</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> TextStreamer  <span style=color:#6272a4># TextStreamer可以实时输出</span>
</span></span><span style=display:flex><span>text_streamer <span style=color:#ff79c6>=</span> TextStreamer(tokenizer)
</span></span><span style=display:flex><span>_ <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>generate(<span style=color:#ff79c6>**</span>inputs, streamer <span style=color:#ff79c6>=</span> text_streamer, max_new_tokens <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>128</span>)
</span></span></code></pre></div><h4 id=准备微调数据集-1>准备微调数据集</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4>#准备微调数据集</span>
</span></span><span style=display:flex><span>EOS_TOKEN <span style=color:#ff79c6>=</span> tokenizer<span style=color:#ff79c6>.</span>eos_token <span style=color:#6272a4># 必须添加EOS_TOKEN，是一个特殊标记，用于表示句子或序列的结束</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 这个实际上是个数据预处理函数</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>formatting_prompts_func</span>(examples):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 跟数据集的格式结构一致 </span>
</span></span><span style=display:flex><span>    instructions <span style=color:#ff79c6>=</span> examples[<span style=color:#f1fa8c>&#34;instruction&#34;</span>]
</span></span><span style=display:flex><span>    inputs       <span style=color:#ff79c6>=</span> examples[<span style=color:#f1fa8c>&#34;input&#34;</span>]
</span></span><span style=display:flex><span>    outputs      <span style=color:#ff79c6>=</span> examples[<span style=color:#f1fa8c>&#34;output&#34;</span>]
</span></span><span style=display:flex><span>    texts <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> instruction, <span style=color:#8be9fd;font-style:italic>input</span>, output <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>zip</span>(instructions, inputs, outputs):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 必须添加EOS_TOKEN, 否则无限生成</span>
</span></span><span style=display:flex><span>        text <span style=color:#ff79c6>=</span> alpaca_prompt<span style=color:#ff79c6>.</span>format(instruction, <span style=color:#8be9fd;font-style:italic>input</span>, output) <span style=color:#ff79c6>+</span> EOS_TOKEN
</span></span><span style=display:flex><span>        texts<span style=color:#ff79c6>.</span>append(text)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> { <span style=color:#f1fa8c>&#34;text&#34;</span> : texts, }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> datasets <span style=color:#ff79c6>import</span> load_dataset
</span></span><span style=display:flex><span><span style=color:#6272a4># 选用中文数据集，内容来自于百度贴吧-弱智吧</span>
</span></span><span style=display:flex><span>dataset <span style=color:#ff79c6>=</span> load_dataset(<span style=color:#f1fa8c>&#34;kigner/ruozhiba-llama3&#34;</span>, split <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;train&#34;</span>)  <span style=color:#6272a4># 从HuggingFace上下载数据集，也支持加载本地的数据集</span>
</span></span><span style=display:flex><span>dataset <span style=color:#ff79c6>=</span> dataset<span style=color:#ff79c6>.</span>map(formatting_prompts_func, batched <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>)
</span></span></code></pre></div><p>自定义数据格式：多项选择数据集</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 多项选择题数据集</span>
</span></span><span style=display:flex><span>dataset <span style=color:#ff79c6>=</span> load_dataset(<span style=color:#f1fa8c>&#34;TheFinAI/flare-zh-fineval&#34;</span>, split<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;test&#34;</span>)
</span></span><span style=display:flex><span>dataset <span style=color:#ff79c6>=</span> dataset<span style=color:#ff79c6>.</span>remove_columns([<span style=color:#f1fa8c>&#34;text&#34;</span>, <span style=color:#f1fa8c>&#34;id&#34;</span>, <span style=color:#f1fa8c>&#34;choices&#34;</span>, <span style=color:#f1fa8c>&#34;answer&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 微调前测试</span>
</span></span><span style=display:flex><span>alpaca_prompt <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;The following is a multiple-choice question, and the correct answer will be returned according to the requirements; ### Instruction: Below is the specific requirement, ### Input: Below is the specific question, with four options; ### Response: Below is the correct answer, please Return ### Response: The answer below.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Instruction:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Input:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Response:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>&#34;&#34;&#34;</span> <span style=color:#6272a4># 定义prompt结构</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>EOS_TOKEN <span style=color:#ff79c6>=</span> tokenizer<span style=color:#ff79c6>.</span>eos_token
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>inputs <span style=color:#ff79c6>=</span> tokenizer(
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>    alpaca_prompt<span style=color:#ff79c6>.</span>format(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#39;对于以下填空题，请从下面选项中选择一个正确的答案。你的答案应该为：&#34;A&#34;、&#34;B&#34;、&#34;C&#34;或 &#34;D&#34;&#39;</span>, <span style=color:#6272a4># instruction</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;Context:下列各项税法原则中，属于税法基本原则核心的是____。</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>A,税收公平原则</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>B,税收效率原则</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>C,实质课税原则</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>D,税收法定原则&#34;</span>, <span style=color:#6272a4># input</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;</span>, <span style=color:#6272a4># output - leave this blank for generation!</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>], return_tensors <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;pt&#34;</span>)<span style=color:#ff79c6>.</span>to(<span style=color:#f1fa8c>&#34;cuda&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> TextStreamer  <span style=color:#6272a4># TextStreamer可以实时输出</span>
</span></span><span style=display:flex><span>text_streamer <span style=color:#ff79c6>=</span> TextStreamer(tokenizer)
</span></span><span style=display:flex><span>_ <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>generate(<span style=color:#ff79c6>**</span>inputs, streamer <span style=color:#ff79c6>=</span> text_streamer, max_new_tokens <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>128</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 数据预处理函数</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>formatting_prompts_func</span>(examples):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 跟数据集的格式结构一致</span>
</span></span><span style=display:flex><span>    golds        <span style=color:#ff79c6>=</span> examples[<span style=color:#f1fa8c>&#34;gold&#34;</span>]
</span></span><span style=display:flex><span>    queries      <span style=color:#ff79c6>=</span> examples[<span style=color:#f1fa8c>&#34;query&#34;</span>]
</span></span><span style=display:flex><span>    texts <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> gold, query <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>zip</span>(golds, queries):
</span></span><span style=display:flex><span>        query_without_unused_str <span style=color:#ff79c6>=</span> query<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#34;Answer:&#34;</span>)[<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>        context_list <span style=color:#ff79c6>=</span> query_without_unused_str<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>        instruction <span style=color:#ff79c6>=</span> context_list[<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>input</span>       <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span><span style=color:#ff79c6>.</span>join(context_list[<span style=color:#bd93f9>1</span>:])
</span></span><span style=display:flex><span>        output      <span style=color:#ff79c6>=</span> context_list[gold<span style=color:#ff79c6>+</span><span style=color:#bd93f9>2</span>]
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 必须添加EOS_TOKEN, 否则无限生成</span>
</span></span><span style=display:flex><span>        text <span style=color:#ff79c6>=</span> alpaca_prompt<span style=color:#ff79c6>.</span>format(instruction, <span style=color:#8be9fd;font-style:italic>input</span>, output) <span style=color:#ff79c6>+</span> EOS_TOKEN
</span></span><span style=display:flex><span>        texts<span style=color:#ff79c6>.</span>append(text)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> { <span style=color:#f1fa8c>&#34;text&#34;</span> : texts}
</span></span></code></pre></div><h4 id=设置微调参数-1>设置微调参数</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 设置微调参数，微调方法选择LoRA</span>
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> FastLanguageModel<span style=color:#ff79c6>.</span>get_peft_model(
</span></span><span style=display:flex><span>    model,
</span></span><span style=display:flex><span>    r <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>16</span>, <span style=color:#6272a4># 建议8, 16, 32, 64, 128</span>
</span></span><span style=display:flex><span>    target_modules <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#34;q_proj&#34;</span>, <span style=color:#f1fa8c>&#34;k_proj&#34;</span>, <span style=color:#f1fa8c>&#34;v_proj&#34;</span>, <span style=color:#f1fa8c>&#34;o_proj&#34;</span>,
</span></span><span style=display:flex><span>                      <span style=color:#f1fa8c>&#34;gate_proj&#34;</span>, <span style=color:#f1fa8c>&#34;up_proj&#34;</span>, <span style=color:#f1fa8c>&#34;down_proj&#34;</span>,],
</span></span><span style=display:flex><span>    lora_alpha <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>16</span>,
</span></span><span style=display:flex><span>    lora_dropout <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>, <span style=color:#6272a4># Supports any, but = 0 is optimized</span>
</span></span><span style=display:flex><span>    bias <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;none&#34;</span>,    <span style=color:#6272a4># Supports any, but = &#34;none&#34; is optimized</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># [NEW] &#34;unsloth&#34; uses 30% less VRAM, fits 2x larger batch sizes!</span>
</span></span><span style=display:flex><span>    use_gradient_checkpointing <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;unsloth&#34;</span>, <span style=color:#6272a4># 梯度检查点，True or &#34;unsloth&#34; for very long context</span>
</span></span><span style=display:flex><span>    random_state <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>3407</span>,
</span></span><span style=display:flex><span>    use_rslora <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>,  <span style=color:#6272a4># We support rank stabilized LoRA</span>
</span></span><span style=display:flex><span>    loftq_config <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>, <span style=color:#6272a4># LoftQ是一种针对大型语言模型（LLMs）的量化框架，主要作用是减少大型预训练模型的存储和计算资源需求</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> trl <span style=color:#ff79c6>import</span> SFTTrainer
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> TrainingArguments
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 使用SFT微调，unsloth支持SFT、DPO微调</span>
</span></span><span style=display:flex><span>trainer <span style=color:#ff79c6>=</span> SFTTrainer(
</span></span><span style=display:flex><span>    model <span style=color:#ff79c6>=</span> model,
</span></span><span style=display:flex><span>    tokenizer <span style=color:#ff79c6>=</span> tokenizer,
</span></span><span style=display:flex><span>    train_dataset <span style=color:#ff79c6>=</span> dataset,
</span></span><span style=display:flex><span>    dataset_text_field <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;text&#34;</span>,   <span style=color:#6272a4># 跟数据集定义的map函数formatting_prompts_func返回的key一致</span>
</span></span><span style=display:flex><span>    max_seq_length <span style=color:#ff79c6>=</span> max_seq_length,
</span></span><span style=display:flex><span>    dataset_num_proc <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2</span>,   <span style=color:#6272a4># 用于tokenize数据集的worker数量，packing=False时生效</span>
</span></span><span style=display:flex><span>    packing <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>, <span style=color:#6272a4># Can make training 5x faster for short sequences.</span>
</span></span><span style=display:flex><span>    args <span style=color:#ff79c6>=</span> TrainingArguments(
</span></span><span style=display:flex><span>        <span style=color:#6272a4># num_train_epochs=3,  训练的总轮数</span>
</span></span><span style=display:flex><span>        per_device_train_batch_size <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2</span>,  <span style=color:#6272a4># 控制训练的批量大小</span>
</span></span><span style=display:flex><span>        gradient_accumulation_steps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>4</span>, <span style=color:#6272a4># 梯度累积技术通过分割数据为更小的子批量来实现模拟大批量训练的效果。如果将per_device_train_batch_size设为4且gradient_accumulation_steps设为2，则最终的总批量大小实际上是8（4乘以2）</span>
</span></span><span style=display:flex><span>        warmup_steps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>5</span>,   <span style=color:#6272a4># 是针对学习率learning rate优化的一种策略，在训练初期使用较小的学习率（从0开始），在一定步数（比如1000步）内逐渐提高到正常大小(设置的learning_rate)，避免模型过早进入局部最优而过拟合；在训练后期再慢慢将学习率降低到 0，避免后期训练还出现较大的参数变化</span>
</span></span><span style=display:flex><span>        max_steps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>60</span>,     <span style=color:#6272a4># 微调步数，影响微调后模型的输出质量，-1为无限制，根据数据集大小来</span>
</span></span><span style=display:flex><span>        learning_rate <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2e-4</span>,   <span style=color:#6272a4># 学习率是决定模型在训练期间如何更新其权重的关键超参数。</span>
</span></span><span style=display:flex><span>        fp16 <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>not</span> torch<span style=color:#ff79c6>.</span>cuda<span style=color:#ff79c6>.</span>is_bf16_supported(),
</span></span><span style=display:flex><span>        bf16 <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>cuda<span style=color:#ff79c6>.</span>is_bf16_supported(), <span style=color:#6272a4># bfloat16的设计允许处理更广泛的数值范围，而不会显著牺牲计算精度</span>
</span></span><span style=display:flex><span>        logging_steps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>        optim <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;adamw_8bit&#34;</span>,  <span style=color:#6272a4># 优化器的作用在于引导模型训练过程，通过最小化误差或提升准确性来进行微调。8位分页AdamW与LoRA可以显著降低AdamW的总内存消耗</span>
</span></span><span style=display:flex><span>        weight_decay <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.01</span>, <span style=color:#6272a4># 权重衰减是一种鼓励模型维持较小权重值的技术，通过这种方式实现对模型的正则化，以避免复杂度过高的模型。有助于模型不过分依赖于任何单一的输入特征</span>
</span></span><span style=display:flex><span>        lr_scheduler_type <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;linear&#34;</span>, <span style=color:#6272a4># 最常用的还是线性两段式调整学习率</span>
</span></span><span style=display:flex><span>        seed <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>3407</span>,  <span style=color:#6272a4># 设置随机数生成器的种子，以确保实验的可重复性子，为了确保结果的可重复性，在每次训练中使用相同的种子值，具体值并不重要</span>
</span></span><span style=display:flex><span>        output_dir <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;outputs&#34;</span>,   <span style=color:#6272a4># 模型训练的输出目录</span>
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h4 id=开始训练模型-1>开始训练模型</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer<span style=color:#ff79c6>.</span>padding_side <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;right&#39;</span>
</span></span><span style=display:flex><span>trainer_stats <span style=color:#ff79c6>=</span> trainer<span style=color:#ff79c6>.</span>train()
</span></span></code></pre></div><p>输出类似这种格式的数据，如果出现grad_norm为nan，说明在计算梯度范数时出现了数值问题</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>{&#39;loss&#39;: 0.8567, &#39;grad_norm&#39;: 0.5789440870285034, &#39;learning_rate&#39;: 0.0001039568345323741, &#39;epoch&#39;: 1.45}
</span></span></code></pre></div><h4 id=验证微调模型-1>验证微调模型</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>FastLanguageModel<span style=color:#ff79c6>.</span>for_inference(model) <span style=color:#6272a4># Enable native 2x faster inference</span>
</span></span><span style=display:flex><span>inputs <span style=color:#ff79c6>=</span> tokenizer(
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>    alpaca_prompt<span style=color:#ff79c6>.</span>format(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;请使用中文回答问题&#34;</span>, <span style=color:#6272a4># instruction</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;海绵宝宝的书法是不是叫做海绵体?&#34;</span>, <span style=color:#6272a4># input</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;</span>, <span style=color:#6272a4># output - leave this blank for generation!</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>], return_tensors <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;pt&#34;</span>)<span style=color:#ff79c6>.</span>to(<span style=color:#f1fa8c>&#34;cuda&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> TextStreamer  <span style=color:#6272a4># TextStreamer可以实时输出</span>
</span></span><span style=display:flex><span>text_streamer <span style=color:#ff79c6>=</span> TextStreamer(tokenizer)
</span></span><span style=display:flex><span>_ <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>generate(<span style=color:#ff79c6>**</span>inputs, streamer <span style=color:#ff79c6>=</span> text_streamer, max_new_tokens <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>128</span>)
</span></span></code></pre></div><h4 id=保存微调模型-1>保存微调模型</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 保存训练后的LoRA模型，并不是一个完整的模型</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>save_pretrained(<span style=color:#f1fa8c>&#34;phi-3-mini-lora-model&#34;</span>) <span style=color:#6272a4># Local saving</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#ff79c6>.</span>save_pretrained(<span style=color:#f1fa8c>&#34;phi-3-mini-lora-model&#34;</span>)
</span></span></code></pre></div><h4 id=合并模型-1>合并模型</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 保存为HF格式，vLLM可加载使用</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>save_pretrained_merged(<span style=color:#f1fa8c>&#34;phi-3-mini-model/&#34;</span>, tokenizer, save_method <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;merged_16bit&#34;</span>,)  <span style=color:#6272a4># 也可选保存为其它精度格式，如4bit </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 保存为16bit GGUF格式</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>save_pretrained_gguf(<span style=color:#f1fa8c>&#34;phi-3-mini-model/&#34;</span>, tokenizer, quantization_method <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;f16&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 保存为4bit GGUF格式，HF格式到GGUF格式的转换是集成了llama.cpp来实现的</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>save_pretrained_gguf(<span style=color:#f1fa8c>&#34;phi-3-mini-model/&#34;</span>, tokenizer, quantization_method <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;q4_k_m&#34;</span>)
</span></span></code></pre></div><p>在colab上使用llama.cpp转换格式一直未成功&mldr;&mldr;，不确定是不是colab终端环境问题</p><p>手动执行模型转换和量化步骤</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python llama.cpp/convert-hf-to-gguf.py --outfile phi-3-mini-4k-unsloth.fp16.gguf --outtype f16  phi-3-mini-model/
</span></span><span style=display:flex><span>./llama.cpp/quantize phi-3-mini-4k-unsloth.fp16.gguf phi-3-mini-4k-unsloth.Q4_K_M.gguf Q4_K_M
</span></span></code></pre></div><h4 id=本地加载gguf模型验证-1>本地加载GGUF模型验证</h4><p>GGUG模型使用<a href=https://gpt4all.io/index.html>gpt4all</a>加载查看效果
<img src=https://raw.githubusercontent.com/yaoice/blog-images/main/2024-01-18-llm_develop_readnotes/20240118-gpt4all-model-verify-phi-3-mini.png alt></p><h2 id=transformers-trainer微调>Transformers Trainer微调</h2><h3 id=加载模型-2>加载模型</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> AutoModelForCausalLM, BitsAndBytesConfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># bitsandbytes集成支持8bit和4bit精度数据类型，用于加载大模型可以节省内存</span>
</span></span><span style=display:flex><span>quantization_config <span style=color:#ff79c6>=</span> BitsAndBytesConfig(load_in_4bit<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> AutoModelForCausalLM<span style=color:#ff79c6>.</span>from_pretrained(<span style=color:#f1fa8c>&#34;microsoft/Phi-3-mini-4k-instruct&#34;</span>, 
</span></span><span style=display:flex><span>    trust_remote_code<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, 
</span></span><span style=display:flex><span>    device_map<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>    quantization_config<span style=color:#ff79c6>=</span>quantization_config, torch_dtype<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>)
</span></span></code></pre></div><p>load_in_4bit应用4位动态量化来极大地减少资源需求，也支持load_in_8bit</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 初始化分词器</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#ff79c6>=</span> AutoTokenizer<span style=color:#ff79c6>.</span>from_pretrained(<span style=color:#f1fa8c>&#34;microsoft/Phi-3-mini-4k-instruct&#34;</span>)
</span></span></code></pre></div><h3 id=微调前测试-2>微调前测试</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 微调前测试</span>
</span></span><span style=display:flex><span>alpaca_prompt <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Instruction:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Input:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>### Response:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{}</span><span style=color:#f1fa8c>&#34;&#34;&#34;</span> <span style=color:#6272a4># 定义prompt结构</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#ff79c6>.</span>pad_token <span style=color:#ff79c6>=</span> tokenizer<span style=color:#ff79c6>.</span>eos_token  <span style=color:#6272a4># 大多数模型默认没设置pad token</span>
</span></span><span style=display:flex><span>inputs <span style=color:#ff79c6>=</span> tokenizer(
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>    alpaca_prompt<span style=color:#ff79c6>.</span>format(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;请使用中文回答问题&#34;</span>, <span style=color:#6272a4># instruction</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;海绵宝宝的书法是不是叫做海绵体?&#34;</span>, <span style=color:#6272a4># input</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;</span>, <span style=color:#6272a4># output - leave this blank for generation!</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>], return_tensors <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;pt&#34;</span>, padding<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>, truncation<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)<span style=color:#ff79c6>.</span>to(<span style=color:#f1fa8c>&#34;cuda&#34;</span>)   <span style=color:#6272a4># tokenizer可以返回实际输入到模型的张量(tensor)，张量实际上是多维数组，pt是PyTorch，tf是TensorFlow</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> TextStreamer  <span style=color:#6272a4># TextStreamer可以实时输出</span>
</span></span><span style=display:flex><span>text_streamer <span style=color:#ff79c6>=</span> TextStreamer(tokenizer)
</span></span><span style=display:flex><span>generated_ids <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>generate(<span style=color:#ff79c6>**</span>inputs, streamer <span style=color:#ff79c6>=</span> text_streamer, max_new_tokens <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>128</span>)
</span></span></code></pre></div><p>通过tokenizer decode可以看到模型输出的文本内容</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer<span style=color:#ff79c6>.</span>batch_decode(generated_ids, skip_special_tokens<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span></code></pre></div><h3 id=加载peft-adapter>加载PEFT adapter</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>ValueError: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details
</span></span></code></pre></div><p>上面这段英文的意思是：对一个完全量化的模型进行微调，这是不被支持的。量化模型的参数是固定的，无法直接训练。因此，必须在量化模型上附加可训练的适配器(adapters)，这样才可以进行微调。这里推荐一种参数高效微调方法(PEFT)，在微调过程中冻结预训练模型的参数，并在其顶部添加少量可训练参数（adapters）。adapters被训练以学习特定任务的信息。这种方法已被证明非常节省内存，同时具有较低的计算使用量，同时产生与完全微调模型相当的结果。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> peft <span style=color:#ff79c6>import</span> LoraConfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lora_config <span style=color:#ff79c6>=</span> LoraConfig(
</span></span><span style=display:flex><span>    r <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>16</span>, <span style=color:#6272a4># 建议8, 16, 32, 64, 128</span>
</span></span><span style=display:flex><span>    target_modules <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#34;q_proj&#34;</span>, <span style=color:#f1fa8c>&#34;k_proj&#34;</span>, <span style=color:#f1fa8c>&#34;v_proj&#34;</span>, <span style=color:#f1fa8c>&#34;o_proj&#34;</span>,
</span></span><span style=display:flex><span>                      <span style=color:#f1fa8c>&#34;gate_proj&#34;</span>, <span style=color:#f1fa8c>&#34;up_proj&#34;</span>, <span style=color:#f1fa8c>&#34;down_proj&#34;</span>,],
</span></span><span style=display:flex><span>    lora_alpha <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>16</span>,
</span></span><span style=display:flex><span>    lora_dropout <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>, <span style=color:#6272a4># Supports any, but = 0 is optimized</span>
</span></span><span style=display:flex><span>    bias <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;none&#34;</span>,    <span style=color:#6272a4># Supports any, but = &#34;none&#34; is optimized</span>
</span></span><span style=display:flex><span>    use_rslora <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>,  <span style=color:#6272a4># We support rank stabilized LoRA</span>
</span></span><span style=display:flex><span>    loftq_config <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>, <span style=color:#6272a4># And LoftQ</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>add_adapter(lora_config, adapter_name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;adapter_1&#34;</span>)
</span></span></code></pre></div><h3 id=准备微调数据集-2>准备微调数据集</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4>#准备微调数据集</span>
</span></span><span style=display:flex><span>EOS_TOKEN <span style=color:#ff79c6>=</span> tokenizer<span style=color:#ff79c6>.</span>eos_token <span style=color:#6272a4># 必须添加EOS_TOKEN，是一个特殊标记，用于表示句子或序列的结束</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 这个实际上是个数据预处理函数</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>formatting_prompts_func</span>(examples):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 跟数据集的格式结构一致 </span>
</span></span><span style=display:flex><span>    instructions <span style=color:#ff79c6>=</span> examples[<span style=color:#f1fa8c>&#34;instruction&#34;</span>]
</span></span><span style=display:flex><span>    inputs       <span style=color:#ff79c6>=</span> examples[<span style=color:#f1fa8c>&#34;input&#34;</span>]
</span></span><span style=display:flex><span>    outputs      <span style=color:#ff79c6>=</span> examples[<span style=color:#f1fa8c>&#34;output&#34;</span>]
</span></span><span style=display:flex><span>    texts <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> instruction, <span style=color:#8be9fd;font-style:italic>input</span>, output <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>zip</span>(instructions, inputs, outputs):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 必须添加EOS_TOKEN, 否则无限生成</span>
</span></span><span style=display:flex><span>        text <span style=color:#ff79c6>=</span> alpaca_prompt<span style=color:#ff79c6>.</span>format(instruction, <span style=color:#8be9fd;font-style:italic>input</span>, output) <span style=color:#ff79c6>+</span> EOS_TOKEN
</span></span><span style=display:flex><span>        texts<span style=color:#ff79c6>.</span>append(text)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> { <span style=color:#f1fa8c>&#34;text&#34;</span> : texts}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> datasets <span style=color:#ff79c6>import</span> load_dataset
</span></span><span style=display:flex><span><span style=color:#6272a4># 选用中文数据集，内容来自于百度贴吧-弱智吧</span>
</span></span><span style=display:flex><span>dataset <span style=color:#ff79c6>=</span> load_dataset(<span style=color:#f1fa8c>&#34;kigner/ruozhiba-llama3&#34;</span>, split <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;train&#34;</span>)  <span style=color:#6272a4># 从HuggingFace上下载数据集，也支持加载本地的数据集</span>
</span></span><span style=display:flex><span>dataset <span style=color:#ff79c6>=</span> dataset<span style=color:#ff79c6>.</span>map(formatting_prompts_func, batched <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>)
</span></span></code></pre></div><h3 id=设置微调参数-2>设置微调参数</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> trl <span style=color:#ff79c6>import</span> SFTTrainer
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> TrainingArguments
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#ff79c6>.</span>padding_side <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#39;right&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 使用SFT Trainer，还有DPO、KTO、CPO、ORPO、Reward、PPO等其它Trainer. 想要使用自回归技术在文本数据集上微调像Llama、Mistral这样的语言模型，考虑使用trl的SFTTrainer</span>
</span></span><span style=display:flex><span>trainer <span style=color:#ff79c6>=</span> SFTTrainer(
</span></span><span style=display:flex><span>    model <span style=color:#ff79c6>=</span> model,
</span></span><span style=display:flex><span>    tokenizer <span style=color:#ff79c6>=</span> tokenizer,
</span></span><span style=display:flex><span>    train_dataset <span style=color:#ff79c6>=</span> dataset,
</span></span><span style=display:flex><span>    max_seq_length <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2048</span>,
</span></span><span style=display:flex><span>    dataset_text_field <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;text&#34;</span>,   <span style=color:#6272a4># 跟数据集定义的map函数formatting_prompts_func返回的key一致</span>
</span></span><span style=display:flex><span>    dataset_num_proc <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2</span>,   <span style=color:#6272a4># 用于tokenize数据集的worker数量，packing=False时生效</span>
</span></span><span style=display:flex><span>    packing <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>, <span style=color:#6272a4># Can make training 5x faster for short sequences.</span>
</span></span><span style=display:flex><span>    args <span style=color:#ff79c6>=</span> TrainingArguments(
</span></span><span style=display:flex><span>        <span style=color:#6272a4># num_train_epochs=3,  训练的总轮数</span>
</span></span><span style=display:flex><span>        per_device_train_batch_size <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2</span>,  <span style=color:#6272a4># 控制训练的批量大小</span>
</span></span><span style=display:flex><span>        gradient_accumulation_steps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>4</span>, <span style=color:#6272a4># 梯度累积技术通过分割数据为更小的子批量来实现模拟大批量训练的效果。如果将per_device_train_batch_size设为4且gradient_accumulation_steps设为2，则最终的总批量大小实际上是8（4乘以2）</span>
</span></span><span style=display:flex><span>        warmup_steps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>5</span>,   <span style=color:#6272a4># 是针对学习率learning rate优化的一种策略，在训练初期使用较小的学习率（从0开始），在一定步数（比如1000步）内逐渐提高到正常大小(设置的learning_rate)，避免模型过早进入局部最优而过拟合；在训练后期再慢慢将学习率降低到 0，避免后期训练还出现较大的参数变化</span>
</span></span><span style=display:flex><span>        max_steps <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>,     <span style=color:#6272a4># 微调步数，影响微调后模型的输出质量，-1为无限制，根据数据集大小来</span>
</span></span><span style=display:flex><span>        learning_rate <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2e-4</span>,   <span style=color:#6272a4># 学习率是决定模型在训练期间如何更新其权重的关键超参数。</span>
</span></span><span style=display:flex><span>        fp16 <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>not</span> torch<span style=color:#ff79c6>.</span>cuda<span style=color:#ff79c6>.</span>is_bf16_supported(),
</span></span><span style=display:flex><span>        bf16 <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>cuda<span style=color:#ff79c6>.</span>is_bf16_supported(), <span style=color:#6272a4># bfloat16的设计允许处理更广泛的数值范围，而不会显著牺牲计算精度</span>
</span></span><span style=display:flex><span>        logging_steps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>        optim <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;adamw_8bit&#34;</span>,  <span style=color:#6272a4># 优化器的作用在于引导模型训练过程，通过最小化误差或提升准确性来进行微调。8位分页AdamW与LoRA可以显著降低AdamW的总内存消耗</span>
</span></span><span style=display:flex><span>        weight_decay <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.01</span>, <span style=color:#6272a4># 权重衰减是一种鼓励模型维持较小权重值的技术，通过这种方式实现对模型的正则化，以避免复杂度过高的模型。有助于模型不过分依赖于任何单一的输入特征</span>
</span></span><span style=display:flex><span>        lr_scheduler_type <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;linear&#34;</span>, <span style=color:#6272a4># 最常用的还是线性两段式调整学习率</span>
</span></span><span style=display:flex><span>        seed <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>3407</span>,  <span style=color:#6272a4># 设置随机数生成器的种子，以确保实验的可重复性子，为了确保结果的可重复性，在每次训练中使用相同的种子值，具体值并不重要</span>
</span></span><span style=display:flex><span>        output_dir <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;outputs&#34;</span>,   <span style=color:#6272a4># 模型训练的输出目录</span>
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=开始训练模型-2>开始训练模型</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>trainer_stats <span style=color:#ff79c6>=</span> trainer<span style=color:#ff79c6>.</span>train()
</span></span></code></pre></div><h3 id=验证微调模型-2>验证微调模型</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>inputs <span style=color:#ff79c6>=</span> tokenizer(
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>    alpaca_prompt<span style=color:#ff79c6>.</span>format(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;请使用中文回答问题&#34;</span>, <span style=color:#6272a4># instruction</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;海绵宝宝的书法是不是叫做海绵体?&#34;</span>, <span style=color:#6272a4># input</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;</span>, <span style=color:#6272a4># output - leave this blank for generation!</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>], return_tensors <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;pt&#34;</span>)<span style=color:#ff79c6>.</span>to(<span style=color:#f1fa8c>&#34;cuda&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> TextStreamer  <span style=color:#6272a4># TextStreamer可以实时输出</span>
</span></span><span style=display:flex><span>text_streamer <span style=color:#ff79c6>=</span> TextStreamer(tokenizer)
</span></span><span style=display:flex><span>_ <span style=color:#ff79c6>=</span> model<span style=color:#ff79c6>.</span>generate(<span style=color:#ff79c6>**</span>inputs, streamer <span style=color:#ff79c6>=</span> text_streamer, max_new_tokens <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>128</span>)
</span></span></code></pre></div><h3 id=保存微调模型-2>保存微调模型</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#ff79c6>.</span>save_pretrained(<span style=color:#f1fa8c>&#34;lora_model&#34;</span>) <span style=color:#6272a4># Local saving</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#ff79c6>.</span>save_pretrained(<span style=color:#f1fa8c>&#34;lora_model&#34;</span>)
</span></span></code></pre></div><h3 id=合并模型-2>合并模型</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 合并LoRa模型和基础模型</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> torch
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> transformers <span style=color:#ff79c6>import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> peft <span style=color:#ff79c6>import</span> PeftModel, LoraConfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#ff79c6>=</span> AutoTokenizer<span style=color:#ff79c6>.</span>from_pretrained(<span style=color:#f1fa8c>&#34;lora_model/&#34;</span>)
</span></span><span style=display:flex><span>base_model <span style=color:#ff79c6>=</span> AutoModelForCausalLM<span style=color:#ff79c6>.</span>from_pretrained(
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;microsoft/Phi-3-mini-4k-instruct&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#torch_dtype=torch.bfloat16,</span>
</span></span><span style=display:flex><span>    device_map<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>    trust_remote_code<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 这里有个小问题：上面跑微调设置loftq_config为None，待会合并的时候会报错，故这里特殊处理了下</span>
</span></span><span style=display:flex><span>lora_config <span style=color:#ff79c6>=</span> LoraConfig<span style=color:#ff79c6>.</span>from_pretrained(<span style=color:#f1fa8c>&#34;lora_model/&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> lora_config<span style=color:#ff79c6>.</span>loftq_config <span style=color:#ff79c6>is</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>    lora_config<span style=color:#ff79c6>.</span>loftq_config <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lora_model <span style=color:#ff79c6>=</span> PeftModel<span style=color:#ff79c6>.</span>from_pretrained(
</span></span><span style=display:flex><span>    base_model,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;lora_model/&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#torch_dtype=torch.bfloat16,</span>
</span></span><span style=display:flex><span>    config<span style=color:#ff79c6>=</span>lora_config
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;Applying the LoRA&#34;</span>)
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> lora_model<span style=color:#ff79c6>.</span>merge_and_unload()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Saving the target model to phi-3-mini-4k-with-lora&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># 保存为HF格式</span>
</span></span><span style=display:flex><span>model<span style=color:#ff79c6>.</span>save_pretrained(<span style=color:#f1fa8c>&#34;phi-3-mini-4k-with-lora&#34;</span>)
</span></span><span style=display:flex><span>tokenizer<span style=color:#ff79c6>.</span>save_pretrained(<span style=color:#f1fa8c>&#34;phi-3-mini-4k-with-lora&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># 使用llama.cpp转换格式，HF格式转化为16bit GGUF格式</span>
</span></span><span style=display:flex><span>python llama.cpp/convert-hf-to-gguf.py --outfile phi-3-mini-4k-lora.fp16.gguf --outtype f16 phi-3-mini-4k-with-lora/
</span></span><span style=display:flex><span><span style=color:#6272a4># 量化为4bit GGUF格式</span>
</span></span><span style=display:flex><span>./llama.cpp/quantize phi-3-mini-4k-lora.fp16.gguf phi-3-mini-4k-lora.Q4_K_M.gguf Q4_K_M
</span></span></code></pre></div><h2 id=分布式训练工具>分布式训练工具</h2><h3 id=accelerate>Accelerate</h3><h3 id=deepspeed>DeepSpeed</h3><h1 id=llm大模型api第三方库>LLM大模型API第三方库</h1><ul><li><a href=https://github.com/BerriAI/litellm>https://github.com/BerriAI/litellm</a>：兼容OpenAI格式的统一大模型API，大多支持国外的模型</li><li><a href=https://github.com/songquanpeng/one-api>https://github.com/songquanpeng/one-api</a>：兼容OpenAI格式的统一大模型API，大多支持国内的模型</li></ul><h1 id=使用langchain开发应用程序>使用LangChain开发应用程序</h1><p>官网提供了一个ChatGPT风格的LangChain帮助文档搜索：<a href=https://chat.langchain.com/>https://chat.langchain.com/</a></p><p>LangChain Hub: <a href=https://smith.langchain.com/hub>https://smith.langchain.com/hub</a>, 一个用于管理和共享LLM 提示词（Prompt）的在线平台</p><h2 id=简介introduction-1>简介Introduction</h2><p>LangChain是一套专为LLM开发打造的开源框架，实现了LLM多种强大能力的利用，提供了Chain、Agent、Tool等多种封装工具，
基于LangChain可以便捷开发应用程序，极大化发挥LLM潜能。目前使用LangChin已经成为LLM开发的必备能力之一。</p><h2 id=模型提示和输出解释器modelspromptsparsers>模型、提示和输出解释器Models,Prompts,Parsers</h2><p>LLM开发的一些重要概念：模型、提示和解释器</p><h3 id=直接调用openai>直接调用OpenAI</h3><h4 id=计算11>计算1+1</h4><p>直接通过OpenAl接口封装的函数<code>CreateChatCompletion</code>让模型回答：1+1是什么？</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>1+1是什么？
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), prompt)
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(res)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>1+1等于2。
</code></pre><h4 id=用普通话表达海盗邮件>用普通话表达海盗邮件</h4><p>现在用一个更为丰富、复杂的场景，假如你是一家电商公司的员工，客户中有一位名为海盗A的特殊顾客。他在你们的平台上购买了一个榨汁机，目的是为了制作美味的奶昔。但在制作过程中，
由于某种原因，奶昔的盖子突然弹开，导致厨房的墙上洒满了奶昔。想象一下这名海盗的愤怒和挫败之情。用充满愤怒的英语方言，给客服中心写了一封邮件</p><p>为了解决这一挑战，我们设定了以下两个目标：</p><ul><li>首先，我们希望模型能够将这封充满海盗方言的邮件翻译成普通话，这样客服团队就能更容易地理解其内容。</li><li>其次，在进行翻译时，我们期望模型能采用平和和尊重的语气，这不仅能确保信息准确传达，还能保持与顾客之间的和谐关系。</li></ul><p>为了让引导模型的输出，定义了一个文本表达风格标签<code>style</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>func</span> <span style=color:#50fa7b>main</span>() {
</span></span><span style=display:flex><span>	text <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>更糟糕的是，保修条款可不包括清理我厨房的费用。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>伙计，赶紧给我过来！
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>	style <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>&#34;正式普通话，用一个平静、尊敬、有礼貌的语调&#34;</span>
</span></span><span style=display:flex><span>	prompt <span style=color:#ff79c6>:=</span> <span style=color:#f1fa8c>`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>把下面由两个双引号分隔的文本翻译成一种&#34;%s&#34;风格。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>文本：&#34;%s&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>	client <span style=color:#ff79c6>:=</span> <span style=color:#50fa7b>newOpenAIClient</span>()
</span></span><span style=display:flex><span>	resp, err <span style=color:#ff79c6>:=</span> client.<span style=color:#50fa7b>CreateChatCompletion</span>(context.<span style=color:#50fa7b>TODO</span>(), fmt.<span style=color:#50fa7b>Sprintf</span>(prompt, style, text))
</span></span><span style=display:flex><span>	<span style=color:#ff79c6>if</span> err <span style=color:#ff79c6>!=</span> <span style=color:#ff79c6>nil</span> {
</span></span><span style=display:flex><span>		fmt.<span style=color:#50fa7b>Printf</span>(<span style=color:#f1fa8c>&#34;ChatCompletion error: %v\n&#34;</span>, err)
</span></span><span style=display:flex><span>		<span style=color:#ff79c6>return</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	res <span style=color:#ff79c6>:=</span> resp.Choices[<span style=color:#bd93f9>0</span>].Message.Content
</span></span><span style=display:flex><span>	fmt.<span style=color:#50fa7b>Println</span>(res)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>尊敬的先生/小姐，我现在实在感到非常生气，因为我的搅拌机盖子竟然飞了出去，将我厨房的墙壁溅上果汁！
更糟糕的是，保修条款并不包括清理厨房的费用。请您尽快前来帮我解决这个问题！感谢您的合作。
</code></pre><p>进行语言风格转换之后，可以看到明显的语气变化。</p><h3 id=通过langchain使用openai>通过LangChain使用OpenAI</h3><p>上面的例子通过调用OpenAI接口成功地对邮件内容进行了风格转换，接下来使用LangChain来实现同样的效果。</p><h4 id=模型>模型</h4><p>从LangChain库导入OpenAI的对话模型ChatOpenAI，LangChain官网还集成了众多其它对话模型：<code>https://python.langchain.com/docs/integrations/chat/</code></p><pre tabindex=0><code># pip3 install langchain
# pip3 list |grep langchain
langchain                0.1.11
langchain-community      0.0.27
langchain-core           0.1.30
langchain-openai         0.0.7
langchain-text-splitters 0.0.1
</code></pre><p>这里安装的LangChain的版本是0.1.11</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;*******************&#34;</span>
</span></span><span style=display:flex><span>openai_proxy <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.***.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    chat <span style=color:#ff79c6>=</span>ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        openai_proxy<span style=color:#ff79c6>=</span>openai_proxy)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(chat)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>client=&lt;openai.resources.chat.completions.Completions object at 0x10aee9af0&gt; async_client=&lt;openai.resources.chat.completions.AsyncCompletions object at 0x10aeeb0b0&gt; temperature=0.0 openai_api_key=SecretStr(&#39;**********&#39;) openai_proxy=&#39;https://api.chatanywhere.com.cn/v1&#39;
</code></pre><h4 id=使用提示模版>使用提示模版</h4><p>LangChain提供了接口，方便更快速的构造和使用提示。</p><p>1.用普通话表达海盗邮件</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;*******************&#34;</span>
</span></span><span style=display:flex><span>openai_proxy <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.***.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    customer_style <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;正式普通话 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>    用一个平静、尊敬的语气
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    customer_email <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>更糟糕的是，保修条款可不包括清理我厨房的费用。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>伙计，赶紧给我过来！
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    template_string <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;把由三个反引号分隔的文本</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>翻译成一种</span><span style=color:#f1fa8c>{my_style}</span><span style=color:#f1fa8c>风格。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>文本: ```</span><span style=color:#f1fa8c>{my_text}</span><span style=color:#f1fa8c>```
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用提示模版，可以定义消息格式</span>
</span></span><span style=display:flex><span>    prompt_template <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(template_string)
</span></span><span style=display:flex><span>    customer_messages <span style=color:#ff79c6>=</span> prompt_template<span style=color:#ff79c6>.</span>format_messages(
</span></span><span style=display:flex><span>        my_style <span style=color:#ff79c6>=</span> customer_style,
</span></span><span style=display:flex><span>        my_text <span style=color:#ff79c6>=</span> customer_email
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 打印客户消息类型</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;客户消息类型:&#34;</span>,<span style=color:#8be9fd;font-style:italic>type</span>(customer_messages),<span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 打印第一个客户消息类型</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;第一个客户消息类型:&#34;</span>, <span style=color:#8be9fd;font-style:italic>type</span>(customer_messages[<span style=color:#bd93f9>0</span>]),<span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 打印第一个元素</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;第一个客户消息: &#34;</span>, customer_messages[<span style=color:#bd93f9>0</span>],<span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>客户消息类型: &lt;class &#39;list&#39;&gt; 

第一个客户消息类型: &lt;class &#39;langchain_core.messages.human.HumanMessage&#39;&gt; 

第一个客户消息:  content=&#39;把由三个反引号分隔的文本翻译成一种正式普通话     用一个平静、尊敬的语气\n风格。文本: ```\n    嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！\n更糟糕的是，保修条款可不包括清理我厨房的费用。\n伙计，赶紧给我过来！\n```\n&#39; 
</code></pre><p>上面的消息格式看起来还不太友好，使用ChatOpenAI模型来转化消息格式</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;******&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.***.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    customer_style <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;正式普通话 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>    用一个平静、尊敬的语气
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    customer_email <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>更糟糕的是，保修条款可不包括清理我厨房的费用。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>伙计，赶紧给我过来！
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    template_string <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;把由三个反引号分隔的文本</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>翻译成一种</span><span style=color:#f1fa8c>{my_style}</span><span style=color:#f1fa8c>风格。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>文本: ```</span><span style=color:#f1fa8c>{my_text}</span><span style=color:#f1fa8c>```
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用提示模版</span>
</span></span><span style=display:flex><span>    prompt_template <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(template_string)
</span></span><span style=display:flex><span>    customer_messages <span style=color:#ff79c6>=</span> prompt_template<span style=color:#ff79c6>.</span>format_messages(
</span></span><span style=display:flex><span>        my_style <span style=color:#ff79c6>=</span> customer_style,
</span></span><span style=display:flex><span>        my_text <span style=color:#ff79c6>=</span> customer_email
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 强制转换类型</span>
</span></span><span style=display:flex><span>    openai_messages <span style=color:#ff79c6>=</span> chat(customer_messages)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(openai_messages<span style=color:#ff79c6>.</span>content)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>您好，我现在感到非常愤怒，我的搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！更糟糕的是，保修条款并不包括清理我厨房的费用。朋友，请赶紧过来帮帮我！感激不尽。
</code></pre><p>2.用海盗方言回复邮件</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;******&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.***.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    service_style <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;一个有礼貌的语气 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>    使用海盗风格
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    service_response <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;嘿，顾客， </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>保修不包括厨房的清洁费用， </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>因为您在启动搅拌机之前 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>忘记盖上盖子而误用搅拌机, </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>这是您的错。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>倒霉！ 再见！
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    template_string <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;把由三个反引号分隔的文本</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>翻译成一种</span><span style=color:#f1fa8c>{my_style}</span><span style=color:#f1fa8c>风格。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>文本: ```</span><span style=color:#f1fa8c>{my_text}</span><span style=color:#f1fa8c>```
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用提示模版</span>
</span></span><span style=display:flex><span>    prompt_template <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(template_string)
</span></span><span style=display:flex><span>    customer_messages <span style=color:#ff79c6>=</span> prompt_template<span style=color:#ff79c6>.</span>format_messages(
</span></span><span style=display:flex><span>        my_style <span style=color:#ff79c6>=</span> service_style,
</span></span><span style=display:flex><span>        my_text <span style=color:#ff79c6>=</span> service_response
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    openai_messages <span style=color:#ff79c6>=</span> chat<span style=color:#ff79c6>.</span>invoke(customer_messages)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(openai_messages<span style=color:#ff79c6>.</span>content)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>啊哟，尊贵的客人，抱歉地通知您，保修不包括厨房的清洁费用。因为您在启动搅拌机之前忘记盖上盖子而误用搅拌机，这可是您的疏忽啊。真是倒霉！祝您一天愉快，再见！愿您的航程一帆风顺！Yo-ho-ho！
</code></pre><p>3.为什么需要提示模版</p><p>使用提示模版，可以让我们更为方便地重复使用设计好的提示。LangChain还提供了提示模版用于一些常用场景。比如自动摘要、问答、连接到SQL数据库、连接到不同的API。</p><h4 id=输出解释器>输出解释器</h4><p>1.不使用输出解释器提取客户评价中的信息</p><p>给定的评价customer_review，从中提取信息，按以下格式输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;礼物&#34;</span>: 是的,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;交货天数&#34;</span>: <span style=color:#bd93f9>5</span>,
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>&#34;价钱&#34;</span>: <span style=color:#f1fa8c>&#34;很贵&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;******&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.***.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    customer_review <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>   这款吹叶机非常神奇。 它有四个设置：</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>吹蜡烛、微风、风城、龙卷风。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>两天后就到了，正好赶上我妻子的</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>周年纪念礼物。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>我想我的妻子会喜欢它到说不出话来。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>到目前为止，我是唯一一个使用它的人，而且我一直</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>每隔一天早上用它来清理草坪上的叶子。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>它比其他吹叶机稍微贵一点，</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>但我认为它的额外功能是值得的。 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    template_string <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>对于以下文本，请从中提取以下信息：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>礼物：该商品是作为礼物送给别人的吗？ </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>如果是，则回答 是的；如果否或未知，则回答 不是。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>交货天数：产品需要多少天</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>到达？ 如果没有找到该信息，则输出-1。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>价钱：提取有关价值或价格的任何句子，</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>并将它们输出为逗号分隔的 Python 列表。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>使用以下键将输出格式化为 JSON：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>礼物
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>交货天数
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>价钱
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>文本: </span><span style=color:#f1fa8c>{my_text}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用提示模版</span>
</span></span><span style=display:flex><span>    prompt_template <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(template_string)
</span></span><span style=display:flex><span>    customer_messages <span style=color:#ff79c6>=</span> prompt_template<span style=color:#ff79c6>.</span>format_messages(
</span></span><span style=display:flex><span>        my_text <span style=color:#ff79c6>=</span> customer_review
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    openai_messages <span style=color:#ff79c6>=</span> chat<span style=color:#ff79c6>.</span>invoke(customer_messages)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;结果类型：&#34;</span>, <span style=color:#8be9fd;font-style:italic>type</span>(openai_messages<span style=color:#ff79c6>.</span>content))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;结果：&#34;</span>, openai_messages<span style=color:#ff79c6>.</span>content)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>结果类型： &lt;class &#39;str&#39;&gt;
结果： {
    &#34;礼物&#34;: &#34;是的&#34;,
    &#34;交货天数&#34;: 2,
    &#34;价钱&#34;: [&#34;它比其他吹叶机稍微贵一点&#34;]
}
</code></pre><p>返回的结果类型是字符串，想方便提取信息的话，还是要使用LangChain中的输出解释器。</p><p>2.使用输出解释器提取客户评价中的信息</p><p>使用LangChain的输出解释器</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.output_parsers <span style=color:#ff79c6>import</span> ResponseSchema, StructuredOutputParser
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;******&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.***.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    customer_review <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>   这款吹叶机非常神奇。 它有四个设置：</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>吹蜡烛、微风、风城、龙卷风。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>两天后就到了，正好赶上我妻子的</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>周年纪念礼物。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>我想我的妻子会喜欢它到说不出话来。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>到目前为止，我是唯一一个使用它的人，而且我一直</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>每隔一天早上用它来清理草坪上的叶子。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>它比其他吹叶机稍微贵一点，</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>但我认为它的额外功能是值得的。 
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    template_string <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>对于以下文本，请从中提取以下信息：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>礼物：该商品是作为礼物送给别人的吗？
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>如果是，则回答 是的；如果否或未知，则回答 不是。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>交货天数：产品到达需要多少天？ 如果没有找到该信息，则输出-1。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>价钱：提取有关价值或价格的任何句子，并将它们输出为逗号分隔的Python列表。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>文本: </span><span style=color:#f1fa8c>{my_text}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{format_instructions}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用提示模版</span>
</span></span><span style=display:flex><span>    prompt_template <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(template_string)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用输出解释器</span>
</span></span><span style=display:flex><span>    gift_schema <span style=color:#ff79c6>=</span> ResponseSchema(name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;礼物&#34;</span>, description<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;这件物品是作为礼物送给别人的吗？</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>                            如果是，则回答 是的，</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>                            如果否或未知，则回答 不是。&#34;</span>)
</span></span><span style=display:flex><span>    delivery_days_schema <span style=color:#ff79c6>=</span> ResponseSchema(name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;交货天数&#34;</span>,
</span></span><span style=display:flex><span>                                      description<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;产品需要多少天才能到达？</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>                                      如果没有找到该信息，则输出-1。&#34;</span>)
</span></span><span style=display:flex><span>    price_value_schema <span style=color:#ff79c6>=</span> ResponseSchema(name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;价钱&#34;</span>,
</span></span><span style=display:flex><span>                                    description<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;提取有关价值或价格的任何句子，</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>                                    并将它们输出为逗号分隔的Python列表&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    response_schemas <span style=color:#ff79c6>=</span> [gift_schema, delivery_days_schema, price_value_schema]
</span></span><span style=display:flex><span>    output_parser <span style=color:#ff79c6>=</span> StructuredOutputParser<span style=color:#ff79c6>.</span>from_response_schemas(response_schemas)
</span></span><span style=display:flex><span>    format_instructions <span style=color:#ff79c6>=</span> output_parser<span style=color:#ff79c6>.</span>get_format_instructions()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    customer_messages <span style=color:#ff79c6>=</span> prompt_template<span style=color:#ff79c6>.</span>format_messages(
</span></span><span style=display:flex><span>        my_text<span style=color:#ff79c6>=</span>customer_review,
</span></span><span style=display:flex><span>        format_instructions<span style=color:#ff79c6>=</span>format_instructions
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    openai_messages <span style=color:#ff79c6>=</span> chat(customer_messages)
</span></span><span style=display:flex><span>    output_dict <span style=color:#ff79c6>=</span> output_parser<span style=color:#ff79c6>.</span>parse(openai_messages<span style=color:#ff79c6>.</span>content)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;结果类型：&#34;</span>, <span style=color:#8be9fd;font-style:italic>type</span>(output_dict))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;结果：&#34;</span>, output_dict)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>结果类型： &lt;class &#39;dict&#39;&gt;
结果： {&#39;礼物&#39;: &#39;是的&#39;, &#39;交货天数&#39;: &#39;两天后&#39;, &#39;价钱&#39;: &#39;它比其他吹叶机稍微贵一点&#39;}
</code></pre><p>从输出结果来看，结果类型类型为字典dict，操作dict数据结构更方便提取数据</p><h2 id=记忆memory>记忆Memory</h2><p>使用LangChain中的记忆模块，将先前的对话嵌入到语言模型中，使其具有连续对话的能力。
使用LangChain中的记忆(Memory)模块时，它旨在保存、组织和跟踪整个对话的历史，从而为用户和模型之间的交互提供连续的上下文。</p><p>这里主要介绍常用的四种记忆模块，其他模块可以查阅文档</p><ul><li>对话缓存记忆(ConversationBufferMemory)</li><li>对话缓存窗口记忆(ConversationBufferWindowMemory)</li><li>对话令牌缓存记忆(ConversationTokenBufferMemory)</li><li>对话摘要缓存记忆(ConversationSummaryBufferMemory)</li></ul><p>在LangChain中，记忆指的是大语言模型的短期记忆。为什么称为短期记忆？因为当用户与训练好的LLM进行对话时，LLM会暂时记住用户的输入和它已经生成的输出，以便预测之后的输出，而模型输出完毕后，它便会“遗忘”之前用户的输入和它的输出。</p><p>如果想延长LLM短期记忆的保留时间，需要借助一些外部记忆方式来进行记忆，以便能够知道历史对话信息。</p><h3 id=对话缓存记忆>对话缓存记忆</h3><h4 id=初始化对话模型>初始化对话模型</h4><p>初始化对话模型，并进行多轮对话</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> ConversationChain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.memory <span style=color:#ff79c6>import</span> ConversationBufferMemory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;******&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.***.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    memory <span style=color:#ff79c6>=</span> ConversationBufferMemory()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    conversation <span style=color:#ff79c6>=</span> ConversationChain(llm<span style=color:#ff79c6>=</span>llm, memory<span style=color:#ff79c6>=</span>memory, verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;你好，我叫特特鲁斯&#34;</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;1+1等于多少？&#34;</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;我叫什么名字？&#34;</span>)
</span></span><span style=display:flex><span>	conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; Entering new ConversationChain chain...
</span></span><span style=display:flex><span>Prompt after formatting:
</span></span><span style=display:flex><span>The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Current conversation:
</span></span><span style=display:flex><span>Human: 你好，我叫特特鲁斯
</span></span><span style=display:flex><span>AI:  你好，特特鲁斯！很高兴认识你。我是一个人工智能程序，可以回答你的问题或者和你聊天。你有什么想知道的吗？
</span></span><span style=display:flex><span>Human: 我叫什么名字
</span></span><span style=display:flex><span>AI: 你叫特特鲁斯。很特别的名字！你知道吗，特特鲁斯这个名字在拉丁语中意味着“勇敢的战士”。很有力量的名字呢！有什么其他问题想问我吗？
</span></span><span style=display:flex><span>Human: 1+1等于多少
</span></span><span style=display:flex><span>AI: 1加1等于2。这是一个非常简单的数学问题，答案是2。如果你有其他数学问题或者其他想知道的事情，都可以问我哦！我会尽力回答你的。
</span></span><span style=display:flex><span>Human: 
</span></span><span style=display:flex><span>AI:
</span></span></code></pre></div><p>使用predict进行预测时，LangChain会生成一些提示，使系统进行友好的对话</p><h4 id=查看记忆缓存>查看记忆缓存</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># memory.buffer_as_messages记忆了当前为止所有 的对话信息</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(memory<span style=color:#ff79c6>.</span>buffer_as_messages)
</span></span><span style=display:flex><span><span style=color:#6272a4># load_memory_variables也可以打印缓存中的历史消息</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(memory<span style=color:#ff79c6>.</span>load_memory_variables({}))
</span></span></code></pre></div><p>输出</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>[HumanMessage(content=&#39;你好，我叫特特鲁斯&#39;), AIMessage(content=&#39; 你好，特特鲁斯！很高兴认识你。我是一个人工智能助手，可以回答你的问题或者和你聊天。有什么我可以帮助你的吗？&#39;), HumanMessage(content=&#39;我叫什么名字&#39;), AIMessage(content=&#39;抱歉，我不知道你的名字。你可以告诉我吗？&#39;), HumanMessage(content=&#39;1+1等于多少&#39;), AIMessage(content=&#39;1加1等于2。您还有其他问题吗？&#39;), HumanMessage(content=&#39;&#39;), AIMessage(content=&#39;如果您有任何其他问题或者想要聊天，随时告诉我哦！我随时准备好帮助您。&#39;)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>{&#39;history&#39;: &#39;Human: 你好，我叫特特鲁斯\nAI:  你好，特特鲁斯！我是一个AI助手，很高兴认识你。你有什么问题或者想要聊什么吗？\nHuman: 我叫什么名字\nAI: 你叫特特鲁斯。特特鲁斯是一个很独特的名字，听起来很有个性。你喜欢这个名字吗？如果你有任何其他问题或者想要聊什么，随时告诉我哦！\nHuman: 1+1等于多少\nAI: 1加1等于2。这是一个非常基本的数学问题，答案是2。如果你有任何其他数学问题或者其他想要了解的知识，都可以问我哦！我会尽力帮助你。\nHuman: \nAI: 你有任何其他问题或者想要聊什么吗？我可以提供各种信息和帮助。&#39;}
</span></span></code></pre></div><h4 id=添加内容到记忆缓存>添加内容到记忆缓存</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> ConversationChain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.memory <span style=color:#ff79c6>import</span> ConversationBufferMemory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;******&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.***.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    memory <span style=color:#ff79c6>=</span> ConversationBufferMemory()
</span></span><span style=display:flex><span>    memory<span style=color:#ff79c6>.</span>save_context({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我叫ice&#34;</span>}, {<span style=color:#f1fa8c>&#34;output&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我叫tracy&#34;</span>})
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    conversation <span style=color:#ff79c6>=</span> ConversationChain(llm<span style=color:#ff79c6>=</span>llm, memory<span style=color:#ff79c6>=</span>memory, verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;你好，我叫特特鲁斯&#34;</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;1+1等于多少？&#34;</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;我叫什么名字？&#34;</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(memory<span style=color:#ff79c6>.</span>load_memory_variables({}))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; Entering new ConversationChain chain...
</span></span><span style=display:flex><span>Prompt after formatting:
</span></span><span style=display:flex><span>The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Current conversation:
</span></span><span style=display:flex><span>Human: 你好，我叫ice
</span></span><span style=display:flex><span>AI: 你好，我叫tracy
</span></span><span style=display:flex><span>Human: 你好，我叫特特鲁斯
</span></span><span style=display:flex><span>AI: 很高兴认识你，特特鲁斯！你有什么想要了解或者讨论的吗？
</span></span><span style=display:flex><span>Human: 1+1等于多少？
</span></span><span style=display:flex><span>AI: 1加1等于2。这是一个基本的数学问题，答案是2。您还有其他问题吗？
</span></span><span style=display:flex><span>Human: 我叫什么名字？
</span></span><span style=display:flex><span>AI: 您说您叫特特鲁斯。您的名字是特特鲁斯。有什么其他问题吗？
</span></span><span style=display:flex><span>Human: 
</span></span><span style=display:flex><span>AI:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>{&#39;history&#39;: &#39;Human: 你好，我叫ice\nAI: 你好，我叫tracy\nHuman: 你好，我叫特特鲁斯\nAI: 很高兴认识你，特特鲁斯！你有什么想要了解或者讨论的吗？\nHuman: 1+1等于多少？\nAI: 1加1等于2。这是一个基本的数学问题，答案是2。您还有其他问题吗？\nHuman: 我叫什么名字？\nAI: 您说您叫特特鲁斯。您的名字是特特鲁斯。有什么其他问题吗？\nHuman: \nAI: 您有什么其他问题或者想要讨论的吗？我可以提供各种信息和帮助。&#39;}
</span></span></code></pre></div><p>使用<code>save_context</code>添加内容到buffer中，然后通过<code>memory.load_memory_variables({})</code>打印对话历史。在使用大型语言模型进行聊天对话时，大型语言模型本身实际上是无状态的。语言模型本身并不记得到目前为止的历史对话。</p><h3 id=对话缓存窗口记忆>对话缓存窗口记忆</h3><p>对话随着时间积累会越来越长，内存也占用越来越多，这就需要大量的token发送到大模型。为了节约token，对话缓存窗口记忆可以设置大小限制</p><h4 id=添加对话到窗口记忆>添加对话到窗口记忆</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.memory <span style=color:#ff79c6>import</span> ConversationBufferWindowMemory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;xxxx&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.xxx.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    memory <span style=color:#ff79c6>=</span> ConversationBufferWindowMemory(k<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>    memory<span style=color:#ff79c6>.</span>save_context({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我叫ice&#34;</span>}, {<span style=color:#f1fa8c>&#34;output&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我叫tracy&#34;</span>})
</span></span><span style=display:flex><span>    memory<span style=color:#ff79c6>.</span>save_context({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;你好，朋友&#34;</span>}, {<span style=color:#f1fa8c>&#34;output&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我们一起玩吧&#34;</span>})
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(memory<span style=color:#ff79c6>.</span>load_memory_variables({}))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>{&#39;history&#39;: &#39;Human: 你好，朋友\nAI: 你好，我们一起玩吧&#39;}
</code></pre><p>使用<code>ConversationBufferWindowMemory</code>来实现交互的滑动窗口，设置k=1只保留最近的一个对话记忆。</p><h4 id=在对话链中应用窗口记忆>在对话链中应用窗口记忆</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> ConversationChain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.memory <span style=color:#ff79c6>import</span> ConversationBufferWindowMemory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;xxxx&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    memory <span style=color:#ff79c6>=</span> ConversationBufferWindowMemory(k<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>    conversation <span style=color:#ff79c6>=</span> ConversationChain(llm<span style=color:#ff79c6>=</span>llm, memory<span style=color:#ff79c6>=</span>memory, verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;你好，我叫特特鲁斯&#34;</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;1+1等于多少？&#34;</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;我叫什么名字？&#34;</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(memory<span style=color:#ff79c6>.</span>load_memory_variables({}))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; Entering new ConversationChain chain...
</span></span><span style=display:flex><span>Prompt after formatting:
</span></span><span style=display:flex><span>The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Current conversation:
</span></span><span style=display:flex><span>Human: 我叫什么名字？
</span></span><span style=display:flex><span>AI: 抱歉，我无法知道你的名字，因为我是一个人工智能程序，无法获取你的个人信息。如果你有其他问题或需要帮助，请随时告诉我。
</span></span><span style=display:flex><span>Human: 
</span></span><span style=display:flex><span>AI:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>{&#39;history&#39;: &#39;Human: \nAI: 你好！有什么我可以帮助你的吗？&#39;}
</span></span></code></pre></div><p>从输出结果来看，窗口记忆只能记住上一轮对话的信息</p><h3 id=对话字符缓存记忆>对话字符缓存记忆</h3><p>使用对话字符缓存记忆，内存将限制保存的token数量</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> ConversationChain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.memory <span style=color:#ff79c6>import</span> ConversationTokenBufferMemory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    memory <span style=color:#ff79c6>=</span> ConversationTokenBufferMemory(llm<span style=color:#ff79c6>=</span>llm, max_token_limit<span style=color:#ff79c6>=</span><span style=color:#bd93f9>20</span>)
</span></span><span style=display:flex><span>    memory<span style=color:#ff79c6>.</span>save_context({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我叫ice&#34;</span>}, {<span style=color:#f1fa8c>&#34;output&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我叫tracy&#34;</span>})
</span></span><span style=display:flex><span>    memory<span style=color:#ff79c6>.</span>save_context({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;你好，朋友&#34;</span>}, {<span style=color:#f1fa8c>&#34;output&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我们一起玩吧&#34;</span>})
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(memory<span style=color:#ff79c6>.</span>load_memory_variables({}))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>{&#39;history&#39;: &#39;AI: 你好，我们一起玩吧&#39;}
</code></pre><p>ChatGPT是使用了一种基于字节对编码(BPE)的方法啦进行tokenization。BPE是一种常见的tokenization技术，将输入文本分割成较小的子词单元。OpenAl是用tiktoken这个库来计算token的，tiktoken在github上是开源的<a href=https://github.com/openai/tiktoken>https://github.com/openai/tiktoken</a>.关于汉子和英文单词的token计算方式，知乎上一篇文章讲解：<a href=https://www.zhihu.com/question/594159910>https://www.zhihu.com/question/594159910</a></p><p>查询OpenAI的字符和token的映射关系：<a href=https://platform.openai.com/tokenizer>https://platform.openai.com/tokenizer</a></p><h3 id=对话摘要缓存记忆>对话摘要缓存记忆</h3><p>对话摘要缓存记忆，使用LLM对到目前为止历史对话自动总结摘要</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.memory <span style=color:#ff79c6>import</span> ConversationSummaryBufferMemory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    schedule <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;在八点你和你的产品团队有一个会议。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你需要做一个PPT。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>上午9点到12点你需要忙于LangChain。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>Langchain是一个有用的工具，因此你的项目进展的非常快。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>中午，在意大利餐厅与一位开车来的顾客共进午餐 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>走了一个多小时的路程与你见面，只为了解最新的 AI。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>确保你带了笔记本电脑可以展示最新的 LLM 样例.&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    memory <span style=color:#ff79c6>=</span> ConversationSummaryBufferMemory(llm<span style=color:#ff79c6>=</span>llm, max_token_limit<span style=color:#ff79c6>=</span><span style=color:#bd93f9>20</span>)
</span></span><span style=display:flex><span>    memory<span style=color:#ff79c6>.</span>save_context({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我叫ice&#34;</span>}, {<span style=color:#f1fa8c>&#34;output&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我叫tracy&#34;</span>})
</span></span><span style=display:flex><span>    memory<span style=color:#ff79c6>.</span>save_context({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;你好，朋友&#34;</span>}, {<span style=color:#f1fa8c>&#34;output&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我们一起玩吧&#34;</span>})
</span></span><span style=display:flex><span>    memory<span style=color:#ff79c6>.</span>save_context({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;今天的日程安排是什么？&#34;</span>}, {<span style=color:#f1fa8c>&#34;output&#34;</span>: <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span>schedule<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>})
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(memory<span style=color:#ff79c6>.</span>load_memory_variables({}))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>{&#39;history&#39;: &#39;Human: 你好，我叫ice\nAI: 你好，我叫tracy\nHuman: 你好，朋友\nAI: 你好，我们一起玩吧\nHuman: 今天的日程安排是什么？\nAI: 在八点你和你的产品团队有一个会议。 你需要做一个PPT。 上午9点到12点你需要忙于LangChain。Langchain是一个有用的工具，因此你的项目进展的非常快。中午，在意大利餐厅与一位开车来的顾客共进午餐 走了一个多小时的路程与你见面，只为了解最新的 AI。 确保你带了笔记本电脑可以展示最新的 LLM 样例.&#39;}
</span></span></code></pre></div><p>基于对话摘要缓存记忆的对话链</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> ConversationChain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.memory <span style=color:#ff79c6>import</span> ConversationSummaryBufferMemory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    schedule <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;在八点你和你的产品团队有一个会议。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你需要做一个PPT。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>上午9点到12点你需要忙于LangChain。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>Langchain是一个有用的工具，因此你的项目进展的非常快。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>中午，在意大利餐厅与一位开车来的顾客共进午餐 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>走了一个多小时的路程与你见面，只为了解最新的 AI。 </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>确保你带了笔记本电脑可以展示最新的 LLM 样例.&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    memory <span style=color:#ff79c6>=</span> ConversationSummaryBufferMemory(llm<span style=color:#ff79c6>=</span>llm, max_token_limit<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1000</span>)
</span></span><span style=display:flex><span>    memory<span style=color:#ff79c6>.</span>save_context({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我叫ice&#34;</span>}, {<span style=color:#f1fa8c>&#34;output&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我叫tracy&#34;</span>})
</span></span><span style=display:flex><span>    memory<span style=color:#ff79c6>.</span>save_context({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;你好，朋友&#34;</span>}, {<span style=color:#f1fa8c>&#34;output&#34;</span>: <span style=color:#f1fa8c>&#34;你好，我们一起玩吧&#34;</span>})
</span></span><span style=display:flex><span>    memory<span style=color:#ff79c6>.</span>save_context({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;今天的日程安排是什么？&#34;</span>}, {<span style=color:#f1fa8c>&#34;output&#34;</span>: <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span>schedule<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    conversation <span style=color:#ff79c6>=</span> ConversationChain(llm<span style=color:#ff79c6>=</span>llm, memory<span style=color:#ff79c6>=</span>memory, verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    conversation<span style=color:#ff79c6>.</span>predict(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;展示什么样的样例最好呢&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(memory<span style=color:#ff79c6>.</span>load_memory_variables({}))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>{&#39;history&#39;: &#39;Human: 你好，我叫ice\nAI: 你好，我叫tracy\nHuman: 你好，朋友\nAI: 你好，我们一起玩吧\nHuman: 今天的日程安排是什么？\nAI: 在八点你和你的产品团队有一个会议。 你需要做一个PPT。 上午9点到12点你需要忙于LangChain。Langchain是一个有用的工具，因此你的项目进展的非常快。中午，在意大利餐厅与一位开车来的顾客共进午餐 走了一个多小时的路程与你见面，只为了解最新的 AI。 确保你带了笔记本电脑可以展示最新的 LLM 样例.\nHuman: 展示什么样的样例最好呢\nAI: 展示一些关于LangChain如何提高生产效率的案例会很有帮助。你可以展示一些实际的数据和结果，以及用户的反馈和体验。这样可以更直观地展示LangChain的价值和优势。希望这些建议对你有所帮助！&#39;}
</span></span></code></pre></div><p>从输出结果来看，摘要内容更新了</p><h2 id=回调callbacks>回调Callbacks</h2><p>LangChain提供回调机制，允许hook到大模型应用的各个阶段，通过订阅这些时间来触发回调函数。官方文档的回调章节介绍：
<a href=https://python.langchain.com/docs/modules/callbacks/>https://python.langchain.com/docs/modules/callbacks/</a></p><h3 id=回调处理>回调处理</h3><p>这里有个重要的概念回调处理，CallbackHandlers是实现该CallbackHandler接口的对象，该接口对于每个可以订阅的事件都有一个方法。CallbackManager当事件被触发时，将在每个处理程序上调用适当的方法。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>BaseCallbackHandler</span>(
</span></span><span style=display:flex><span>    LLMManagerMixin,
</span></span><span style=display:flex><span>    ChainManagerMixin,
</span></span><span style=display:flex><span>    ToolManagerMixin,
</span></span><span style=display:flex><span>    RetrieverManagerMixin,
</span></span><span style=display:flex><span>    CallbackManagerMixin,
</span></span><span style=display:flex><span>    RunManagerMixin,
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>继承这么多类，展开的话相当于</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>BaseCallbackHandler</span>:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;Base callback handler that can be used to handle callbacks from langchain.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_llm_start</span>(
</span></span><span style=display:flex><span>        self, serialized: Dict[<span style=color:#8be9fd;font-style:italic>str</span>, Any], prompts: List[<span style=color:#8be9fd;font-style:italic>str</span>], <span style=color:#ff79c6>**</span>kwargs: Any
</span></span><span style=display:flex><span>    ) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when LLM starts running.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_chat_model_start</span>(
</span></span><span style=display:flex><span>        self, serialized: Dict[<span style=color:#8be9fd;font-style:italic>str</span>, Any], messages: List[List[BaseMessage]], <span style=color:#ff79c6>**</span>kwargs: Any
</span></span><span style=display:flex><span>    ) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when Chat Model starts running.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_llm_new_token</span>(self, token: <span style=color:#8be9fd;font-style:italic>str</span>, <span style=color:#ff79c6>**</span>kwargs: Any) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run on new LLM token. Only available when streaming is enabled.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_llm_end</span>(self, response: LLMResult, <span style=color:#ff79c6>**</span>kwargs: Any) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when LLM ends running.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_llm_error</span>(
</span></span><span style=display:flex><span>        self, error: Union[Exception, KeyboardInterrupt], <span style=color:#ff79c6>**</span>kwargs: Any
</span></span><span style=display:flex><span>    ) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when LLM errors.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_chain_start</span>(
</span></span><span style=display:flex><span>        self, serialized: Dict[<span style=color:#8be9fd;font-style:italic>str</span>, Any], inputs: Dict[<span style=color:#8be9fd;font-style:italic>str</span>, Any], <span style=color:#ff79c6>**</span>kwargs: Any
</span></span><span style=display:flex><span>    ) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when chain starts running.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_chain_end</span>(self, outputs: Dict[<span style=color:#8be9fd;font-style:italic>str</span>, Any], <span style=color:#ff79c6>**</span>kwargs: Any) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when chain ends running.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_chain_error</span>(
</span></span><span style=display:flex><span>        self, error: Union[Exception, KeyboardInterrupt], <span style=color:#ff79c6>**</span>kwargs: Any
</span></span><span style=display:flex><span>    ) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when chain errors.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_tool_start</span>(
</span></span><span style=display:flex><span>        self, serialized: Dict[<span style=color:#8be9fd;font-style:italic>str</span>, Any], input_str: <span style=color:#8be9fd;font-style:italic>str</span>, <span style=color:#ff79c6>**</span>kwargs: Any
</span></span><span style=display:flex><span>    ) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when tool starts running.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_tool_end</span>(self, output: Any, <span style=color:#ff79c6>**</span>kwargs: Any) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when tool ends running.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_tool_error</span>(
</span></span><span style=display:flex><span>        self, error: Union[Exception, KeyboardInterrupt], <span style=color:#ff79c6>**</span>kwargs: Any
</span></span><span style=display:flex><span>    ) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when tool errors.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_text</span>(self, text: <span style=color:#8be9fd;font-style:italic>str</span>, <span style=color:#ff79c6>**</span>kwargs: Any) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run on arbitrary text.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_agent_action</span>(self, action: AgentAction, <span style=color:#ff79c6>**</span>kwargs: Any) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run on agent action.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_agent_finish</span>(self, finish: AgentFinish, <span style=color:#ff79c6>**</span>kwargs: Any) <span style=color:#ff79c6>-&gt;</span> Any:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run on agent end.&#34;&#34;&#34;</span>
</span></span></code></pre></div><h3 id=回调在哪里传递>回调在哪里传递</h3><p>callbacks在整个 API 中的大多数对象（链、模型、工具、代理等）上都可用，位于两个不同的位置：</p><ul><li>构造函数回调：在构造函数中定义，例如LLMChain(callbacks=[handler], tags=[&lsquo;a-tag&rsquo;])。在这种情况下，回调将用于对该对象进行的所有调用，并且仅限于该对象，例如，如果将处理程序传递给构造函数LLMChain，则附加到该链的模型将不会使用它。</li><li>请求回调：在用于发出请求的“invoke”方法中定义。在这种情况下，回调将仅用于该特定请求及其包含的所有子请求（例如，对LLMChain的调用会触发对模型的调用，模型使用在方法中传递的相同处理程序invoke()）。在invoke()方法中回调是通过配置参数传递的。使用“调用”方法的示例（注意：相同的方法可用于batch、ainvoke和abatch方法。）</li></ul><p>这两者有什么区别?</p><ul><li>构造函数回调对于日志记录、监控等用例最有用，这些用例不特定于单个请求，而是特定于整个链。例如，如果您想记录对 发出的所有请LLMChain，您可以将处理程序传递给构造函数。</li><li>请求回调对于诸如流式传输之类的用例最有用，您希望将单个请求的输出流式传输到特定的Websocket连接，或其他类似的用例。例如，如果您想将单个请求的输出流式传输到 websocket，您可以将处理invoke()程序传递给该方法</li></ul><p>一个异步回调的例子</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.messages <span style=color:#ff79c6>import</span> HumanMessage
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.outputs <span style=color:#ff79c6>import</span> LLMResult
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.callbacks.base <span style=color:#ff79c6>import</span> AsyncCallbackHandler, BaseCallbackHandler
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> typing <span style=color:#ff79c6>import</span> Any, Dict, List
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> asyncio
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>MyCustomSyncHandler</span>(BaseCallbackHandler):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_llm_new_token</span>(self, token: <span style=color:#8be9fd;font-style:italic>str</span>, <span style=color:#ff79c6>**</span>kwargs) <span style=color:#ff79c6>-&gt;</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Sync handler being called in a `thread_pool_executor`: token: </span><span style=color:#f1fa8c>{</span>token<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>MyCustomAsyncHandler</span>(AsyncCallbackHandler):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;Async callback handler that can be used to handle callbacks from langchain.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>async</span> <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_llm_start</span>(
</span></span><span style=display:flex><span>        self, serialized: Dict[<span style=color:#8be9fd;font-style:italic>str</span>, Any], prompts: List[<span style=color:#8be9fd;font-style:italic>str</span>], <span style=color:#ff79c6>**</span>kwargs: Any
</span></span><span style=display:flex><span>    ) <span style=color:#ff79c6>-&gt;</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when chain starts running.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;zzzz....&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>await</span> asyncio<span style=color:#ff79c6>.</span>sleep(<span style=color:#bd93f9>0.3</span>)
</span></span><span style=display:flex><span>        class_name <span style=color:#ff79c6>=</span> serialized[<span style=color:#f1fa8c>&#34;name&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;Hi! I just woke up. Your llm is starting&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>async</span> <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>on_llm_end</span>(self, response: LLMResult, <span style=color:#ff79c6>**</span>kwargs: Any) <span style=color:#ff79c6>-&gt;</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Run when chain ends running.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;zzzz....&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>await</span> asyncio<span style=color:#ff79c6>.</span>sleep(<span style=color:#bd93f9>0.3</span>)
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;Hi! I just woke up. Your llm is ending&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>async</span> <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># To enable streaming, we pass in `streaming=True` to the ChatModel constructor</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Additionally, we pass in a list with our custom handler</span>
</span></span><span style=display:flex><span>    chat <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        max_tokens<span style=color:#ff79c6>=</span><span style=color:#bd93f9>30</span>,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        streaming<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>        callbacks<span style=color:#ff79c6>=</span>[MyCustomSyncHandler(), MyCustomAsyncHandler()],
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>await</span> chat<span style=color:#ff79c6>.</span>agenerate([[HumanMessage(content<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;Tell me a joke&#34;</span>)]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    asyncio<span style=color:#ff79c6>.</span>run(main())
</span></span></code></pre></div><p>如果打算使用async API，建议使用AsyncCallbackHandler以避免阻塞runloop。这里
在使用异步方法运行LLM/链/工具/代理时使用同步，它仍然可以工作。如果这个同步的CallbackHandler是线程安全的，
那就没有问题。</p><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>zzzz....
</span></span><span style=display:flex><span>Hi! I just woke up. Your llm is starting
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token: 
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token: Why
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token:  couldn
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token: &#39;t
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token:  the
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token:  bicycle
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token:  stand
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token:  up
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token:  by
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token:  itself
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token: ?
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token: Because
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token:  it
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token:  was
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token:  two
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token:  tired
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token: !
</span></span><span style=display:flex><span>Sync handler being called in a `thread_pool_executor`: token: 
</span></span><span style=display:flex><span>zzzz....
</span></span><span style=display:flex><span>Hi! I just woke up. Your llm is ending
</span></span></code></pre></div><h2 id=模型链chains>模型链Chains</h2><p>链是将大语言模型(LLM)和提示(Prompt)结合在一起，这样可以对文本进行一系列操作。使用链一个典型的流程：</p><ul><li>创建一个链，链接受输入</li><li>使用提示模版对其格式化</li><li>将格式化的内容发送给LLM</li></ul><p>可以把多个链组合在一起，或者链与其他组件组合形成一个更复杂的链。</p><h3 id=大语言模型链>大语言模型链</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> LLMChain
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 1.初始化语言模型</span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 2.初始化提示模版：接受一个名为product的变量。该prompt将要求LLM生成一个描述制造该产品的公司的最佳名称</span>
</span></span><span style=display:flex><span>    prompt <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(<span style=color:#f1fa8c>&#34;描述制造</span><span style=color:#f1fa8c>{product}</span><span style=color:#f1fa8c>该产品的公司的最佳名称是什么&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 3.构建大语言模型链，链～=LLM+Prompt</span>
</span></span><span style=display:flex><span>    chain <span style=color:#ff79c6>=</span> LLMChain(llm<span style=color:#ff79c6>=</span>llm, prompt<span style=color:#ff79c6>=</span>prompt)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 4.运行大语言模型链</span>
</span></span><span style=display:flex><span>    product <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;大号床单套装&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(chain<span style=color:#ff79c6>.</span>invoke(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span>{<span style=color:#f1fa8c>&#34;product&#34;</span>: product})<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#39;text&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&#34;豪华床品有限公司&#34;
</span></span></code></pre></div><h3 id=简单顺序链>简单顺序链</h3><p>顾名思义顺序链是按定义顺序执行的链，简单顺序链是顺序链中的最简单类型，其中每个步骤都有一个输入/输出，一个步骤的输出是下一个步骤的输入。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> LLMChain, SimpleSequentialChain
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 1.初始化语言模型</span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 2.初始化提示模版1：这个提示将接受产品并返回最佳名称来描述该公司</span>
</span></span><span style=display:flex><span>    prompt1 <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(<span style=color:#f1fa8c>&#34;描述制造</span><span style=color:#f1fa8c>{product}</span><span style=color:#f1fa8c>该产品的公司的最佳名称是什么&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 3.初始化提示模版2：接受公司名称，然后输出该公司的长为20个单词的描述</span>
</span></span><span style=display:flex><span>    prompt2 <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(<span style=color:#f1fa8c>&#34;写一个20个单词的描述对于这个公司：</span><span style=color:#f1fa8c>{company_name}</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 4.构建大语言模型子链，链～=LLM+Prompt</span>
</span></span><span style=display:flex><span>    chain_one <span style=color:#ff79c6>=</span> LLMChain(llm<span style=color:#ff79c6>=</span>llm, prompt<span style=color:#ff79c6>=</span>prompt1)
</span></span><span style=display:flex><span>    chain_two <span style=color:#ff79c6>=</span> LLMChain(llm<span style=color:#ff79c6>=</span>llm, prompt<span style=color:#ff79c6>=</span>prompt2)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 5.构建一个简单顺序链，把两个子链组合起来</span>
</span></span><span style=display:flex><span>    simple_chain <span style=color:#ff79c6>=</span> SimpleSequentialChain(chains<span style=color:#ff79c6>=</span>[chain_one, chain_two], verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 6.运行简单顺序链</span>
</span></span><span style=display:flex><span>    product <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;大号床单套装&#34;</span>
</span></span><span style=display:flex><span>    simple_chain<span style=color:#ff79c6>.</span>invoke(product)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; Entering new SimpleSequentialChain chain...
</span></span><span style=display:flex><span>&#34;豪华床品有限公司&#34;
</span></span><span style=display:flex><span>&#34;豪华床品有限公司&#34;提供高品质、舒适的床上用品，让您享受豪华睡眠体验，提升生活品质。
</span></span></code></pre></div><h3 id=复杂顺序链>复杂顺序链</h3><p>当有多个输入或多个输出时，就需要复杂顺序链来实现；简单顺序链只针对一个输入和一个输出时。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> LLMChain, SequentialChain
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 1.初始化语言模型</span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 2.子链1: 翻译成英语</span>
</span></span><span style=display:flex><span>    prompt1 <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;把下面的文本翻译成英文: </span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{review}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    chain_one <span style=color:#ff79c6>=</span> LLMChain(llm<span style=color:#ff79c6>=</span>llm, prompt<span style=color:#ff79c6>=</span>prompt1, output_key<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;english_review&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 2.子链2: 用一句话总结下面的文本</span>
</span></span><span style=display:flex><span>    prompt2 <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;用一句话总结下面的文本：</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{english_review}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    chain_two <span style=color:#ff79c6>=</span> LLMChain(llm<span style=color:#ff79c6>=</span>llm, prompt<span style=color:#ff79c6>=</span>prompt2, output_key<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;summary&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 4.子链3: 下面文本使用什么语言</span>
</span></span><span style=display:flex><span>    prompt3 <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;下面的文本使用的是什么语言：</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{review}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    chain_three <span style=color:#ff79c6>=</span> LLMChain(llm<span style=color:#ff79c6>=</span>llm, prompt<span style=color:#ff79c6>=</span>prompt3, output_key<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;language&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 4.子链4: 使用特定的语言对下面的总结写一个后续回复</span>
</span></span><span style=display:flex><span>    prompt4 <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;使用特定的语言对下面的总结写一个后续回复：</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;总结: </span><span style=color:#f1fa8c>{summary}</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>语言: </span><span style=color:#f1fa8c>{language}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    chain_four <span style=color:#ff79c6>=</span> LLMChain(llm<span style=color:#ff79c6>=</span>llm, prompt<span style=color:#ff79c6>=</span>prompt4, output_key<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;followup_message&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 5.构建一个顺序链，把4个子链组合起来</span>
</span></span><span style=display:flex><span>    chain <span style=color:#ff79c6>=</span> SequentialChain(
</span></span><span style=display:flex><span>        chains<span style=color:#ff79c6>=</span>[chain_one, chain_two, chain_three, chain_four],
</span></span><span style=display:flex><span>        input_variables<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;review&#34;</span>],
</span></span><span style=display:flex><span>        output_variables<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;english_review&#34;</span>,<span style=color:#f1fa8c>&#34;summary&#34;</span>, <span style=color:#f1fa8c>&#34;followup_message&#34;</span>],
</span></span><span style=display:flex><span>        verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>        return_all<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 6.运行顺序链</span>
</span></span><span style=display:flex><span>    review <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;Kubernetes 和更广泛的容器生态系统正发展为通用计算平台和生态系统，</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>    可以媲美甚至超越虚拟机 (VM)，作为现代云基础设施和应用程序的基本构建块。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>    该生态系统使组织能够提供高生产力的平台即服务 (PaaS)，</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>    解决围绕云原生开发的多个基础设施相关和操作相关任务与问题，</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>    以便开发团队专注于编码和创新&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(chain<span style=color:#ff79c6>.</span>invoke(review))
</span></span><span style=display:flex><span>     
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>{&#39;review&#39;: &#39;Kubernetes 和更广泛的容器生态系统正发展为通用计算平台和生态系统，    可以媲美甚至超越虚拟机 (VM)，作为现代云基础设施和应用程序的基本构建块。    该生态系统使组织能够提供高生产力的平台即服务 (PaaS)，    解决围绕云原生开发的多个基础设施相关和操作相关任务与问题，    以便开发团队专注于编码和创新&#39;, &#39;english_review&#39;: &#39;Kubernetes and the broader container ecosystem are evolving into a universal computing platform and ecosystem that can rival or even surpass virtual machines (VMs) as the fundamental building blocks of modern cloud infrastructure and applications. This ecosystem enables organizations to deliver highly productive platform-as-a-service (PaaS), addressing multiple infrastructure and operational tasks and issues related to cloud-native development, allowing development teams to focus on coding and innovation.&#39;, &#39;summary&#39;: &#39;Kubernetes and containers are becoming the new standard for cloud infrastructure, enabling organizations to focus on coding and innovation.&#39;, &#39;followup_message&#39;: &#39;非常赞同这个总结！Kubernetes和容器正在成为云基础设施的新标准，让组织能够更专注于编码和创新。这种趋势对于推动技术发展和提高效率都有着重要的作用。希望更多的企业能够采用这些先进的技术，实现更快速的发展和创新。&#39;}
</span></span></code></pre></div><h3 id=路由链>路由链</h3><p>路由链顾名思义可以定义路由，具体路由到某一个子链上去，这样就可以实现更复杂的链操作。</p><p>路由器由两个组件组成：（类似网络中的路由概念）</p><ul><li>路由链：路由器链本身，负责选择要调用的下一个链</li><li>目的链：路由器链可以路由到的链</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate, PromptTemplate
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> LLMChain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains.router <span style=color:#ff79c6>import</span> MultiPromptChain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains.router.llm_router <span style=color:#ff79c6>import</span> LLMRouterChain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains.router.llm_router <span style=color:#ff79c6>import</span> RouterOutputParser
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 1.初始化语言模型</span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 2.初始化物理问题提示模版</span>
</span></span><span style=display:flex><span>    physics_template <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;你是一个非常聪明的物理专家</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你擅长用一种简洁并且易于理解的方式去回答问题</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>当你不知道问题的答案时，你承认</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你不知道.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>这是一个问题:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{input}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 3.初始化数学问题提示模版</span>
</span></span><span style=display:flex><span>    math_template <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;你是一个非常优秀的数学家。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你擅长回答数学问题。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你之所以如此优秀,</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>是因为你能够将棘手的问题分解为组成部分,</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>回答组成部分，然后将它们组合在一起，回答更广泛的问题。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>这是一个问题：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{input}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 4.初始化历史问题提示模版</span>
</span></span><span style=display:flex><span>    history_template <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;你是以为非常优秀的历史学家。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你对一系列历史时期的人物、事件和背景有着极好的学识和理解</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你有能力思考、反思、辩证、讨论和评估过去。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你尊重历史证据，并有能力利用它来支持你的解释和判断。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>这是一个问题:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{input}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 5.初始化计算机问题提示模版</span>
</span></span><span style=display:flex><span>    computerscience_template <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;你是一个成功的计算机科学专家。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你有创造力、协作精神、</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>前瞻性思维、自信、解决问题的能力、</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>对理论和算法的理解以及出色的沟通技巧。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你非常擅长回答编程问题。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你之所以如此优秀，是因为你知道</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>如何通过以机器可以轻松解释的命令式步骤描述解决方案来解决问题,</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>并且你知道如何选择在时间复杂性和空间复杂性之间取得良好平衡的解决方案。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>这是一个问题:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{input}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 6.对上述提示模版进行命名和描述，这些信息传递给路由链，路由链决定使用哪个子链</span>
</span></span><span style=display:flex><span>    prompt_infos <span style=color:#ff79c6>=</span> [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;name&#34;</span>: <span style=color:#f1fa8c>&#34;物理学&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;desc&#34;</span>: <span style=color:#f1fa8c>&#34;擅长回答关于物理学的问题&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;prompt_template&#34;</span>: physics_template
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;name&#34;</span>: <span style=color:#f1fa8c>&#34;数学&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;desc&#34;</span>: <span style=color:#f1fa8c>&#34;擅长回答数学问题&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;prompt_template&#34;</span>: math_template
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;name&#34;</span>: <span style=color:#f1fa8c>&#34;历史&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;desc&#34;</span>: <span style=color:#f1fa8c>&#34;擅长回答历史问题&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;prompt_template&#34;</span>: history_template
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;name&#34;</span>: <span style=color:#f1fa8c>&#34;计算机科学&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;desc&#34;</span>: <span style=color:#f1fa8c>&#34;擅长回答计算机科学问题&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;prompt_template&#34;</span>: computerscience_template
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 7.基于提示模版信息创建对应的目的链</span>
</span></span><span style=display:flex><span>    destination_chains <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> p_info <span style=color:#ff79c6>in</span> prompt_infos:
</span></span><span style=display:flex><span>        name <span style=color:#ff79c6>=</span> p_info[<span style=color:#f1fa8c>&#34;name&#34;</span>]
</span></span><span style=display:flex><span>        prompt_tempalte <span style=color:#ff79c6>=</span> p_info[<span style=color:#f1fa8c>&#34;prompt_template&#34;</span>]
</span></span><span style=display:flex><span>        prompt <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(
</span></span><span style=display:flex><span>            template<span style=color:#ff79c6>=</span>prompt_tempalte)
</span></span><span style=display:flex><span>        chain <span style=color:#ff79c6>=</span> LLMChain(
</span></span><span style=display:flex><span>            llm<span style=color:#ff79c6>=</span>llm,
</span></span><span style=display:flex><span>            prompt<span style=color:#ff79c6>=</span>prompt,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        destination_chains[name] <span style=color:#ff79c6>=</span> chain
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    destinations <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span>p[<span style=color:#f1fa8c>&#39;name&#39;</span>]<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>: </span><span style=color:#f1fa8c>{</span>p[<span style=color:#f1fa8c>&#39;desc&#39;</span>]<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span> <span style=color:#ff79c6>for</span> p <span style=color:#ff79c6>in</span> prompt_infos]
</span></span><span style=display:flex><span>    destinations_str <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span><span style=color:#ff79c6>.</span>join(destinations)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 8.创建默认目的链，类似路由表中的默认路由</span>
</span></span><span style=display:flex><span>    default_prompt <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_template(<span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{input}</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>    default_chain <span style=color:#ff79c6>=</span> LLMChain(
</span></span><span style=display:flex><span>        llm<span style=color:#ff79c6>=</span>llm,
</span></span><span style=display:flex><span>        prompt<span style=color:#ff79c6>=</span>default_prompt,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 9.定义不同链之间的路由模版，返回的格式要求为什么是destination和next_inputs，</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 跟RouterOutputParser相绑定的。为什么是4个花括号?因为要进行两次format，两个花括号相当于输出一个花括号</span>
</span></span><span style=display:flex><span>    MULTI_PROMPT_ROUTER_TEMPLATE <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>给语言模型一个原始文本输入,</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>让其选择最适合输入的模型提示.</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>系统将为您提供可用提示的名称以及最适合改提示的描述</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>如果你认为修改原始输入最终会导致语言模型做出更好的响应,</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>你也可以修改原始输入.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;&lt; 格式 &gt;&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>返回一个带有JSON对象的markdown代码片段, 该JSON对象的格式如下:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>```json
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>{{{{
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;destination&#34;: string 使用提示名字或者使用&#34;DEFAULT&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;next_inputs&#34;: string 原始输入的改进版本
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>}}}}
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>```
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>记住：&#34;destination&#34;必须是下面指定的候选提示名称之一, </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>或者如果输入不太适合任何候选提示, </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>则可以是&#34;DEFAULT&#34;。</span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>记住:如果您认为不需要任何修改, </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>则&#34;next_inputs&#34;可以只是原始输入。
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;&lt; 候选提示 &gt;&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{destinations}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;&lt; 输入 &gt;&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>{{input}}
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 10.构建路由链</span>
</span></span><span style=display:flex><span>    router_template <span style=color:#ff79c6>=</span> MULTI_PROMPT_ROUTER_TEMPLATE<span style=color:#ff79c6>.</span>format(
</span></span><span style=display:flex><span>        destinations<span style=color:#ff79c6>=</span>destinations_str
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(router_template)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(router_template)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    router_prompt <span style=color:#ff79c6>=</span> PromptTemplate(
</span></span><span style=display:flex><span>        template<span style=color:#ff79c6>=</span>router_template,
</span></span><span style=display:flex><span>        input_variables<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;input&#34;</span>],
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    router_prompt<span style=color:#ff79c6>.</span>output_parser <span style=color:#ff79c6>=</span> RouterOutputParser()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    router_chain <span style=color:#ff79c6>=</span> LLMRouterChain<span style=color:#ff79c6>.</span>from_llm(llm, router_prompt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 11.创建多提示链</span>
</span></span><span style=display:flex><span>    chain <span style=color:#ff79c6>=</span> MultiPromptChain(
</span></span><span style=display:flex><span>        router_chain<span style=color:#ff79c6>=</span>router_chain,
</span></span><span style=display:flex><span>        destination_chains<span style=color:#ff79c6>=</span>destination_chains,
</span></span><span style=display:flex><span>        default_chain<span style=color:#ff79c6>=</span>default_chain,
</span></span><span style=display:flex><span>        verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 12.运行路由链</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(chain<span style=color:#ff79c6>.</span>invoke(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span>{<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;1+3等于多少?&#34;</span>}))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(chain<span style=color:#ff79c6>.</span>invoke(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span>{<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;黑洞是什么?&#34;</span>}))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(chain<span style=color:#ff79c6>.</span>invoke(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span>{<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;五代十国是什么?&#34;</span>}))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(chain<span style=color:#ff79c6>.</span>invoke(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span>{<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;最流行的编程语言是什么?&#34;</span>}))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(chain<span style=color:#ff79c6>.</span>invoke(<span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span>{<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;你喜欢什么?&#34;</span>}))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; Entering new MultiPromptChain chain...
</span></span><span style=display:flex><span>数学: {&#39;input&#39;: &#39;1 + 3 等于多少?&#39;}
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>{&#39;input&#39;: &#39;1 + 3 等于多少?&#39;, &#39;text&#39;: &#39;1 + 3 = 4.&#39;}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Entering new MultiPromptChain chain...
</span></span><span style=display:flex><span>物理学: {&#39;input&#39;: &#39;黑洞是什么?&#39;}
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>{&#39;input&#39;: &#39;黑洞是什么?&#39;, &#39;text&#39;: &#39;黑洞是宇宙中一种非常密集的天体，它的引力非常强大，甚至连光都无法逃离它的吸引力。黑洞形成于恒星死亡时，其质量非常大，体积非常小，因此被称为“黑洞”。在黑洞的事件视界内，引力非常强大，甚至时间和空间都会被扭曲。目前科学家对黑洞的研究仍在进行中，仍有很多未解之谜。&#39;}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Entering new MultiPromptChain chain...
</span></span><span style=display:flex><span>历史: {&#39;input&#39;: &#39;五代十国是中国历史上的一个时期，指的是五代时期和十国时期的合称。&#39;}
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>{&#39;input&#39;: &#39;五代十国是中国历史上的一个时期，指的是五代时期和十国时期的合称。&#39;, &#39;text&#39;: &#39;请问你对五代十国时期的政治、经济和文化特点有什么深入的见解和分析？你认为这个时期对中国历史的发展有着怎样的影响？&#39;}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Entering new MultiPromptChain chain...
</span></span><span style=display:flex><span>计算机科学: {&#39;input&#39;: &#39;最流行的编程语言是什么?&#39;}
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>{&#39;input&#39;: &#39;最流行的编程语言是什么?&#39;, &#39;text&#39;: &#39;目前最流行的编程语言之一是Python。Python是一种简单易学、功能强大的编程语言，被广泛用于数据科学、人工智能、Web开发等领域。它具有丰富的库和工具，使得开发人员可以快速高效地完成各种任务。另外，JavaScript、Java、C++、C#等编程语言也在不同领域有着广泛的应用和较高的流行度。&#39;}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Entering new MultiPromptChain chain...
</span></span><span style=display:flex><span>None: {&#39;input&#39;: &#39;你喜欢什么?&#39;}
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>{&#39;input&#39;: &#39;你喜欢什么?&#39;, &#39;text&#39;: &#39;作为一个AI助手，我没有情感和喜好，我只是一个程序，可以帮助您解决问题和提供信息。请问有什么可以帮助您的吗？&#39;}
</span></span></code></pre></div><h3 id=检索问答链>检索问答链</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.document_loaders.csv_loader <span style=color:#ff79c6>import</span> CSVLoader
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.vectorstores.docarray <span style=color:#ff79c6>import</span> DocArrayInMemorySearch
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> OpenAIEmbeddings
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> RetrievalQA
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用LangChain文档加载器csv类型对数据进行导入, csv表格自造数据</span>
</span></span><span style=display:flex><span>    file <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
</span></span><span style=display:flex><span>    csv_loader <span style=color:#ff79c6>=</span> CSVLoader(file_path<span style=color:#ff79c6>=</span>file)
</span></span><span style=display:flex><span>    docs <span style=color:#ff79c6>=</span> csv_loader<span style=color:#ff79c6>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用OpenAI的向量嵌入</span>
</span></span><span style=display:flex><span>    embedding <span style=color:#ff79c6>=</span> OpenAIEmbeddings(
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化向量存储，文档列表、向量嵌入作为参数</span>
</span></span><span style=display:flex><span>    vector_db <span style=color:#ff79c6>=</span> DocArrayInMemorySearch<span style=color:#ff79c6>.</span>from_documents(docs, embedding)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用OpenAI语言模型</span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用检索问答链来回答问题，基于向量存储创建检索器</span>
</span></span><span style=display:flex><span>    retriever <span style=color:#ff79c6>=</span> vector_db<span style=color:#ff79c6>.</span>as_retriever()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># from_chain_type参数说明：</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># llm：语言模型</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># retriever：检索器</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># chain_type：链类型</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># chain_type = stuff, 是将所有查询得到的文档组合成一个文档传入下一步</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># chain_type = map_reduce, 将所有块与问题一起传递给语言模型，获取回复，使用另一个语言模型调用将所有单独的回复总结成最终答案，它可以在任意数量的文档上运行。可以并行处理单个问题，同时也需要更多的调用。它将所有文档视为独立的</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># chain_type = refine, 用于循环许多文档，际上是迭代的，建立在先前文档的答案之上，非常适合前后因果信息并随时间逐步构建答案，依赖于先前调用的结果。它通常需要更长的时间，并且基本上需要与map_reduce一样多的调用</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># chain_type = map_rerank, 对每个文档进行单个语言模型调用，要求它返回一个分数，选择最高分，这依赖于语言模型知道分数应该是什么，需要告诉它，如果它与文档相关，则应该是高分</span>
</span></span><span style=display:flex><span>    retrieva_qa <span style=color:#ff79c6>=</span> RetrievalQA<span style=color:#ff79c6>.</span>from_chain_type(
</span></span><span style=display:flex><span>        llm<span style=color:#ff79c6>=</span>llm,
</span></span><span style=display:flex><span>        retriever<span style=color:#ff79c6>=</span>retriever,
</span></span><span style=display:flex><span>        chain_type<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;stuff&#34;</span>,
</span></span><span style=display:flex><span>        verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    query <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;请用markdown表格的方式列出所有跟云相关的标题，并对每个标题进行抽象总结&#34;</span>
</span></span><span style=display:flex><span>    result <span style=color:#ff79c6>=</span> retrieva_qa({<span style=color:#f1fa8c>&#34;query&#34;</span>: query})
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(result[<span style=color:#f1fa8c>&#39;result&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>| 标题                                       | 抽象总结                           |
|--------------------------------------------|------------------------------------|
| 一图带你看懂云原生                        | 通过图示解释云原生的概念和特点     |
| 云原生可观测平台国际产品调研          | 调研国际市场上的云原生可观测平台产品 |
| 云原生可观测平台国内产品调研          | 调研国内市场上的云原生可观测平台产品 |
| 云平台前端框架方案          | 探讨云平台前端框架解决方案 |
</code></pre><h2 id=基于文档的问答>基于文档的问答</h2><h3 id=直接使用向量存储查询>直接使用向量存储查询</h3><p>使用大语言模型构建一个基于给定文档和文档集合的问答系统是一种非常经典的应用场景。基于文档问答的这个实现，涉及到LangChain的其它组件，
比如：嵌入模型(Embedding Models)和向量存储。</p><blockquote><p>大型深度学习模型中的嵌入(Embedding)是指将高维度输入数据（如文本或图像）映射到低维度空间的向量表示。在自然语言处理（NLP）中，嵌入
通常用于将单词或短语映射到向量空间中的连续值，以便进行文本分类、情感分析、机器翻译等任务。</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.document_loaders.csv_loader <span style=color:#ff79c6>import</span> CSVLoader
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.indexes <span style=color:#ff79c6>import</span> VectorstoreIndexCreator
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.vectorstores.docarray <span style=color:#ff79c6>import</span> DocArrayInMemorySearch
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> OpenAIEmbeddings
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 1.初始化语言模型</span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 2.使用LangChain文档加载器csv类型对数据进行导入, csv数据可以自行创造</span>
</span></span><span style=display:flex><span>    file <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
</span></span><span style=display:flex><span>    csv_loader <span style=color:#ff79c6>=</span> CSVLoader(file_path<span style=color:#ff79c6>=</span>file)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 3.基于文档加载器创建LangChain向量存储索引，这里使用向量内存存储</span>
</span></span><span style=display:flex><span>    index <span style=color:#ff79c6>=</span> VectorstoreIndexCreator(
</span></span><span style=display:flex><span>        vectorstore_cls<span style=color:#ff79c6>=</span>DocArrayInMemorySearch,
</span></span><span style=display:flex><span>        embedding<span style=color:#ff79c6>=</span>OpenAIEmbeddings(
</span></span><span style=display:flex><span>            api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>            base_url<span style=color:#ff79c6>=</span>openai_url))<span style=color:#ff79c6>.</span>from_loaders([csv_loader])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 4.查询创建的向量存储，问题要跟csv内容有所关联</span>
</span></span><span style=display:flex><span>    query <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;请用markdown表格的方式列出所有跟云相关的标题，并对每个标题进行抽象总结&#34;</span>
</span></span><span style=display:flex><span>    response <span style=color:#ff79c6>=</span> index<span style=color:#ff79c6>.</span>query(question<span style=color:#ff79c6>=</span>query, llm<span style=color:#ff79c6>=</span>llm)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(response)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>| 标题                        | 抽象总结         |
|----------------------------|-----------------|
| 一图带你看懂云原生            | 介绍云原生概念和特点   |
| 可观测平台国际产品调研         | 可观测平台国际产品调研 |
| 可观测平台国内产品调研         | 可观测平台国际产品调研 |
| 云平台前端框架方案            | 云平台前端框架方案介绍 |
</code></pre><h3 id=向量嵌入和向量存储>向量嵌入和向量存储</h3><p>大语言模型有上下文长度限制，直接处理长文档有点困难。要想实现长文档的问答，需引入向量嵌入(Embeddings)和向量存储(VectorStore)等技术；如何构建处理大规模长文档的问答系统？
1.使用Embeddings算法对文档进行向量化，语义相近的文本片段用相近的向量表示。
2.将向量化的文档切为小块，存入向量数据库；向量数据库对各文档片段进行索引，支持快速检索。</p><p>使用向量技术架构的话，当用户提问时，先将问题转化为向量，在向量数据库中快速查找到语义最相关的文档片段，然后再把这些文档片段和问题
一起发送给语言模型，返回生成的回答。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.document_loaders.csv_loader <span style=color:#ff79c6>import</span> CSVLoader
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.vectorstores.docarray <span style=color:#ff79c6>import</span> DocArrayInMemorySearch
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> OpenAIEmbeddings
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> RetrievalQA
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用LangChain文档加载器csv类型对数据进行导入</span>
</span></span><span style=display:flex><span>    file <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
</span></span><span style=display:flex><span>    csv_loader <span style=color:#ff79c6>=</span> CSVLoader(file_path<span style=color:#ff79c6>=</span>file)
</span></span><span style=display:flex><span>    docs <span style=color:#ff79c6>=</span> csv_loader<span style=color:#ff79c6>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用OpenAI的向量嵌入</span>
</span></span><span style=display:flex><span>    embedding <span style=color:#ff79c6>=</span> OpenAIEmbeddings(
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化向量存储，文档列表、向量嵌入作为参数</span>
</span></span><span style=display:flex><span>    vector_db <span style=color:#ff79c6>=</span> DocArrayInMemorySearch<span style=color:#ff79c6>.</span>from_documents(docs, embedding)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 返回一个文档列表，默认返回4个最相近语义的文档</span>
</span></span><span style=display:flex><span>    docs <span style=color:#ff79c6>=</span> vector_db<span style=color:#ff79c6>.</span>similarity_search(<span style=color:#f1fa8c>&#34;推荐一篇跟存储相关的文章&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;返回文档的数量：</span><span style=color:#f1fa8c>{0}</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span><span style=color:#ff79c6>.</span>format(<span style=color:#8be9fd;font-style:italic>len</span>(docs)))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;第一个文档是：</span><span style=color:#f1fa8c>{0}</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span><span style=color:#ff79c6>.</span>format(docs[<span style=color:#bd93f9>0</span>]))
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;第二个文档是：</span><span style=color:#f1fa8c>{0}</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span><span style=color:#ff79c6>.</span>format(docs[<span style=color:#bd93f9>1</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用OpenAI语言模型</span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 把返回的文档列表，构造成提示发送给语言模型来回答</span>
</span></span><span style=display:flex><span>    qdocs <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;</span><span style=color:#ff79c6>.</span>join([docs[i]<span style=color:#ff79c6>.</span>page_content <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(docs))])
</span></span><span style=display:flex><span>    response <span style=color:#ff79c6>=</span> llm<span style=color:#ff79c6>.</span>invoke(
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span>qdocs<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>问题：请用markdown表格的方式列出所有跟云相关的标题，并对每个标题进行抽象总结&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(response<span style=color:#ff79c6>.</span>content)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用检索问答链来回答问题，基于向量存储创建检索器</span>
</span></span><span style=display:flex><span>    retriever <span style=color:#ff79c6>=</span> vector_db<span style=color:#ff79c6>.</span>as_retriever()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    retrieva_qa <span style=color:#ff79c6>=</span> RetrievalQA<span style=color:#ff79c6>.</span>from_chain_type(
</span></span><span style=display:flex><span>        llm<span style=color:#ff79c6>=</span>llm,
</span></span><span style=display:flex><span>        retriever<span style=color:#ff79c6>=</span>retriever,
</span></span><span style=display:flex><span>        chain_type<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;stuff&#34;</span>,
</span></span><span style=display:flex><span>        verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    query <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;请用markdown表格的方式列出所有跟云相关的标题，并对每个标题进行抽象总结&#34;</span>
</span></span><span style=display:flex><span>    result <span style=color:#ff79c6>=</span> retrieva_qa({<span style=color:#f1fa8c>&#34;query&#34;</span>: query})
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(result[<span style=color:#f1fa8c>&#39;result&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>返回文档的数量：4

第一个文档是：page_content=&#39;\ufeff标题: 一文快速部署并配置普罗米远端存储——VictoriaMetrics\n链接: https://xxx.com/teams/k100017/docs/f54d5dbc83f811eda4b5b6df5a597271?company_from=6df6b7dadb4311e880ee5254002b9121\n作者: xxx\n领域: 云原生&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 31}

第二个文档是：page_content=&#39;\ufeff标题: 高质量的技术分享应该包含哪些内容？\n链接: https://xxx.com/teams/k100017/docs/57540e2ef0da11eb8adaaad8bc976c66?company_from=6df6b7dadb4311e880ee5254002b9121\n作者: xxx\n领域: 工程规范&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 67}

| 标题                                                   | 抽象总结                               |
|--------------------------------------------------------|----------------------------------------|
| 一文快速部署并配置普罗米远端存储——VictoriaMetrics     | 部署和配置远端存储的快速指南             |
| 高质量的技术分享应该包含哪些内容？                       | 技术分享内容的要素和标准                 |
| 一文浅析kubernetes event及其持久化方案                  | 分析Kubernetes事件及其持久化解决方案     |
| 你不知道的Postman效率提升技巧                          | 提升使用Postman工具效率的技巧           |
</code></pre><p>这里用到了LangChain的检索问答链</p><h2 id=评估evaluation>评估Evaluation</h2><p>评估是检验语言模型问答质量的关键环节。评估可以检验语言模型在不同文档上的问答效果，还可以通过比较不同模型，选择最佳系统。此外，定期评估也可以检查模型质量的衰减。评估通常有两个目的：</p><ul><li>检验LLM应用是否达到了验收标准</li><li>分析改动对于LLM应用性能的影响</li></ul><p>基本的思路就是利用语言模型本身和链本身，来辅助评估其他的语言模型、链和应用程序。</p><p>excel样本数据：</p><table><thead><tr><th>产品名称</th><th>产品类型</th><th>产品简介</th><th>适用场景</th><th>融资主体</th><th>融资额度</th><th>融资期限</th><th>融资成本</th><th>担保方式</th><th>风险控制</th><th>优势</th><th>案例</th></tr></thead><tbody><tr><td>应收账款质押融资</td><td>动产融资</td><td>以应收账款为质押品获取融资</td><td>核心企业、中小企业</td><td>核心企业、中小企业</td><td>100万元以上</td><td>1个月-3年</td><td>5%-8%</td><td>应收账款质押、信用担保、保证担保等</td><td>应收账款真实性、债权清晰性、履约能力等</td><td>融资便捷、成本较低、提高资金利用率</td><td>某大型制造企业利用应收账款质押融资，获得了1000万元的流动资金，用于采购原材料，有效缓解了资金压力，促进生产经营。</td></tr><tr><td>仓单融资</td><td>动产融资</td><td>以仓单为质押品获取融资</td><td>核心企业、中小企业</td><td>核心企业、中小企业</td><td>100万元以上</td><td>1个月-3年</td><td>4%-7%</td><td>仓单质押、信用担保、保证担保等</td><td>货物真实性、权属清晰性、仓储安全等</td><td>融资便捷、成本较低、盘活存货资产</td><td>某贸易企业利用仓单融资，获得了500万元的流动资金，用于扩大进出口业务，提高了资金周转效率。</td></tr><tr><td>订单融资</td><td>信用融资</td><td>以订单为基础获取融资</td><td>核心企业、中小企业</td><td>核心企业、中小企业</td><td>100万元以上</td><td>1个月-1年</td><td>3%-6%</td><td>订单真实性、买方信用状况等</td><td>订单池管理、风险分散等</td><td>融资便捷、成本较低、提升供应链协同效率</td><td>某电商企业利用订单融资，获得了2000万元的流动资金，用于备货发货，满足了订单快速增长的需求。</td></tr><tr><td>动产抵押融资</td><td>动产融资</td><td>以动产（如设备、车辆等）为质押品获取融资</td><td>中小企业</td><td>中小企业</td><td>50万元以上</td><td>1个月-3年</td><td>5%-8%</td><td>动产抵押、信用担保、保证担保等</td><td>动产权属清晰性、评估价值等</td><td>融资便捷、提高资产利用率</td><td>某科技企业利用动产抵押融资，获得了100万元的流动资金，用于研发新产品，提升了企业竞争力。</td></tr><tr><td>保单融资</td><td>信用融资</td><td>以保单为质押品获取融资</td><td>核心企业、中小企业</td><td>核心企业、中小企业</td><td>100万元以上</td><td>1个月-1年</td><td>3%-6%</td><td>保单质押、信用担保、保证担保等</td><td>保单真实性、保费支付记录等</td><td>融资便捷、成本较低、盘活保单资产</td><td>某制造企业利用保单融资，获得了500万元的流动资金，用于采购原材料，降低了融资成本。</td></tr><tr><td>流水贷款</td><td>信用融资</td><td>以企业历史经营数据为基础获取融资</td><td>核心企业、中小企业</td><td>核心企业、中小企业</td><td>100万元以上</td><td>1个月-3年</td><td>4%-7%</td><td>企业财务数据、经营状况等</td><td>信用评级、风险监控等</td><td>融资便捷、无需抵押、手续简便</td><td>某零售企业利用流水贷款，获得了200万元的流动资金，</td></tr></tbody></table><h3 id=创建待评估的llm应用>创建待评估的LLM应用</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.document_loaders.csv_loader <span style=color:#ff79c6>import</span> CSVLoader
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用LangChain文档加载器csv类型对数据进行导入</span>
</span></span><span style=display:flex><span>    file <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
</span></span><span style=display:flex><span>    csv_loader <span style=color:#ff79c6>=</span> CSVLoader(file_path<span style=color:#ff79c6>=</span>file)
</span></span><span style=display:flex><span>    docs <span style=color:#ff79c6>=</span> csv_loader<span style=color:#ff79c6>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用OpenAI语言模型</span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 打印几条样本数据</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(docs[<span style=color:#bd93f9>0</span>:<span style=color:#bd93f9>5</span>])):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(docs[i]<span style=color:#ff79c6>.</span>page_content <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>产品名称: 应收账款质押融资
</span></span><span style=display:flex><span>产品类型: 动产融资
</span></span><span style=display:flex><span>产品简介: 以应收账款为质押品获取融资
</span></span><span style=display:flex><span>适用场景: 核心企业、中小企业
</span></span><span style=display:flex><span>融资主体: 核心企业、中小企业
</span></span><span style=display:flex><span>融资额度: 100万元以上
</span></span><span style=display:flex><span>融资期限: 1个月-3年
</span></span><span style=display:flex><span>融资成本: 5%-8%
</span></span><span style=display:flex><span>担保方式: 应收账款质押、信用担保、保证担保等
</span></span><span style=display:flex><span>风险控制: 应收账款真实性、债权清晰性、履约能力等
</span></span><span style=display:flex><span>优势: 融资便捷、成本较低、提高资金利用率
</span></span><span style=display:flex><span>案例: 某大型制造企业利用应收账款质押融资，获得了1000万元的流动资金，用于采购原材料，有效缓解了资金压力，促进生产经营。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>产品名称: 仓单融资
</span></span><span style=display:flex><span>产品类型: 动产融资
</span></span><span style=display:flex><span>产品简介: 以仓单为质押品获取融资
</span></span><span style=display:flex><span>适用场景: 核心企业、中小企业
</span></span><span style=display:flex><span>融资主体: 核心企业、中小企业
</span></span><span style=display:flex><span>融资额度: 100万元以上
</span></span><span style=display:flex><span>融资期限: 1个月-3年
</span></span><span style=display:flex><span>融资成本: 4%-7%
</span></span><span style=display:flex><span>担保方式: 仓单质押、信用担保、保证担保等
</span></span><span style=display:flex><span>风险控制: 货物真实性、权属清晰性、仓储安全等
</span></span><span style=display:flex><span>优势: 融资便捷、成本较低、盘活存货资产
</span></span><span style=display:flex><span>案例: 某贸易企业利用仓单融资，获得了500万元的流动资金，用于扩大进出口业务，提高了资金周转效率。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>产品名称: 订单融资
</span></span><span style=display:flex><span>产品类型: 信用融资
</span></span><span style=display:flex><span>产品简介: 以订单为基础获取融资
</span></span><span style=display:flex><span>适用场景: 核心企业、中小企业
</span></span><span style=display:flex><span>融资主体: 核心企业、中小企业
</span></span><span style=display:flex><span>融资额度: 100万元以上
</span></span><span style=display:flex><span>融资期限: 1个月-1年
</span></span><span style=display:flex><span>融资成本: 3%-6%
</span></span><span style=display:flex><span>担保方式: 订单真实性、买方信用状况等
</span></span><span style=display:flex><span>风险控制: 订单池管理、风险分散等
</span></span><span style=display:flex><span>优势: 融资便捷、成本较低、提升供应链协同效率
</span></span><span style=display:flex><span>案例: 某电商企业利用订单融资，获得了2000万元的流动资金，用于备货发货，满足了订单快速增长的需求。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>产品名称: 动产抵押融资
</span></span><span style=display:flex><span>产品类型: 动产融资
</span></span><span style=display:flex><span>产品简介: 以动产（如设备、车辆等）为质押品获取融资
</span></span><span style=display:flex><span>适用场景: 中小企业
</span></span><span style=display:flex><span>融资主体: 中小企业
</span></span><span style=display:flex><span>融资额度: 50万元以上
</span></span><span style=display:flex><span>融资期限: 1个月-3年
</span></span><span style=display:flex><span>融资成本: 5%-8%
</span></span><span style=display:flex><span>担保方式: 动产抵押、信用担保、保证担保等
</span></span><span style=display:flex><span>风险控制: 动产权属清晰性、评估价值等
</span></span><span style=display:flex><span>优势: 融资便捷、提高资产利用率
</span></span><span style=display:flex><span>案例: 某科技企业利用动产抵押融资，获得了100万元的流动资金，用于研发新产品，提升了企业竞争力。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>产品名称: 保单融资
</span></span><span style=display:flex><span>产品类型: 信用融资
</span></span><span style=display:flex><span>产品简介: 以保单为质押品获取融资
</span></span><span style=display:flex><span>适用场景: 核心企业、中小企业
</span></span><span style=display:flex><span>融资主体: 核心企业、中小企业
</span></span><span style=display:flex><span>融资额度: 100万元以上
</span></span><span style=display:flex><span>融资期限: 1个月-1年
</span></span><span style=display:flex><span>融资成本: 3%-6%
</span></span><span style=display:flex><span>担保方式: 保单质押、信用担保、保证担保等
</span></span><span style=display:flex><span>风险控制: 保单真实性、保费支付记录等
</span></span><span style=display:flex><span>优势: 融资便捷、成本较低、盘活保单资产
</span></span><span style=display:flex><span>案例: 某制造企业利用保单融资，获得了500万元的流动资金，用于采购原材料，降低了融资成本。
</span></span></code></pre></div><h4 id=手动创建测试用例>手动创建测试用例</h4><p>这里文档格式是csv文件，CSVLoader对文件的每一行数据进行分割，根据输出的格式手动设置几条问答对</p><pre tabindex=0><code>examples = [
    {
        &#34;query&#34;: &#34;仓单融资的产品的优点是什么?&#34;,
        &#34;answer&#34;: &#34;成本低、操作便捷&#34;
    },
    {
        &#34;query&#34;: &#34;订单融资产品适用哪些企业?&#34;,
        &#34;answer&#34;: &#34;有实力的核心企业，还有一些中小企业&#34;
    }
]
</code></pre><h4 id=llm自动生成测试用例>LLM自动生成测试用例</h4><p>一个模型评估的大致流程：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>手动创建问题和答案 -&gt; 使用LLM自动创建问答测试用例 -&gt; 使用同一个LLM回答 -&gt; 让另一个LLM进行答案判断
</span></span></code></pre></div><p>借助LangChain的<code>QAGenerateChain</code>可以自动创建大量问答测试集，自动化评估是LangChain框架的一大优势，极大降低开发RAG系统的门槛。
原生的QAGenerateChain只支持中文，这里需要继承下QAGenerateChain类，然后重写下from_llm方法</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.document_loaders.csv_loader <span style=color:#ff79c6>import</span> CSVLoader
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.evaluation.qa <span style=color:#ff79c6>import</span> QAGenerateChain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.base_language <span style=color:#ff79c6>import</span> BaseLanguageModel
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> PromptTemplate
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> typing <span style=color:#ff79c6>import</span> Any
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>template <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;You are a teacher coming up with questions to ask on a quiz.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Given the following document, please generate a question and answer based on that document.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Example Format:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;Begin Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>...
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;End Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>QUESTION: question here
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>ANSWER: answer here
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>These questions should be detailed and be based explicitly on information in the document. Begin!
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;Begin Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{doc}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;End Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>请使用中文输出
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>PROMPT <span style=color:#ff79c6>=</span> PromptTemplate(
</span></span><span style=display:flex><span>    input_variables<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;doc&#34;</span>],
</span></span><span style=display:flex><span>    template<span style=color:#ff79c6>=</span>template,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 继承QAGenerateChain，重写from_llm方法</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>ZhCNQAGenerateChain</span>(QAGenerateChain):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;LLM Chain for generating examples for question answering.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    @classmethod
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>from_llm</span>(cls, llm: BaseLanguageModel, <span style=color:#ff79c6>**</span>kwargs: Any) <span style=color:#ff79c6>-&gt;</span> QAGenerateChain:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Load QA Generate Chain from LLM.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> cls(llm<span style=color:#ff79c6>=</span>llm, prompt<span style=color:#ff79c6>=</span>PROMPT, <span style=color:#ff79c6>**</span>kwargs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用LangChain文档加载器csv类型对数据进行导入</span>
</span></span><span style=display:flex><span>    file <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
</span></span><span style=display:flex><span>    csv_loader <span style=color:#ff79c6>=</span> CSVLoader(file_path<span style=color:#ff79c6>=</span>file)
</span></span><span style=display:flex><span>    docs <span style=color:#ff79c6>=</span> csv_loader<span style=color:#ff79c6>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用OpenAI语言模型</span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 创建ZhCNQAGenerateChain链</span>
</span></span><span style=display:flex><span>    sample_qa_chain <span style=color:#ff79c6>=</span> ZhCNQAGenerateChain<span style=color:#ff79c6>.</span>from_llm(llm)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 调用apply方法自动创建问答对</span>
</span></span><span style=display:flex><span>    examples <span style=color:#ff79c6>=</span> sample_qa_chain<span style=color:#ff79c6>.</span>apply([{<span style=color:#f1fa8c>&#34;doc&#34;</span>: t} <span style=color:#ff79c6>for</span> t <span style=color:#ff79c6>in</span> docs])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 打印问答对</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> examples:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(i[<span style=color:#f1fa8c>&#39;qa_pairs&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>{&#39;query&#39;: &#39;什么是“应收账款质押融资”产品的主要特点和优势？ &#39;, &#39;answer&#39;: &#39;“应收账款质押融资”产品的主要特点包括产品类型为动产融资，适用场景为核心企业和中小企业，融资额度为100万元以上，融资期限为1个月至3年，融资成本为5%-8%，担保方式包括应收账款质押、信用担保、保证担保等。其优势在于融资便捷、成本较低、提高资金利用率。&#39;}
</span></span><span style=display:flex><span>{&#39;query&#39;: &#39;仓单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？&#39;, &#39;answer&#39;: &#39;仓单融资产品适用于核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。&#39;}
</span></span><span style=display:flex><span>{&#39;query&#39;: &#39;什么是订单融资的产品简介和适用场景？融资额度和期限是多少？融资成本是多少？担保方式和风险控制措施是什么？&#39;, &#39;answer&#39;: &#39;订单融资是以订单为基础获取融资的信用融资产品，适用于核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年，融资成本为3%-6%。担保方式包括订单真实性和买方信用状况等，风险控制措施包括订单池管理和风险分散。&#39;}
</span></span><span style=display:flex><span>{&#39;query&#39;: &#39;什么是动产抵押融资的产品类型和适用场景？&#39;, &#39;answer&#39;: &#39;动产抵押融资的产品类型是动产融资，适用场景是中小企业。&#39;}
</span></span><span style=display:flex><span>{&#39;query&#39;: &#39;保单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？&#39;, &#39;answer&#39;: &#39;保单融资产品适用于核心企业和中小企业，融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年。&#39;}
</span></span><span style=display:flex><span>{&#39;query&#39;: &#39;流水贷款的产品类型是什么？融资额度是多少？融资期限是多久？&#39;, &#39;answer&#39;: &#39;流水贷款的产品类型是信用融资，融资额度为100万元以上，融资期限为1个月至3年。&#39;}
</span></span></code></pre></div><h3 id=人工评估>人工评估</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># flake8: noqa</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.globals <span style=color:#ff79c6>import</span> set_debug
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.document_loaders.csv_loader <span style=color:#ff79c6>import</span> CSVLoader
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.vectorstores.docarray <span style=color:#ff79c6>import</span> DocArrayInMemorySearch
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> OpenAIEmbeddings
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.indexes <span style=color:#ff79c6>import</span> VectorstoreIndexCreator
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> RetrievalQA
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.evaluation.qa <span style=color:#ff79c6>import</span> QAGenerateChain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.base_language <span style=color:#ff79c6>import</span> BaseLanguageModel
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> PromptTemplate
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> typing <span style=color:#ff79c6>import</span> Any
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>template <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;You are a teacher coming up with questions to ask on a quiz.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Given the following document, please generate a question and answer based on that document.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Example Format:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;Begin Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>...
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;End Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>QUESTION: question here
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>ANSWER: answer here
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>These questions should be detailed and be based explicitly on information in the document. Begin!
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;Begin Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{doc}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;End Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>请使用中文输出
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>PROMPT <span style=color:#ff79c6>=</span> PromptTemplate(
</span></span><span style=display:flex><span>    input_variables<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;doc&#34;</span>],
</span></span><span style=display:flex><span>    template<span style=color:#ff79c6>=</span>template,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>examples <span style=color:#ff79c6>=</span> [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#39;query&#39;</span>: <span style=color:#f1fa8c>&#39;仓单融资的产品的优点是什么?&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#39;answer&#39;</span>: <span style=color:#f1fa8c>&#39;成本低、操作便捷&#39;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#39;query&#39;</span>: <span style=color:#f1fa8c>&#39;订单融资产品适用哪些企业?&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#39;answer&#39;</span>: <span style=color:#f1fa8c>&#39;有实力的核心企业，还有一些中小企业&#39;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 继承QAGenerateChain，重写from_llm方法</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>ZhCNQAGenerateChain</span>(QAGenerateChain):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;LLM Chain for generating examples for question answering.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    @classmethod
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>from_llm</span>(cls, llm: BaseLanguageModel, <span style=color:#ff79c6>**</span>kwargs: Any) <span style=color:#ff79c6>-&gt;</span> QAGenerateChain:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Load QA Generate Chain from LLM.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> cls(llm<span style=color:#ff79c6>=</span>llm, prompt<span style=color:#ff79c6>=</span>PROMPT, <span style=color:#ff79c6>**</span>kwargs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 开启LangChain全局debug</span>
</span></span><span style=display:flex><span>    set_debug(<span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用LangChain文档加载器csv类型对数据进行导入</span>
</span></span><span style=display:flex><span>    file <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
</span></span><span style=display:flex><span>    csv_loader <span style=color:#ff79c6>=</span> CSVLoader(file_path<span style=color:#ff79c6>=</span>file)
</span></span><span style=display:flex><span>    docs <span style=color:#ff79c6>=</span> csv_loader<span style=color:#ff79c6>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 基于文档加载器创建LangChain向量存储索引，这里使用向量内存存储</span>
</span></span><span style=display:flex><span>    index <span style=color:#ff79c6>=</span> VectorstoreIndexCreator(
</span></span><span style=display:flex><span>        vectorstore_cls<span style=color:#ff79c6>=</span>DocArrayInMemorySearch,
</span></span><span style=display:flex><span>        embedding<span style=color:#ff79c6>=</span>OpenAIEmbeddings(
</span></span><span style=display:flex><span>            api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>            base_url<span style=color:#ff79c6>=</span>openai_url))<span style=color:#ff79c6>.</span>from_loaders([csv_loader])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 使用OpenAI语言模型</span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 创建检索QA链</span>
</span></span><span style=display:flex><span>    retrieval_qa_chain <span style=color:#ff79c6>=</span> RetrievalQA<span style=color:#ff79c6>.</span>from_chain_type(
</span></span><span style=display:flex><span>        llm<span style=color:#ff79c6>=</span>llm,
</span></span><span style=display:flex><span>        retriever<span style=color:#ff79c6>=</span>index<span style=color:#ff79c6>.</span>vectorstore<span style=color:#ff79c6>.</span>as_retriever(),
</span></span><span style=display:flex><span>        chain_type<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;stuff&#34;</span>,
</span></span><span style=display:flex><span>        verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 创建ZhCNQAGenerateChain链</span>
</span></span><span style=display:flex><span>    sample_qa_chain <span style=color:#ff79c6>=</span> ZhCNQAGenerateChain<span style=color:#ff79c6>.</span>from_llm(llm)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 调用apply方法自动创建问答对</span>
</span></span><span style=display:flex><span>    llm_examples <span style=color:#ff79c6>=</span> sample_qa_chain<span style=color:#ff79c6>.</span>apply([{<span style=color:#f1fa8c>&#34;doc&#34;</span>: t} <span style=color:#ff79c6>for</span> t <span style=color:#ff79c6>in</span> docs])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 整合测试用例，将手动测试用例和LLM测试用例合并</span>
</span></span><span style=display:flex><span>    llm_examples <span style=color:#ff79c6>=</span> [v <span style=color:#ff79c6>for</span> item <span style=color:#ff79c6>in</span> llm_examples <span style=color:#ff79c6>for</span> _, v <span style=color:#ff79c6>in</span> item<span style=color:#ff79c6>.</span>items()]
</span></span><span style=display:flex><span>    new_examples <span style=color:#ff79c6>=</span> examples <span style=color:#ff79c6>+</span> llm_examples
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 打印手动测试用例的第一个问题的LLM答案</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(retrieval_qa_chain<span style=color:#ff79c6>.</span>invoke({<span style=color:#f1fa8c>&#34;query&#34;</span>: examples[<span style=color:#bd93f9>0</span>][<span style=color:#f1fa8c>&#39;query&#39;</span>]})) <span style=color:#6272a4># type: ignore[misc]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[</span>chain/start<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain<span style=color:#ff79c6>]</span> Entering Chain run with input:
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>inputs<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/start<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 2:llm:ChatOpenAI<span style=color:#ff79c6>]</span> Entering LLM run with input:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;prompts&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 应收账款质押融资\\n产品类型: 动产融资\\n产品简介: 以应收账款为质押品获取融资\\n适用场景: 核心企业、中小企业\\n融资主体: 核心企业、中小企业\\n融资额度: 100万元以上\\n融资期限: 1个月-3年\\n融资成本: 5%-8%\\n担保方式: 应收账款质押、信用担保、保证担保等\\n风险控制: 应收账款真实性、债权清晰性、履约能力等\\n优势: 融资便捷、成本较低、提高资金利用率\\n案例: 某大型制造企业利用应收账款质押融资，获得了1000万元的流动资金，用于采购原材料，有效缓解了资金压力，促进生产经营。&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 0}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/start<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 3:llm:ChatOpenAI<span style=color:#ff79c6>]</span> Entering LLM run with input:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;prompts&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 仓单融资\\n产品类型: 动产融资\\n产品简介: 以仓单为质押品获取融资\\n适用场景: 核心企业、中小企业\\n融资主体: 核心企业、中小企业\\n融资额度: 100万元以上\\n融资期限: 1个月-3年\\n融资成本: 4%-7%\\n担保方式: 仓单质押、信用担保、保证担保等\\n风险控制: 货物真实性、权属清晰性、仓储安全等\\n优势: 融资便捷、成本较低、盘活存货资产\\n案例: 某贸易企业利用仓单融资，获得了500万元的流动资金，用于扩大进出口业务，提高了资金周转效率。&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 1}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/start<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 4:llm:ChatOpenAI<span style=color:#ff79c6>]</span> Entering LLM run with input:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;prompts&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 订单融资\\n产品类型: 信用融资\\n产品简介: 以订单为基础获取融资\\n适用场景: 核心企业、中小企业\\n融资主体: 核心企业、中小企业\\n融资额度: 100万元以上\\n融资期限: 1个月-1年\\n融资成本: 3%-6%\\n担保方式: 订单真实性、买方信用状况等\\n风险控制: 订单池管理、风险分散等\\n优势: 融资便捷、成本较低、提升供应链协同效率\\n案例: 某电商企业利用订单融资，获得了2000万元的流动资金，用于备货发货，满足了订单快速增长的需求。&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 2}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/start<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 5:llm:ChatOpenAI<span style=color:#ff79c6>]</span> Entering LLM run with input:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;prompts&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 动产抵押融资\\n产品类型: 动产融资\\n产品简介: 以动产（如设备、车辆等）为质押品获取融资\\n适用场景: 中小企业\\n融资主体: 中小企业\\n融资额度: 50万元以上\\n融资期限: 1个月-3年\\n融资成本: 5%-8%\\n担保方式: 动产抵押、信用担保、保证担保等\\n风险控制: 动产权属清晰性、评估价值等\\n优势: 融资便捷、提高资产利用率\\n案例: 某科技企业利用动产抵押融资，获得了100万元的流动资金，用于研发新产品，提升了企业竞争力。&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 3}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/start<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 6:llm:ChatOpenAI<span style=color:#ff79c6>]</span> Entering LLM run with input:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;prompts&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 保单融资\\n产品类型: 信用融资\\n产品简介: 以保单为质押品获取融资\\n适用场景: 核心企业、中小企业\\n融资主体: 核心企业、中小企业\\n融资额度: 100万元以上\\n融资期限: 1个月-1年\\n融资成本: 3%-6%\\n担保方式: 保单质押、信用担保、保证担保等\\n风险控制: 保单真实性、保费支付记录等\\n优势: 融资便捷、成本较低、盘活保单资产\\n案例: 某制造企业利用保单融资，获得了500万元的流动资金，用于采购原材料，降低了融资成本。&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 4}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/start<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 7:llm:ChatOpenAI<span style=color:#ff79c6>]</span> Entering LLM run with input:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;prompts&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;Human: You are a teacher coming up with questions to ask on a quiz.\nGiven the following document, please generate a question and answer based on that document.\n\nExample Format:\n&lt;Begin Document&gt;\n...\n&lt;End Document&gt;\nQUESTION: question here\nANSWER: answer here\n\nThese questions should be detailed and be based explicitly on information in the document. Begin!\n\n&lt;Begin Document&gt;\npage_content=&#39;\\ufeff产品名称: 流水贷款\\n产品类型: 信用融资\\n产品简介: 以企业历史经营数据为基础获取融资\\n适用场景: 核心企业、中小企业\\n融资主体: 核心企业、中小企业\\n融资额度: 100万元以上\\n融资期限: 1个月-3年\\n融资成本: 4%-7%\\n担保方式: 企业财务数据、经营状况等\\n风险控制: 信用评级、风险监控等\\n优势: 融资便捷、无需抵押、手续简便\\n案例: 某零售企业利用流水贷款，获得了200万元的流动资金，&#39; metadata={&#39;source&#39;: &#39;/Users/iceyao/Desktop/test_101.csv&#39;, &#39;row&#39;: 5}\n&lt;End Document&gt;\n请使用中文输出&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/end<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 2:llm:ChatOpenAI<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>17.97s<span style=color:#ff79c6>]</span> Exiting LLM run with output:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;generations&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;text&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 什么是\&#34;应收账款质押融资\&#34;的产品类型？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？\nANSWER: \&#34;应收账款质押融资\&#34;的产品类型是动产融资。融资主体可以是核心企业或中小企业。融资额度为100万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括应收账款质押、信用担保、保证担保等。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;generation_info&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;finish_reason&#34;</span>: <span style=color:#f1fa8c>&#34;stop&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;logprobs&#34;</span>: null
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;ChatGeneration&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;message&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;lc&#34;</span>: 1,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;constructor&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;id&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;langchain&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;schema&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;messages&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;AIMessage&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;kwargs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 什么是\&#34;应收账款质押融资\&#34;的产品类型？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？\nANSWER: \&#34;应收账款质押融资\&#34;的产品类型是动产融资。融资主体可以是核心企业或中小企业。融资额度为100万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括应收账款质押、信用担保、保证担保等。&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;additional_kwargs&#34;</span>: <span style=color:#ff79c6>{}</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;llm_output&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;token_usage&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;completion_tokens&#34;</span>: 179,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;prompt_tokens&#34;</span>: 402,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;total_tokens&#34;</span>: <span style=color:#bd93f9>581</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;model_name&#34;</span>: <span style=color:#f1fa8c>&#34;gpt-3.5-turbo&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;system_fingerprint&#34;</span>: <span style=color:#f1fa8c>&#34;fp_4f0b692a78&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;run&#34;</span>: null
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/end<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 3:llm:ChatOpenAI<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>17.97s<span style=color:#ff79c6>]</span> Exiting LLM run with output:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;generations&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;text&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 仓单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？\nANSWER: 仓单融资产品适用于核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;generation_info&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;finish_reason&#34;</span>: <span style=color:#f1fa8c>&#34;stop&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;logprobs&#34;</span>: null
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;ChatGeneration&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;message&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;lc&#34;</span>: 1,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;constructor&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;id&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;langchain&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;schema&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;messages&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;AIMessage&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;kwargs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 仓单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？\nANSWER: 仓单融资产品适用于核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;additional_kwargs&#34;</span>: <span style=color:#ff79c6>{}</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;llm_output&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;token_usage&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;completion_tokens&#34;</span>: 175,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;prompt_tokens&#34;</span>: 379,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;total_tokens&#34;</span>: <span style=color:#bd93f9>554</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;model_name&#34;</span>: <span style=color:#f1fa8c>&#34;gpt-3.5-turbo&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;system_fingerprint&#34;</span>: <span style=color:#f1fa8c>&#34;fp_4f0b692a78&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;run&#34;</span>: null
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/end<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 4:llm:ChatOpenAI<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>17.97s<span style=color:#ff79c6>]</span> Exiting LLM run with output:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;generations&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;text&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 产品名称为什么是订单融资？产品类型是什么？产品简介是什么？\nANSWER: 产品名称是订单融资，因为该产品是以订单为基础获取融资。产品类型是信用融资。产品简介是以订单为基础获取融资。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;generation_info&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;finish_reason&#34;</span>: <span style=color:#f1fa8c>&#34;stop&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;logprobs&#34;</span>: null
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;ChatGeneration&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;message&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;lc&#34;</span>: 1,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;constructor&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;id&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;langchain&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;schema&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;messages&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;AIMessage&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;kwargs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 产品名称为什么是订单融资？产品类型是什么？产品简介是什么？\nANSWER: 产品名称是订单融资，因为该产品是以订单为基础获取融资。产品类型是信用融资。产品简介是以订单为基础获取融资。&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;additional_kwargs&#34;</span>: <span style=color:#ff79c6>{}</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;llm_output&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;token_usage&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;completion_tokens&#34;</span>: 87,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;prompt_tokens&#34;</span>: 362,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;total_tokens&#34;</span>: <span style=color:#bd93f9>449</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;model_name&#34;</span>: <span style=color:#f1fa8c>&#34;gpt-3.5-turbo&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;system_fingerprint&#34;</span>: <span style=color:#f1fa8c>&#34;fp_4f0b692a78&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;run&#34;</span>: null
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/end<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 5:llm:ChatOpenAI<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>17.97s<span style=color:#ff79c6>]</span> Exiting LLM run with output:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;generations&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;text&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 什么是动产抵押融资的产品类型和适用场景？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？\nANSWER: 动产抵押融资的产品类型是动产融资，适用场景是中小企业。融资主体是中小企业，融资额度为50万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括动产抵押、信用担保、保证担保等。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;generation_info&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;finish_reason&#34;</span>: <span style=color:#f1fa8c>&#34;stop&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;logprobs&#34;</span>: null
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;ChatGeneration&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;message&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;lc&#34;</span>: 1,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;constructor&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;id&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;langchain&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;schema&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;messages&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;AIMessage&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;kwargs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 什么是动产抵押融资的产品类型和适用场景？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？\nANSWER: 动产抵押融资的产品类型是动产融资，适用场景是中小企业。融资主体是中小企业，融资额度为50万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括动产抵押、信用担保、保证担保等。&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;additional_kwargs&#34;</span>: <span style=color:#ff79c6>{}</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;llm_output&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;token_usage&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;completion_tokens&#34;</span>: 181,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;prompt_tokens&#34;</span>: 367,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;total_tokens&#34;</span>: <span style=color:#bd93f9>548</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;model_name&#34;</span>: <span style=color:#f1fa8c>&#34;gpt-3.5-turbo&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;system_fingerprint&#34;</span>: <span style=color:#f1fa8c>&#34;fp_4f0b692a78&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;run&#34;</span>: null
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/end<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 6:llm:ChatOpenAI<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>17.98s<span style=color:#ff79c6>]</span> Exiting LLM run with output:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;generations&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;text&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 保单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？\nANSWER: 保单融资产品适用于核心企业和中小企业，融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;generation_info&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;finish_reason&#34;</span>: <span style=color:#f1fa8c>&#34;stop&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;logprobs&#34;</span>: null
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;ChatGeneration&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;message&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;lc&#34;</span>: 1,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;constructor&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;id&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;langchain&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;schema&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;messages&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;AIMessage&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;kwargs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 保单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？\nANSWER: 保单融资产品适用于核心企业和中小企业，融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年。&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;additional_kwargs&#34;</span>: <span style=color:#ff79c6>{}</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;llm_output&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;token_usage&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;completion_tokens&#34;</span>: 114,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;prompt_tokens&#34;</span>: 364,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;total_tokens&#34;</span>: <span style=color:#bd93f9>478</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;model_name&#34;</span>: <span style=color:#f1fa8c>&#34;gpt-3.5-turbo&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;system_fingerprint&#34;</span>: <span style=color:#f1fa8c>&#34;fp_4f0b692a78&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;run&#34;</span>: null
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/end<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain &gt; 7:llm:ChatOpenAI<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>17.98s<span style=color:#ff79c6>]</span> Exiting LLM run with output:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;generations&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;text&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 什么是流水贷款的产品类型和适用场景？\nANSWER: 流水贷款的产品类型是信用融资，适用场景是核心企业和中小企业。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;generation_info&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;finish_reason&#34;</span>: <span style=color:#f1fa8c>&#34;stop&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;logprobs&#34;</span>: null
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;ChatGeneration&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;message&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;lc&#34;</span>: 1,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;constructor&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;id&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;langchain&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;schema&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;messages&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;AIMessage&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;kwargs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;QUESTION: 什么是流水贷款的产品类型和适用场景？\nANSWER: 流水贷款的产品类型是信用融资，适用场景是核心企业和中小企业。&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;additional_kwargs&#34;</span>: <span style=color:#ff79c6>{}</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;llm_output&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;token_usage&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;completion_tokens&#34;</span>: 58,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;prompt_tokens&#34;</span>: 344,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;total_tokens&#34;</span>: <span style=color:#bd93f9>402</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;model_name&#34;</span>: <span style=color:#f1fa8c>&#34;gpt-3.5-turbo&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;system_fingerprint&#34;</span>: <span style=color:#f1fa8c>&#34;fp_4f0b692a78&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;run&#34;</span>: null
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>chain/end<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:ZhCNQAGenerateChain<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>18.01s<span style=color:#ff79c6>]</span> Exiting Chain run with output:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;outputs&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;qa_pairs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;query&#34;</span>: <span style=color:#f1fa8c>&#34;什么是\&#34;应收账款质押融资\&#34;的产品类型？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;\&#34;应收账款质押融资\&#34;的产品类型是动产融资。融资主体可以是核心企业或中小企业。融资额度为100万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括应收账款质押、信用担保、保证担保等。&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;qa_pairs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;query&#34;</span>: <span style=color:#f1fa8c>&#34;仓单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;仓单融资产品适用于核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;qa_pairs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;query&#34;</span>: <span style=color:#f1fa8c>&#34;产品名称为什么是订单融资？产品类型是什么？产品简介是什么？&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;产品名称是订单融资，因为该产品是以订单为基础获取融资。产品类型是信用融资。产品简介是以订单为基础获取融资。&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;qa_pairs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;query&#34;</span>: <span style=color:#f1fa8c>&#34;什么是动产抵押融资的产品类型和适用场景？融资主体是谁？融资额度和期限是多少？融资成本是多少？担保方式有哪些？&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;动产抵押融资的产品类型是动产融资，适用场景是中小企业。融资主体是中小企业，融资额度为50万元以上，融资期限为1个月至3年，融资成本为5%-8%。担保方式包括动产抵押、信用担保、保证担保等。&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;qa_pairs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;query&#34;</span>: <span style=color:#f1fa8c>&#34;保单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;保单融资产品适用于核心企业和中小企业，融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年。&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;qa_pairs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;query&#34;</span>: <span style=color:#f1fa8c>&#34;什么是流水贷款的产品类型和适用场景？&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;answer&#34;</span>: <span style=color:#f1fa8c>&#34;流水贷款的产品类型是信用融资，适用场景是核心企业和中小企业。&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>chain/start<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:RetrievalQA<span style=color:#ff79c6>]</span> Entering Chain run with input:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;query&#34;</span>: <span style=color:#f1fa8c>&#34;仓单融资的产品的优点是什么?&#34;</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>chain/start<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain<span style=color:#ff79c6>]</span> Entering Chain run with input:
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>inputs<span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>chain/start<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain<span style=color:#ff79c6>]</span> Entering Chain run with input:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;仓单融资的产品的优点是什么?&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;context&#34;</span>: <span style=color:#f1fa8c>&#34;产品名称: 仓单融资\n产品类型: 动产融资\n产品简介: 以仓单为质押品获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-3年\n融资成本: 4%-7%\n担保方式: 仓单质押、信用担保、保证担保等\n风险控制: 货物真实性、权属清晰性、仓储安全等\n优势: 融资便捷、成本较低、盘活存货资产\n案例: 某贸易企业利用仓单融资，获得了500万元的流动资金，用于扩大进出口业务，提高了资金周转效率。\n\n产品名称: 订单融资\n产品类型: 信用融资\n产品简介: 以订单为基础获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-1年\n融资成本: 3%-6%\n担保方式: 订单真实性、买方信用状况等\n风险控制: 订单池管理、风险分散等\n优势: 融资便捷、成本较低、提升供应链协同效率\n案例: 某电商企业利用订单融资，获得了2000万元的流动资金，用于备货发货，满足了订单快速增长的需求。\n\n产品名称: 保单融资\n产品类型: 信用融资\n产品简介: 以保单为质押品获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-1年\n融资成本: 3%-6%\n担保方式: 保单质押、信用担保、保证担保等\n风险控制: 保单真实性、保费支付记录等\n优势: 融资便捷、成本较低、盘活保单资产\n案例: 某制造企业利用保单融资，获得了500万元的流动资金，用于采购原材料，降低了融资成本。\n\n产品名称: 动产抵押融资\n产品类型: 动产融资\n产品简介: 以动产（如设备、车辆等）为质押品获取融资\n适用场景: 中小企业\n融资主体: 中小企业\n融资额度: 50万元以上\n融资期限: 1个月-3年\n融资成本: 5%-8%\n担保方式: 动产抵押、信用担保、保证担保等\n风险控制: 动产权属清晰性、评估价值等\n优势: 融资便捷、提高资产利用率\n案例: 某科技企业利用动产抵押融资，获得了100万元的流动资金，用于研发新产品，提升了企业竞争力。&#34;</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/start<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain &gt; 5:llm:ChatOpenAI<span style=color:#ff79c6>]</span> Entering LLM run with input:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;prompts&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;System: Use the following pieces of context to answer the user&#39;s question. \nIf you don&#39;t know the answer, just say that you don&#39;t know, don&#39;t try to make up an answer.\n----------------\n产品名称: 仓单融资\n产品类型: 动产融资\n产品简介: 以仓单为质押品获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-3年\n融资成本: 4%-7%\n担保方式: 仓单质押、信用担保、保证担保等\n风险控制: 货物真实性、权属清晰性、仓储安全等\n优势: 融资便捷、成本较低、盘活存货资产\n案例: 某贸易企业利用仓单融资，获得了500万元的流动资金，用于扩大进出口业务，提高了资金周转效率。\n\n产品名称: 订单融资\n产品类型: 信用融资\n产品简介: 以订单为基础获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-1年\n融资成本: 3%-6%\n担保方式: 订单真实性、买方信用状况等\n风险控制: 订单池管理、风险分散等\n优势: 融资便捷、成本较低、提升供应链协同效率\n案例: 某电商企业利用订单融资，获得了2000万元的流动资金，用于备货发货，满足了订单快速增长的需求。\n\n产品名称: 保单融资\n产品类型: 信用融资\n产品简介: 以保单为质押品获取融资\n适用场景: 核心企业、中小企业\n融资主体: 核心企业、中小企业\n融资额度: 100万元以上\n融资期限: 1个月-1年\n融资成本: 3%-6%\n担保方式: 保单质押、信用担保、保证担保等\n风险控制: 保单真实性、保费支付记录等\n优势: 融资便捷、成本较低、盘活保单资产\n案例: 某制造企业利用保单融资，获得了500万元的流动资金，用于采购原材料，降低了融资成本。\n\n产品名称: 动产抵押融资\n产品类型: 动产融资\n产品简介: 以动产（如设备、车辆等）为质押品获取融资\n适用场景: 中小企业\n融资主体: 中小企业\n融资额度: 50万元以上\n融资期限: 1个月-3年\n融资成本: 5%-8%\n担保方式: 动产抵押、信用担保、保证担保等\n风险控制: 动产权属清晰性、评估价值等\n优势: 融资便捷、提高资产利用率\n案例: 某科技企业利用动产抵押融资，获得了100万元的流动资金，用于研发新产品，提升了企业竞争力。\nHuman: 仓单融资的产品的优点是什么?&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>llm/end<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain &gt; 5:llm:ChatOpenAI<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1.78s<span style=color:#ff79c6>]</span> Exiting LLM run with output:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;generations&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;text&#34;</span>: <span style=color:#f1fa8c>&#34;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;generation_info&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;finish_reason&#34;</span>: <span style=color:#f1fa8c>&#34;stop&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;logprobs&#34;</span>: null
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;ChatGeneration&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;message&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;lc&#34;</span>: 1,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;constructor&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;id&#34;</span>: <span style=color:#ff79c6>[</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;langchain&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;schema&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;messages&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;AIMessage&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f1fa8c>&#34;kwargs&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;additional_kwargs&#34;</span>: <span style=color:#ff79c6>{}</span>
</span></span><span style=display:flex><span>          <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>      <span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>]</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;llm_output&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;token_usage&#34;</span>: <span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;completion_tokens&#34;</span>: 40,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;prompt_tokens&#34;</span>: 1065,
</span></span><span style=display:flex><span>      <span style=color:#f1fa8c>&#34;total_tokens&#34;</span>: <span style=color:#bd93f9>1105</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;model_name&#34;</span>: <span style=color:#f1fa8c>&#34;gpt-3.5-turbo&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;system_fingerprint&#34;</span>: <span style=color:#f1fa8c>&#34;fp_4f0b692a78&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#ff79c6>}</span>,
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;run&#34;</span>: null
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>chain/end<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1.78s<span style=color:#ff79c6>]</span> Exiting Chain run with output:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;text&#34;</span>: <span style=color:#f1fa8c>&#34;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#34;</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>chain/end<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1.80s<span style=color:#ff79c6>]</span> Exiting Chain run with output:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;output_text&#34;</span>: <span style=color:#f1fa8c>&#34;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#34;</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span>chain/end<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>1:chain:RetrievalQA<span style=color:#ff79c6>]</span> <span style=color:#ff79c6>[</span>2.90s<span style=color:#ff79c6>]</span> Exiting Chain run with output:
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span>
</span></span><span style=display:flex><span>  <span style=color:#f1fa8c>&#34;result&#34;</span>: <span style=color:#f1fa8c>&#34;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#34;</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>}</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>{</span><span style=color:#f1fa8c>&#39;query&#39;</span>: <span style=color:#f1fa8c>&#39;仓单融资的产品的优点是什么?&#39;</span>, <span style=color:#f1fa8c>&#39;result&#39;</span>: <span style=color:#f1fa8c>&#39;仓单融资的产品优点包括融资便捷、成本较低、以及盘活存货资产。&#39;</span><span style=color:#ff79c6>}</span>
</span></span></code></pre></div><p>设置全局debug后，可以看到整个上下文检索的过程，还可以看到token的消耗情况；最终得到的答案比手动测试用例多了<code>盘活存货资产</code>的描述</p><h3 id=使用llm进行评估>使用LLM进行评估</h3><p>用openai语言模型生成问答对，并回答这些问题；用ollama后端的qwen:7b语言模型进行答案判断</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># flake8: noqa</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.globals <span style=color:#ff79c6>import</span> set_debug
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai.chat_models <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.llms.ollama <span style=color:#ff79c6>import</span> Ollama
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.document_loaders.csv_loader <span style=color:#ff79c6>import</span> CSVLoader
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.vectorstores.docarray <span style=color:#ff79c6>import</span> DocArrayInMemorySearch
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> OpenAIEmbeddings
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.indexes <span style=color:#ff79c6>import</span> VectorstoreIndexCreator
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> RetrievalQA
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.evaluation.qa <span style=color:#ff79c6>import</span> QAGenerateChain, QAEvalChain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.base_language <span style=color:#ff79c6>import</span> BaseLanguageModel
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.prompts <span style=color:#ff79c6>import</span> PromptTemplate
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> typing <span style=color:#ff79c6>import</span> Any
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>ollama_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;http://127.0.0.1:11434&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>template <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;You are a teacher coming up with questions to ask on a quiz.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Given the following document, please generate a question and answer based on that document.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>Example Format:
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;Begin Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>...
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;End Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>QUESTION: question here
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>ANSWER: answer here
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>These questions should be detailed and be based explicitly on information in the document. Begin!
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;Begin Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>{doc}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&lt;End Document&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>请使用中文输出
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>PROMPT <span style=color:#ff79c6>=</span> PromptTemplate(
</span></span><span style=display:flex><span>    input_variables<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;doc&#34;</span>],
</span></span><span style=display:flex><span>    template<span style=color:#ff79c6>=</span>template,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 继承QAGenerateChain，重写from_llm方法</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>ZhCNQAGenerateChain</span>(QAGenerateChain):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;LLM Chain for generating examples for question answering.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    @classmethod
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>from_llm</span>(cls, llm: BaseLanguageModel, <span style=color:#ff79c6>**</span>kwargs: Any) <span style=color:#ff79c6>-&gt;</span> QAGenerateChain:
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Load QA Generate Chain from LLM.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> cls(llm<span style=color:#ff79c6>=</span>llm, prompt<span style=color:#ff79c6>=</span>PROMPT, <span style=color:#ff79c6>**</span>kwargs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 关闭LangChain全局debug</span>
</span></span><span style=display:flex><span>    set_debug(<span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 1.使用LangChain文档加载器csv类型对数据进行导入</span>
</span></span><span style=display:flex><span>    file <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;/Users/iceyao/Desktop/test_101.csv&#34;</span>
</span></span><span style=display:flex><span>    csv_loader <span style=color:#ff79c6>=</span> CSVLoader(file_path<span style=color:#ff79c6>=</span>file)
</span></span><span style=display:flex><span>    docs <span style=color:#ff79c6>=</span> csv_loader<span style=color:#ff79c6>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 2.基于文档加载器创建LangChain向量存储索引，这里使用向量内存存储</span>
</span></span><span style=display:flex><span>    index <span style=color:#ff79c6>=</span> VectorstoreIndexCreator(
</span></span><span style=display:flex><span>        vectorstore_cls<span style=color:#ff79c6>=</span>DocArrayInMemorySearch,
</span></span><span style=display:flex><span>        embedding<span style=color:#ff79c6>=</span>OpenAIEmbeddings(
</span></span><span style=display:flex><span>            api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>            base_url<span style=color:#ff79c6>=</span>openai_url))<span style=color:#ff79c6>.</span>from_loaders([csv_loader])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 3.声明OpenAI语言模型，用于自动生成LLM问答用例</span>
</span></span><span style=display:flex><span>    openai_llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 4.声明ollama模型(实际是llama2:13b)，用于评估问答答案</span>
</span></span><span style=display:flex><span>    ollama_llm <span style=color:#ff79c6>=</span> Ollama(base_url<span style=color:#ff79c6>=</span>ollama_url,
</span></span><span style=display:flex><span>                        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>,
</span></span><span style=display:flex><span>                        model<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;qwen:7b&#34;</span>
</span></span><span style=display:flex><span>                        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 5.声明检索QA链</span>
</span></span><span style=display:flex><span>    retrieval_qa_chain <span style=color:#ff79c6>=</span> RetrievalQA<span style=color:#ff79c6>.</span>from_chain_type(
</span></span><span style=display:flex><span>        llm<span style=color:#ff79c6>=</span>openai_llm,
</span></span><span style=display:flex><span>        retriever<span style=color:#ff79c6>=</span>index<span style=color:#ff79c6>.</span>vectorstore<span style=color:#ff79c6>.</span>as_retriever(),
</span></span><span style=display:flex><span>        chain_type<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;stuff&#34;</span>,
</span></span><span style=display:flex><span>        verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 6.声明ZhCNQAGenerateChain链，基于QA生成链</span>
</span></span><span style=display:flex><span>    qa_generate_chain <span style=color:#ff79c6>=</span> ZhCNQAGenerateChain<span style=color:#ff79c6>.</span>from_llm(openai_llm)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 7.ZhCNQAGenerateChain链调用apply方法自动创建问答对</span>
</span></span><span style=display:flex><span>    llm_examples <span style=color:#ff79c6>=</span> qa_generate_chain<span style=color:#ff79c6>.</span>apply([{<span style=color:#f1fa8c>&#34;doc&#34;</span>: t} <span style=color:#ff79c6>for</span> t <span style=color:#ff79c6>in</span> docs])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    examples <span style=color:#ff79c6>=</span> [v <span style=color:#ff79c6>for</span> item <span style=color:#ff79c6>in</span> llm_examples <span style=color:#ff79c6>for</span> _, v <span style=color:#ff79c6>in</span> item<span style=color:#ff79c6>.</span>items()]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 8.检索QA链为测试用例生成预测</span>
</span></span><span style=display:flex><span>    predictions <span style=color:#ff79c6>=</span> retrieval_qa_chain<span style=color:#ff79c6>.</span>batch(examples) <span style=color:#6272a4># type: ignore[misc]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 9.声明QA评估链</span>
</span></span><span style=display:flex><span>    qa_eval_chain <span style=color:#ff79c6>=</span> QAEvalChain<span style=color:#ff79c6>.</span>from_llm(ollama_llm)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 10.QA评估链对其进行评估</span>
</span></span><span style=display:flex><span>    evaluate_results <span style=color:#ff79c6>=</span> qa_eval_chain<span style=color:#ff79c6>.</span>evaluate(examples, predictions) <span style=color:#6272a4># type: ignore[misc]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i, _ <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>(examples):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Example </span><span style=color:#f1fa8c>{</span>i<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>:&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;Question: &#34;</span> <span style=color:#ff79c6>+</span> predictions[i][<span style=color:#f1fa8c>&#39;query&#39;</span>])
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;Real Answer: &#34;</span> <span style=color:#ff79c6>+</span> predictions[i][<span style=color:#f1fa8c>&#39;answer&#39;</span>])
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;Predicted Answer: &#34;</span> <span style=color:#ff79c6>+</span> predictions[i][<span style=color:#f1fa8c>&#39;result&#39;</span>])
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;Predicted Grade: &#34;</span> <span style=color:#ff79c6>+</span> evaluate_results[i][<span style=color:#f1fa8c>&#39;results&#39;</span>])
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; Entering new RetrievalQA chain...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Entering new RetrievalQA chain...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Entering new RetrievalQA chain...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Entering new RetrievalQA chain...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Entering new RetrievalQA chain...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Entering new RetrievalQA chain...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>Example 0:
</span></span><span style=display:flex><span>Question: 什么是“应收账款质押融资”产品的主要特点和优势？请列举至少三点。
</span></span><span style=display:flex><span>Real Answer: 该产品的主要特点和优势包括：以应收账款为质押品获取融资、适用于核心企业和中小企业、融资额度在100万元以上、融资期限为1个月至3年、融资成本在5%-8%之间、担保方式包括应收账款质押、信用担保、保证担保等、风险控制主要关注应收账款真实性、债权清晰性、履约能力等、优势在于融资便捷、成本较低、提高资金利用率。
</span></span><span style=display:flex><span>Predicted Answer: “应收账款质押融资”产品的主要特点和优势包括：
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>1. **融资便捷**：通过将应收账款作为质押品，企业可以相对容易地获取融资，无需进行繁琐的审批流程，提高了融资的速度和效率。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>2. **成本较低**：相比其他融资方式，应收账款质押融资的成本通常在5%-8%之间，相对较低，有助于降低企业的融资成本，提升盈利能力。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>3. **提高资金利用率**：通过将应收账款作为质押品获得融资，企业可以有效地利用未来的收款权益，提前获取资金用于业务发展，提高了资金的利用效率和灵活性。
</span></span><span style=display:flex><span>Predicted Grade: CORRECT
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Example 1:
</span></span><span style=display:flex><span>Question: 仓单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？
</span></span><span style=display:flex><span>Real Answer: 仓单融资产品适用于核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。
</span></span><span style=display:flex><span>Predicted Answer: 仓单融资产品的适用场景是核心企业和中小企业。融资主体也是核心企业和中小企业。融资额度是100万元以上，融资期限为1个月到3年。融资成本为4%-7%。担保方式包括仓单质押、信用担保、保证担保等。
</span></span><span style=display:flex><span>Predicted Grade: CORRECT
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Example 2:
</span></span><span style=display:flex><span>Question: 什么是订单融资的产品简介和适用场景？融资额度和期限是多少？融资成本是多少？担保方式和风险控制措施是什么？
</span></span><span style=display:flex><span>Real Answer: 订单融资是以订单为基础获取融资的信用融资产品，适用于核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年，融资成本为3%-6%。担保方式包括订单真实性和买方信用状况等，风险控制措施包括订单池管理和风险分散。
</span></span><span style=display:flex><span>Predicted Answer: 订单融资的产品简介是以订单为基础获取融资，适用场景是核心企业和中小企业。融资额度是100万元以上，融资期限为1个月到1年，融资成本为3%-6%。担保方式包括订单真实性和买方信用状况等，风险控制措施包括订单池管理和风险分散等。
</span></span><span style=display:flex><span>Predicted Grade: CORRECT
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Example 3:
</span></span><span style=display:flex><span>Question: 什么是动产抵押融资的产品简介？ 
</span></span><span style=display:flex><span>Real Answer: 以动产（如设备、车辆等）为质押品获取融资
</span></span><span style=display:flex><span>Predicted Answer: 动产抵押融资的产品简介是以动产（如设备、车辆等）作为质押品来获取融资。
</span></span><span style=display:flex><span>Predicted Grade: CORRECT
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Example 4:
</span></span><span style=display:flex><span>Question: 保单融资产品的适用场景是什么？融资主体是谁？融资额度和期限分别是多少？融资成本是多少？担保方式有哪些？
</span></span><span style=display:flex><span>Real Answer: 保单融资产品适用于核心企业和中小企业，融资主体也是核心企业和中小企业。融资额度为100万元以上，融资期限为1个月至1年，融资成本为3%-6%。担保方式包括保单质押、信用担保、保证担保等。
</span></span><span style=display:flex><span>Predicted Answer: 保单融资产品的适用场景是核心企业和中小企业。融资主体是核心企业和中小企业。融资额度是100万元以上，融资期限是1个月到1年。融资成本是3%-6%。担保方式包括保单质押、信用担保、保证担保等。
</span></span><span style=display:flex><span>Predicted Grade: CORRECT
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Example 5:
</span></span><span style=display:flex><span>Question: 请问流水贷款的产品类型是什么？
</span></span><span style=display:flex><span>Real Answer: 信用融资
</span></span><span style=display:flex><span>Predicted Answer: 流水贷款的产品类型是信用融资。
</span></span><span style=display:flex><span>Predicted Grade: CORRECT
</span></span></code></pre></div><p>从输出结果来看的话，每一个Example中包含了Question、Real Answer、Predicted Answer、Predicted Grade，Real Answer是
QA生成链基于openai语言模型生成的，Real Answer是QA检索链基于openai语言模型回答的，Predicted Grade是QA评估链基于qwen:7b语言模型
生成的。全自动的评估方式极大地简化了问答系统的评估和优化过程，开发者无需手动准备测试用例，也无需逐一判断正确性。</p><h2 id=代理agent>代理Agent</h2><p>代理作为语言模型的外部模块，可提供计算、逻辑、检索等功能的支持，使语言模型获得异常强大的推理和获取信息的超能力。LangChain的agent跟AI agent不是同一个概念。</p><p>AI agent、大模型、LangChain之间的关系？
AI agent是一种能够感知环境、进行决策和执行动作的智能实体。大模型相当于是AI agent的大脑，LangChain是快速构建AI agent的框架平台。AI agent～=大模型+插件+执行流程，对应人体的控制端、感知端、执行端</p><p>Agent类型区别：<a href=https://python.langchain.com/docs/modules/agents/agent_types/>https://python.langchain.com/docs/modules/agents/agent_types/</a></p><h3 id=使用llm-mathwikipedia工具>使用llm-math/wikipedia工具</h3><p>使用代理，需要满足三个条件：</p><ul><li>一个基础的LLM</li><li>进行交互的工具Tools</li><li>控制交互的代理Agents</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.agents <span style=color:#ff79c6>import</span> AgentExecutor, create_openai_tools_agent, load_tools
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain <span style=color:#ff79c6>import</span> hub
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化一个基础的LLM</span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        streaming<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>        verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化工具，这里用到两个内置工具</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># llm-math: 工具结合语言模型和计算器用以进行数学计算</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># wikipedia: 工具通过API连接到wikipedia进行搜索查询</span>
</span></span><span style=display:flex><span>    tools <span style=color:#ff79c6>=</span> load_tools(tool_names<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;llm-math&#34;</span>, <span style=color:#f1fa8c>&#34;wikipedia&#34;</span>], llm<span style=color:#ff79c6>=</span>llm)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 从hub上拉取prompt模版</span>
</span></span><span style=display:flex><span>    prompt <span style=color:#ff79c6>=</span> hub<span style=color:#ff79c6>.</span>pull(<span style=color:#f1fa8c>&#34;hwchase17/openai-tools-agent&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化agent</span>
</span></span><span style=display:flex><span>    agent <span style=color:#ff79c6>=</span> create_openai_tools_agent(llm, tools, prompt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 运行agent</span>
</span></span><span style=display:flex><span>    agent_executor <span style=color:#ff79c6>=</span> AgentExecutor(
</span></span><span style=display:flex><span>        agent<span style=color:#ff79c6>=</span>agent,
</span></span><span style=display:flex><span>        tools<span style=color:#ff79c6>=</span>tools,
</span></span><span style=display:flex><span>        verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    agent_executor<span style=color:#ff79c6>.</span>invoke({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;计算300的25%&#34;</span>})
</span></span><span style=display:flex><span>    agent_executor<span style=color:#ff79c6>.</span>invoke({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;曹德旺做了哪些善事&#34;</span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; Entering new AgentExecutor chain...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Invoking: `Calculator` with `300*0.25`
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Answer: 75.025% of 300 is 75.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Entering new AgentExecutor chain...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Invoking: `wikipedia` with `曹德旺`
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Page: Cao Dewang
</span></span><span style=display:flex><span>Summary: Cao Dewang (Chinese: 曹德旺; pinyin: Cáo Déwàng; born May 1946), also known as Cho Tak Wong or Tak Wong Cho, is a Chinese entrepreneur. He is the chairman of Fuyao Group, one of the largest glass manufacturers in the world. He is also a member of the Chinese People&#39;s Consultative Conference from Fujian, and chairman of both the China Automobile Glass Association and the Fujian Golf Players&#39; Association.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Page: Jack Wong
</span></span><span style=display:flex><span>Summary: Jack Wong, or Huang Zhang (Chinese: 黄章; pinyin: Huáng Zhāng), is a Chinese billionaire entrepreneur. He is the founder and chairman of Meizu, a Chinese consumer electronics company.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Page: Crocodile Island (film)
</span></span><span style=display:flex><span>Summary: Crocodile Island is a 2020 Chinese action monster film directed by Xu Shixing and Simon Zhao, and starring Gallen Lo as a single father who lands on a crocodile island with his daughter (Liao Yinyue) due to a plane malfunction and must battle with beast-sized creatures inhabiting the island. This web film was released for online streaming on 4 February 2020 on iQiyi. Crocodile Island became a commercial success, grossing ¥16.70 million against a budget of ¥8 million and is currently the highest-grossing web film of 2020 in China.根据维基百科，曹德旺是中国企业家，福耀集团董事长，也是中国汽车玻璃协会和福建高尔夫球员协会的主席。关于他做了哪些善事的具体信息可能需要更深入的研究。您是否希望我帮助您进一步了解曹德旺的善举？
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span></code></pre></div><h3 id=使用pythonrepltool工具>使用PythonREPLTool工具</h3><p>使用PythonREPLTool工具将名字转化为拼音</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_experimental.agents.agent_toolkits.python.base <span style=color:#ff79c6>import</span> create_python_agent
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_experimental.tools <span style=color:#ff79c6>import</span> PythonREPLTool
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        streaming<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>        verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    tool <span style=color:#ff79c6>=</span> PythonREPLTool()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    agent <span style=color:#ff79c6>=</span> create_python_agent(
</span></span><span style=display:flex><span>        llm, tool, verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    customer_list <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#34;张三&#34;</span>, <span style=color:#f1fa8c>&#34;李四&#34;</span>, <span style=color:#f1fa8c>&#34;王五&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    agent<span style=color:#ff79c6>.</span>invoke(
</span></span><span style=display:flex><span>        {<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;使用pinyin拼音库这些客户名字转换为拼音，并打印输出列表: </span><span style=color:#f1fa8c>{</span>customer_list<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>。&#34;</span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; Entering new AgentExecutor chain...
</span></span><span style=display:flex><span>I need to use the pinyin library to convert the names to pinyin.
</span></span><span style=display:flex><span>Action: Python_REPL
</span></span><span style=display:flex><span>Action Input: 
</span></span><span style=display:flex><span>```python
</span></span><span style=display:flex><span>from pypinyin import pinyin
</span></span><span style=display:flex><span>names = [&#39;张三&#39;, &#39;李四&#39;, &#39;王五&#39;]
</span></span><span style=display:flex><span>pinyin_names = [&#34;&#34;.join([y[0] for y in x]) for x in [pinyin(name, style=0) for name in names]]
</span></span><span style=display:flex><span>print(pinyin_names)
</span></span><span style=display:flex><span>```Python REPL can execute arbitrary code. Use with caution.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Observation: [&#39;zhangsan&#39;, &#39;lisi&#39;, &#39;wangwu&#39;]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Thought:The names have been successfully converted to pinyin.
</span></span><span style=display:flex><span>Final Answer: [&#39;zhangsan&#39;, &#39;lisi&#39;, &#39;wangwu&#39;]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span></code></pre></div><p>从输出结果来看，可以看出agent自主决策的一个过程</p><h3 id=自定义工具>自定义工具</h3><p>LangChain tool函数装饰器可以应用于任何函数，将函数转化为LangChain工具，成为agent可以调用的工具.
这里以创建自定义时间的工具为例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.agents <span style=color:#ff79c6>import</span> tool
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.agents <span style=color:#ff79c6>import</span> AgentExecutor, create_openai_tools_agent
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> datetime <span style=color:#ff79c6>import</span> date
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain <span style=color:#ff79c6>import</span> hub
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@tool
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>time</span>(text: <span style=color:#8be9fd;font-style:italic>str</span>) <span style=color:#ff79c6>-&gt;</span> <span style=color:#8be9fd;font-style:italic>str</span>:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> <span style=color:#8be9fd;font-style:italic>str</span>(date<span style=color:#ff79c6>.</span>today())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        streaming<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>        verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    tools <span style=color:#ff79c6>=</span> [time]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 从hub上拉取prompt模版</span>
</span></span><span style=display:flex><span>    prompt <span style=color:#ff79c6>=</span> hub<span style=color:#ff79c6>.</span>pull(<span style=color:#f1fa8c>&#34;hwchase17/openai-tools-agent&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 初始化agent</span>
</span></span><span style=display:flex><span>    agent <span style=color:#ff79c6>=</span> create_openai_tools_agent(llm, tools, prompt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 运行agent</span>
</span></span><span style=display:flex><span>    agent_executor <span style=color:#ff79c6>=</span> AgentExecutor(
</span></span><span style=display:flex><span>        agent<span style=color:#ff79c6>=</span>agent,
</span></span><span style=display:flex><span>        tools<span style=color:#ff79c6>=</span>tools,
</span></span><span style=display:flex><span>        verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    agent_executor<span style=color:#ff79c6>.</span>invoke({<span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;今天的日期是多少&#34;</span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&gt; Entering new AgentExecutor chain...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Invoking: `time` with `{&#39;text&#39;: &#39;today&#39;}`
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>2024-03-27今天是2024年3月27日。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span></code></pre></div><h2 id=lcellangchain-expression-language>LCEL(LangChain Expression Language)</h2><p>LangChain表达式语言（LCEL）是一种轻松地将链组合在一起的声明性方式。 LCEL 从第一天起就被设计为支持将原型投入生产，无需更改代码，从最简单的“提示+LLM”链到最复杂的链</p><h2 id=langsmith>LangSmith</h2><p>LangSmith是一个为构建生产级别的大型语言模型（LLM）应用程序而设计的平台。由 LangChain团队开发。不能私有化部署，提供类似SaaS服务，它提供了密切监控和评估应用程序的功能，帮助开发者快速且自信地发布应用。此外LangSmith可以独立运作，不依赖于LangChain。</p><p>安装langsmith包</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install -U langsmith
</span></span></code></pre></div><p>在LangSmith上<code>https://smith.langchain.com/settings</code>创建API Key，并声明</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>export</span> <span style=color:#8be9fd;font-style:italic>LANGCHAIN_TRACING_V2</span><span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>true</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>export</span> <span style=color:#8be9fd;font-style:italic>LANGCHAIN_API_KEY</span><span style=color:#ff79c6>=</span>&lt;your-api-key&gt;
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>export</span> <span style=color:#8be9fd;font-style:italic>LANGCHAIN_PROJECT</span><span style=color:#ff79c6>=</span>default
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> openai
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langsmith.wrappers <span style=color:#ff79c6>import</span> wrap_openai
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langsmith <span style=color:#ff79c6>import</span> traceable
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Auto-trace LLM calls in-context</span>
</span></span><span style=display:flex><span>client <span style=color:#ff79c6>=</span> wrap_openai(openai<span style=color:#ff79c6>.</span>Client(
</span></span><span style=display:flex><span>    api_key<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>,
</span></span><span style=display:flex><span>    base_url<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;https://api.chatanywhere.tech/v1/&#34;</span> <span style=color:#6272a4># 国内代理</span>
</span></span><span style=display:flex><span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@traceable <span style=color:#6272a4># Auto-trace this function</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>pipeline</span>(user_input: <span style=color:#8be9fd;font-style:italic>str</span>):
</span></span><span style=display:flex><span>    result <span style=color:#ff79c6>=</span> client<span style=color:#ff79c6>.</span>chat<span style=color:#ff79c6>.</span>completions<span style=color:#ff79c6>.</span>create(
</span></span><span style=display:flex><span>        messages<span style=color:#ff79c6>=</span>[{<span style=color:#f1fa8c>&#34;role&#34;</span>: <span style=color:#f1fa8c>&#34;user&#34;</span>, <span style=color:#f1fa8c>&#34;content&#34;</span>: user_input}],
</span></span><span style=display:flex><span>        model<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;gpt-3.5-turbo&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> result<span style=color:#ff79c6>.</span>choices[<span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>message<span style=color:#ff79c6>.</span>content
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pipeline(<span style=color:#f1fa8c>&#34;Hello, world!&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#6272a4># Out:  Hello there! How can I assist you today?</span>
</span></span></code></pre></div><p>运行完后，<code>https://smith.langchain.com/</code>上会有对应链路跟踪信息</p><h2 id=langfuse>LangFuse</h2><p>Langfuse 是一个专为基于大型语言模型 (LLMs) 的应用程序设计的开源观测和分析平台。支持私有化部署。</p><p>docker-compose部署</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># Clone repository</span>
</span></span><span style=display:flex><span>git clone https://github.com/langfuse/langfuse.git
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>cd</span> langfuse
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># Run server and database</span>
</span></span><span style=display:flex><span>docker-compose up -d
</span></span></code></pre></div><p>默认访问服务器的3000端口<code>http://&lt;server-ip>:3000/</code></p><p>安装langfuse python sdk</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install langfuse openai
</span></span></code></pre></div><p>使用langfuse装饰器快速集成，运行完在<code>http://&lt;server-ip>:3000/</code>上可以看到调用信息</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langfuse.decorators <span style=color:#ff79c6>import</span> observe
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langfuse.openai <span style=color:#ff79c6>import</span> openai  <span style=color:#6272a4># OpenAI integration</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>os<span style=color:#ff79c6>.</span>environ[<span style=color:#f1fa8c>&#34;LANGFUSE_SECRET_KEY&#34;</span>] <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;sk-lf-a7ac8caf-24f2-433f-8b82-0b66d0afd238&#34;</span>
</span></span><span style=display:flex><span>os<span style=color:#ff79c6>.</span>environ[<span style=color:#f1fa8c>&#34;LANGFUSE_PUBLIC_KEY&#34;</span>] <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;pk-lf-066d7968-4c93-497b-b0aa-9d825571fa4d&#34;</span>
</span></span><span style=display:flex><span>os<span style=color:#ff79c6>.</span>environ[<span style=color:#f1fa8c>&#34;LANGFUSE_HOST&#34;</span>] <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;http://&lt;server-ip&gt;:3000&#34;</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@observe()
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>story</span>():
</span></span><span style=display:flex><span>    openai<span style=color:#ff79c6>.</span>api_key <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>
</span></span><span style=display:flex><span>    openai<span style=color:#ff79c6>.</span>base_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.tech/v1/&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> openai<span style=color:#ff79c6>.</span>chat<span style=color:#ff79c6>.</span>completions<span style=color:#ff79c6>.</span>create(
</span></span><span style=display:flex><span>        model<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;gpt-3.5-turbo&#34;</span>,
</span></span><span style=display:flex><span>        max_tokens<span style=color:#ff79c6>=</span><span style=color:#bd93f9>100</span>,
</span></span><span style=display:flex><span>        messages<span style=color:#ff79c6>=</span>[
</span></span><span style=display:flex><span>          {<span style=color:#f1fa8c>&#34;role&#34;</span>: <span style=color:#f1fa8c>&#34;system&#34;</span>, <span style=color:#f1fa8c>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;你是一个写作专家&#34;</span>},
</span></span><span style=display:flex><span>          {<span style=color:#f1fa8c>&#34;role&#34;</span>: <span style=color:#f1fa8c>&#34;user&#34;</span>, <span style=color:#f1fa8c>&#34;content&#34;</span>: <span style=color:#f1fa8c>&#34;请写一篇2000字的旅游文章&#34;</span>}
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>    )<span style=color:#ff79c6>.</span>choices[<span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>message<span style=color:#ff79c6>.</span>content
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@observe()
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> story()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>main()
</span></span></code></pre></div><h2 id=use-cases>Use Cases</h2><p>来自于Langchain官网的quickstart例子：<a href=https://python.langchain.com/docs/use_cases>https://python.langchain.com/docs/use_cases</a></p><h3 id=rag问答>RAG问答</h3><h4 id=rag架构>RAG架构</h4><p>RAG应用有两个核心组件：</p><ul><li>索引</li><li>检索和生成</li></ul><p>建立索引过程：</p><ol><li>Load：第一步加载数据，使用<code>DocumentLoaders</code></li><li>Split：使用文本分割器把大文档切分成小的chunk，用于建立索引数据和传入大模型，因为大chunk检索困难、模型有上下文窗口长度限制</li><li>Store：我们需要存储和索引这些分割的文本，这步通常使用向量存储和Embedding模型。</li></ol><p><img src=https://python.langchain.com/v0.1/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png alt=RAG架构></p><p>检索和生成过程：</p><ol><li>Retrieve：根据用户输入，使用<code>Retriever</code>检索器从存储中检索出相关的分割文本</li><li>Generate：根据问题、检索到的数据生成Prompt发送至聊天模型/LLM，聊天模型/LLM生成相应的回答</li></ol><p><img src=https://python.langchain.com/v0.1/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png alt=RAG架构></p><h4 id=代码实现>代码实现</h4><p>安装依赖</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai langchain-chroma bs4</span>
</span></span></code></pre></div><p>一个quickstart例子</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> bs4
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain <span style=color:#ff79c6>import</span> hub
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.document_loaders.web_base <span style=color:#ff79c6>import</span> WebBaseLoader
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_chroma <span style=color:#ff79c6>import</span> Chroma
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.output_parsers <span style=color:#ff79c6>import</span> StrOutputParser
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.runnables <span style=color:#ff79c6>import</span> RunnablePassthrough
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> OpenAIEmbeddings
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_text_splitters <span style=color:#ff79c6>import</span> RecursiveCharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic.v1 <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Load, chunk and index the contents of the blog.</span>
</span></span><span style=display:flex><span>    loader <span style=color:#ff79c6>=</span> WebBaseLoader(
</span></span><span style=display:flex><span>        web_paths<span style=color:#ff79c6>=</span>(<span style=color:#f1fa8c>&#34;https://lilianweng.github.io/posts/2023-06-23-agent/&#34;</span>,),
</span></span><span style=display:flex><span>        bs_kwargs<span style=color:#ff79c6>=</span><span style=color:#8be9fd;font-style:italic>dict</span>(
</span></span><span style=display:flex><span>            parse_only<span style=color:#ff79c6>=</span>bs4<span style=color:#ff79c6>.</span>SoupStrainer(
</span></span><span style=display:flex><span>                class_<span style=color:#ff79c6>=</span>(<span style=color:#f1fa8c>&#34;post-content&#34;</span>, <span style=color:#f1fa8c>&#34;post-title&#34;</span>, <span style=color:#f1fa8c>&#34;post-header&#34;</span>)
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    docs <span style=color:#ff79c6>=</span> loader<span style=color:#ff79c6>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    text_splitter <span style=color:#ff79c6>=</span> RecursiveCharacterTextSplitter(
</span></span><span style=display:flex><span>        chunk_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1000</span>, chunk_overlap<span style=color:#ff79c6>=</span><span style=color:#bd93f9>200</span>)
</span></span><span style=display:flex><span>    splits <span style=color:#ff79c6>=</span> text_splitter<span style=color:#ff79c6>.</span>split_documents(docs)
</span></span><span style=display:flex><span>    vectorstore <span style=color:#ff79c6>=</span> Chroma<span style=color:#ff79c6>.</span>from_documents(
</span></span><span style=display:flex><span>        documents<span style=color:#ff79c6>=</span>splits, embedding<span style=color:#ff79c6>=</span>OpenAIEmbeddings(
</span></span><span style=display:flex><span>            api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>            base_url<span style=color:#ff79c6>=</span>openai_url
</span></span><span style=display:flex><span>        ))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Retrieve and generate using the relevant snippets of the blog.</span>
</span></span><span style=display:flex><span>    retriever <span style=color:#ff79c6>=</span> vectorstore<span style=color:#ff79c6>.</span>as_retriever()
</span></span><span style=display:flex><span>    prompt <span style=color:#ff79c6>=</span> hub<span style=color:#ff79c6>.</span>pull(<span style=color:#f1fa8c>&#34;rlm/rag-prompt&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>format_docs</span>(docs):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n\n</span><span style=color:#f1fa8c>&#34;</span><span style=color:#ff79c6>.</span>join(doc<span style=color:#ff79c6>.</span>page_content <span style=color:#ff79c6>for</span> doc <span style=color:#ff79c6>in</span> docs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    rag_chain <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>        {<span style=color:#f1fa8c>&#34;context&#34;</span>: retriever <span style=color:#ff79c6>|</span> format_docs, <span style=color:#f1fa8c>&#34;question&#34;</span>: RunnablePassthrough()}
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>|</span> prompt
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>|</span> llm
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>|</span> StrOutputParser()
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(rag_chain<span style=color:#ff79c6>.</span>invoke(<span style=color:#f1fa8c>&#34;What is Task Decomposition?&#34;</span>))
</span></span><span style=display:flex><span>    vectorstore<span style=color:#ff79c6>.</span>delete_collection()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Task Decomposition is a technique used to <span style=color:#8be9fd;font-style:italic>break</span> down complex tasks into smaller and simpler steps. This process helps agents or models better understand and tackle the task at hand by dividing it into manageable parts. It can be implemented through prompting techniques like Chain of Thought or Tree of Thoughts, task-specific instructions, or human inputs.
</span></span></code></pre></div><h3 id=提取结构化输出>提取结构化输出</h3><p>一个quickstart的例子，要使用支持function/tool调用能力的聊天模型，还可以通过<code>langchain.output_parsers</code>的方式来处理结构化输出</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic.v1 <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> typing <span style=color:#ff79c6>import</span> List, Optional
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.pydantic_v1 <span style=color:#ff79c6>import</span> BaseModel, Field
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>Person</span>(BaseModel):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;Information about a person.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># ^ Doc-string for the entity Person.</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># This doc-string is sent to the LLM as the description of the schema Person,</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># and it can help to improve extraction results.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Note that:</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 1. Each field is an `optional` -- this allows the model to decline to extract it!</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 2. Each field has a `description` -- this description is used by the LLM.</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Having a good description can help improve extraction results.</span>
</span></span><span style=display:flex><span>    name: Optional[<span style=color:#8be9fd;font-style:italic>str</span>] <span style=color:#ff79c6>=</span> Field(default<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, description<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;The name of the person&#34;</span>)
</span></span><span style=display:flex><span>    hair_color: Optional[<span style=color:#8be9fd;font-style:italic>str</span>] <span style=color:#ff79c6>=</span> Field(
</span></span><span style=display:flex><span>        default<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, description<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;The color of the peron&#39;s hair if known&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    height_in_meters: Optional[<span style=color:#8be9fd;font-style:italic>str</span>] <span style=color:#ff79c6>=</span> Field(
</span></span><span style=display:flex><span>        default<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>, description<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;Height measured in meters&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>Data</span>(BaseModel):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;Extracted data about people.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Creates a model so that we can extract multiple entities.</span>
</span></span><span style=display:flex><span>    people: List[Person]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Define a custom prompt to provide instructions and any additional context.</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 1) You can add examples into the prompt template to improve extraction quality</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 2) Introduce additional parameters to take context into account (e.g., include metadata</span>
</span></span><span style=display:flex><span><span style=color:#6272a4>#    about the document from which the text was extracted.)</span>
</span></span><span style=display:flex><span>    prompt <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_messages(
</span></span><span style=display:flex><span>        [
</span></span><span style=display:flex><span>            (
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;system&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;You are an expert extraction algorithm. &#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;Only extract relevant information from the text. &#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;If you do not know the value of an attribute asked to extract, &#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;return null for the attribute&#39;s value.&#34;</span>,
</span></span><span style=display:flex><span>            ),
</span></span><span style=display:flex><span>            <span style=color:#6272a4># Please see the how-to about improving performance with</span>
</span></span><span style=display:flex><span>            <span style=color:#6272a4># reference examples.</span>
</span></span><span style=display:flex><span>            <span style=color:#6272a4># MessagesPlaceholder(&#39;examples&#39;),</span>
</span></span><span style=display:flex><span>            (<span style=color:#f1fa8c>&#34;human&#34;</span>, <span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{text}</span><span style=color:#f1fa8c>&#34;</span>),
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    runnable <span style=color:#ff79c6>=</span> prompt <span style=color:#ff79c6>|</span> llm<span style=color:#ff79c6>.</span>with_structured_output(schema<span style=color:#ff79c6>=</span>Data)
</span></span><span style=display:flex><span>    text <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(runnable<span style=color:#ff79c6>.</span>invoke({<span style=color:#f1fa8c>&#34;text&#34;</span>: text}))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>people</span><span style=color:#ff79c6>=[</span>Person<span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>name</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;Jeff&#39;</span>, <span style=color:#8be9fd;font-style:italic>hair_color</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;black&#39;</span>, <span style=color:#8be9fd;font-style:italic>height_in_meters</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;1.83&#39;</span><span style=color:#ff79c6>)</span>, Person<span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>name</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;Anna&#39;</span>, <span style=color:#8be9fd;font-style:italic>hair_color</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;black&#39;</span>, <span style=color:#8be9fd;font-style:italic>height_in_meters</span><span style=color:#ff79c6>=</span>None<span style=color:#ff79c6>)]</span>
</span></span></code></pre></div><h3 id=对话检索机器人>对话检索机器人</h3><p>聊天机器人是LLM最流行的应用场景之一，聊天机器人的核心特征是它们可以进行长时间运行的、有状态的对话，并可以使用相关信息回答用户问题。</p><h4 id=架构>架构</h4><p><img src=https://python.langchain.com/v0.1/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png alt=chat_use_case>
聊天机器人通常对私有数据使用检索增强生成（RAG），以更好地回答特定领域的问题。您还可以选择在多个数据源之间进行路由，以确保它仅使用最热门的上下文来回答最终问题，或者选择使用更专业类型的聊天历史记录或内存，而不仅仅是来回传递消息。</p><h4 id=代码实现-1>代码实现</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic.v1 <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.output_parsers <span style=color:#ff79c6>import</span> StrOutputParser
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.runnables <span style=color:#ff79c6>import</span> RunnableBranch
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate, MessagesPlaceholder
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.document_loaders.web_base <span style=color:#ff79c6>import</span> WebBaseLoader
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_text_splitters <span style=color:#ff79c6>import</span> RecursiveCharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_chroma <span style=color:#ff79c6>import</span> Chroma
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> OpenAIEmbeddings
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains.combine_documents <span style=color:#ff79c6>import</span> create_stuff_documents_chain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.runnables <span style=color:#ff79c6>import</span> RunnablePassthrough
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.memory <span style=color:#ff79c6>import</span> ChatMessageHistory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    chat <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loader <span style=color:#ff79c6>=</span> WebBaseLoader(<span style=color:#f1fa8c>&#34;https://docs.smith.langchain.com/overview&#34;</span>)
</span></span><span style=display:flex><span>    data <span style=color:#ff79c6>=</span> loader<span style=color:#ff79c6>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    text_splitter <span style=color:#ff79c6>=</span> RecursiveCharacterTextSplitter(
</span></span><span style=display:flex><span>        chunk_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>500</span>, chunk_overlap<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>    all_splits <span style=color:#ff79c6>=</span> text_splitter<span style=color:#ff79c6>.</span>split_documents(data)
</span></span><span style=display:flex><span>    vectorstore <span style=color:#ff79c6>=</span> Chroma<span style=color:#ff79c6>.</span>from_documents(documents<span style=color:#ff79c6>=</span>all_splits, embedding<span style=color:#ff79c6>=</span>OpenAIEmbeddings(
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key, base_url<span style=color:#ff79c6>=</span>openai_url))
</span></span><span style=display:flex><span>    retriever <span style=color:#ff79c6>=</span> vectorstore<span style=color:#ff79c6>.</span>as_retriever(k<span style=color:#ff79c6>=</span><span style=color:#bd93f9>4</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    query_transform_prompt <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_messages(
</span></span><span style=display:flex><span>        [
</span></span><span style=display:flex><span>            MessagesPlaceholder(variable_name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;messages&#34;</span>),
</span></span><span style=display:flex><span>            (
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;user&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.&#34;</span>,
</span></span><span style=display:flex><span>            ),
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    question_answering_prompt <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_messages(
</span></span><span style=display:flex><span>        [
</span></span><span style=display:flex><span>            (
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;system&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;Answer the user&#39;s questions based on the below context:</span><span style=color:#f1fa8c>\n\n</span><span style=color:#f1fa8c>{context}</span><span style=color:#f1fa8c>&#34;</span>,
</span></span><span style=display:flex><span>            ),
</span></span><span style=display:flex><span>            MessagesPlaceholder(variable_name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;messages&#34;</span>),
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    query_transforming_retriever_chain <span style=color:#ff79c6>=</span> RunnableBranch(
</span></span><span style=display:flex><span>        (
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>lambda</span> x: <span style=color:#8be9fd;font-style:italic>len</span>(x<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;messages&#34;</span>, [])) <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>            <span style=color:#6272a4># If only one message, then we just pass that message&#39;s content to retriever</span>
</span></span><span style=display:flex><span>            (<span style=color:#ff79c6>lambda</span> x: x[<span style=color:#f1fa8c>&#34;messages&#34;</span>][<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>]<span style=color:#ff79c6>.</span>content) <span style=color:#ff79c6>|</span> retriever,
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>        <span style=color:#6272a4># If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever</span>
</span></span><span style=display:flex><span>        query_transform_prompt <span style=color:#ff79c6>|</span> chat <span style=color:#ff79c6>|</span> StrOutputParser() <span style=color:#ff79c6>|</span> retriever,
</span></span><span style=display:flex><span>    )<span style=color:#ff79c6>.</span>with_config(run_name<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;chat_retriever_chain&#34;</span>)
</span></span><span style=display:flex><span>    document_chain <span style=color:#ff79c6>=</span> create_stuff_documents_chain(
</span></span><span style=display:flex><span>        chat, question_answering_prompt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    conversational_retrieval_chain <span style=color:#ff79c6>=</span> RunnablePassthrough<span style=color:#ff79c6>.</span>assign(
</span></span><span style=display:flex><span>        context<span style=color:#ff79c6>=</span>query_transforming_retriever_chain,
</span></span><span style=display:flex><span>    )<span style=color:#ff79c6>.</span>assign(
</span></span><span style=display:flex><span>        answer<span style=color:#ff79c6>=</span>document_chain,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    demo_ephemeral_chat_history <span style=color:#ff79c6>=</span> ChatMessageHistory()
</span></span><span style=display:flex><span>    demo_ephemeral_chat_history<span style=color:#ff79c6>.</span>add_user_message(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;how can langsmith help with testing?&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    response <span style=color:#ff79c6>=</span> conversational_retrieval_chain<span style=color:#ff79c6>.</span>invoke(
</span></span><span style=display:flex><span>        {<span style=color:#f1fa8c>&#34;messages&#34;</span>: demo_ephemeral_chat_history<span style=color:#ff79c6>.</span>messages},
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    demo_ephemeral_chat_history<span style=color:#ff79c6>.</span>add_ai_message(response[<span style=color:#f1fa8c>&#34;answer&#34;</span>])
</span></span><span style=display:flex><span>    demo_ephemeral_chat_history<span style=color:#ff79c6>.</span>add_user_message(<span style=color:#f1fa8c>&#34;tell me more about that!&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(conversational_retrieval_chain<span style=color:#ff79c6>.</span>invoke(
</span></span><span style=display:flex><span>        {<span style=color:#f1fa8c>&#34;messages&#34;</span>: demo_ephemeral_chat_history<span style=color:#ff79c6>.</span>messages}
</span></span><span style=display:flex><span>    ))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>{</span><span style=color:#f1fa8c>&#39;messages&#39;</span>: <span style=color:#ff79c6>[</span>HumanMessage<span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>content</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;how can langsmith help with testing?&#39;</span><span style=color:#ff79c6>)</span>, AIMessage<span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>content</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;LangSmith is designed to aid in the development and testing of production-grade large language model (LLM) applications. Here&#39;s how it can help with testing:\n\n1. **Monitoring and Evaluation**: LangSmith allows you to closely monitor and evaluate your LLM application during testing. This means you can track its performance, identify any issues or bottlenecks, and make necessary improvements before deployment.\n\n2. **Tracing Capabilities**: With LangSmith, you can utilize its tracing capabilities to trace the execution of your LLM application. This helps in understanding how the application behaves under different inputs and scenarios, which is crucial for testing and debugging.\n\n3. **Prompt Hub**: LangSmith includes a Prompt Hub, which is a prompt management tool. This can be useful during testing as it helps in managing and organizing prompts for your LLM application, making it easier to iterate and test different inputs.\n\n4. **Proxy**: LangSmith offers proxy capabilities, which can be utilized to control and manage access to your LLM application during testing. This ensures that only authorized users or systems can interact with the application, enhancing security and control during testing phases.\n\n5. **Cookbook and Additional Resources**: LangSmith provides a Cookbook, which is a collection of tutorials and walkthroughs. These resources can guide you through the testing process, providing best practices and tips for testing LLM applications effectively.\n\nOverall, LangSmith offers a comprehensive set of tools and capabilities to support testing of LLM applications, enabling developers to ship quickly and with confidence.&#34;</span><span style=color:#ff79c6>)</span>, HumanMessage<span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>content</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;tell me more about that!&#39;</span><span style=color:#ff79c6>)]</span>, <span style=color:#f1fa8c>&#39;context&#39;</span>: <span style=color:#ff79c6>[</span>Document<span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>page_content</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;Skip to main contentLangSmith API DocsSearchGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring &amp; AutomationsPrompt HubProxyPricingSelf-HostingCookbookQuick StartOn this pageGetting started with LangSmithIntroduction\u200bLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!Install LangSmith\u200bWe&#39;</span>, <span style=color:#8be9fd;font-style:italic>metadata</span><span style=color:#ff79c6>={</span><span style=color:#f1fa8c>&#39;description&#39;</span>: <span style=color:#f1fa8c>&#39;Introduction&#39;</span>, <span style=color:#f1fa8c>&#39;language&#39;</span>: <span style=color:#f1fa8c>&#39;en&#39;</span>, <span style=color:#f1fa8c>&#39;source&#39;</span>: <span style=color:#f1fa8c>&#39;https://docs.smith.langchain.com/overview&#39;</span>, <span style=color:#f1fa8c>&#39;title&#39;</span>: <span style=color:#f1fa8c>&#39;Getting started with LangSmith | 🦜️🛠️ LangSmith&#39;</span><span style=color:#ff79c6>})</span>, Document<span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>page_content</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources\u200bLangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using LangSmith.LangChain Python: Docs for the Python LangChain&#39;</span>, <span style=color:#8be9fd;font-style:italic>metadata</span><span style=color:#ff79c6>={</span><span style=color:#f1fa8c>&#39;description&#39;</span>: <span style=color:#f1fa8c>&#39;Introduction&#39;</span>, <span style=color:#f1fa8c>&#39;language&#39;</span>: <span style=color:#f1fa8c>&#39;en&#39;</span>, <span style=color:#f1fa8c>&#39;source&#39;</span>: <span style=color:#f1fa8c>&#39;https://docs.smith.langchain.com/overview&#39;</span>, <span style=color:#f1fa8c>&#39;title&#39;</span>: <span style=color:#f1fa8c>&#39;Getting started with LangSmith | 🦜️🛠️ LangSmith&#39;</span><span style=color:#ff79c6>})</span>, Document<span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>page_content</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;Getting started with LangSmith | 🦜️🛠️ LangSmith&#39;</span>, <span style=color:#8be9fd;font-style:italic>metadata</span><span style=color:#ff79c6>={</span><span style=color:#f1fa8c>&#39;description&#39;</span>: <span style=color:#f1fa8c>&#39;Introduction&#39;</span>, <span style=color:#f1fa8c>&#39;language&#39;</span>: <span style=color:#f1fa8c>&#39;en&#39;</span>, <span style=color:#f1fa8c>&#39;source&#39;</span>: <span style=color:#f1fa8c>&#39;https://docs.smith.langchain.com/overview&#39;</span>, <span style=color:#f1fa8c>&#39;title&#39;</span>: <span style=color:#f1fa8c>&#39;Getting started with LangSmith | 🦜️🛠️ LangSmith&#39;</span><span style=color:#ff79c6>})</span>, Document<span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>page_content</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;goes here    datasetName, // The data to predict and grade over    {        evaluationConfig: { customEvaluators: [exactMatch] },        projectMetadata: {            version: &#34;1.0.0&#34;,            revision_id: &#34;beta&#34;,        },    });See more on the evaluation quick start page.Next Steps\u200bCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Pricing: Learn about the pricing model for&#39;</span>, <span style=color:#8be9fd;font-style:italic>metadata</span><span style=color:#ff79c6>={</span><span style=color:#f1fa8c>&#39;description&#39;</span>: <span style=color:#f1fa8c>&#39;Introduction&#39;</span>, <span style=color:#f1fa8c>&#39;language&#39;</span>: <span style=color:#f1fa8c>&#39;en&#39;</span>, <span style=color:#f1fa8c>&#39;source&#39;</span>: <span style=color:#f1fa8c>&#39;https://docs.smith.langchain.com/overview&#39;</span>, <span style=color:#f1fa8c>&#39;title&#39;</span>: <span style=color:#f1fa8c>&#39;Getting started with LangSmith | 🦜️🛠️ LangSmith&#39;</span><span style=color:#ff79c6>})]</span>, <span style=color:#f1fa8c>&#39;answer&#39;</span>: <span style=color:#f1fa8c>&#34;Sure, let&#39;s dive deeper into each aspect of how LangSmith can help with testing:\n\n1. **Monitoring and Evaluation**: LangSmith allows you to monitor various metrics and evaluate the performance of your LLM application during testing. This includes tracking metrics such as accuracy, response time, resource utilization, and more. By closely monitoring these metrics, you can identify any issues or areas for improvement in your application&#39;s performance.\n\n2. **Tracing Capabilities**: Tracing capabilities in LangSmith enable you to trace the execution flow of your LLM application. This means you can track how the application processes input prompts, generates responses, and executes various tasks. Tracing helps in understanding the behavior of the application under different conditions, which is essential for thorough testing and debugging.\n\n3. **Prompt Hub**: The Prompt Hub is a built-in tool in LangSmith for managing prompts used in your LLM application. During testing, you can use the Prompt Hub to organize and manage different test cases and input prompts. This makes it easier to iterate on testing scenarios, compare results, and refine your LLM model based on testing feedback.\n\n4. **Proxy**: LangSmith&#39;s proxy capabilities provide control and management over access to your LLM application. This is particularly useful during testing when you want to restrict access to the application to specific users or systems. By using the proxy features, you can ensure that testing environments are properly controlled and secured, reducing the risk of unauthorized access or misuse.\n\n5. **Cookbook and Additional Resources**: The LangSmith Cookbook and additional resources provide tutorials, walkthroughs, and best practices for testing LLM applications. These resources cover various aspects of testing, including setting up test environments, designing test cases, interpreting test results, and optimizing performance. By leveraging these resources, you can improve the effectiveness and efficiency of your testing processes.\n\nOverall, LangSmith offers a comprehensive suite of tools and resources to support testing of LLM applications at every stage of development. From monitoring and evaluation to tracing, prompt management, proxying, and access to helpful documentation, LangSmith empowers developers to conduct thorough testing and ensure the reliability and performance of their LLM applications before deployment.&#34;</span><span style=color:#ff79c6>}</span>
</span></span></code></pre></div><h3 id=工具使用代理>工具使用&代理</h3><p>使用工具有两种主要方式：</p><ul><li>链(chains)</li><li>代理(agents)</li></ul><h4 id=架构-1>架构</h4><p>链中调用工具
<img src=https://python.langchain.com/v0.1/assets/images/tool_chain-3571e7fbc481d648aff93a2630f812ab.svg alt=LangSmith></p><p>Agent中调用工具
<img src=https://python.langchain.com/v0.1/assets/images/tool_agent-d25fafc271da3ee950ac1fba59cdf490.svg alt=LangSmith></p><h4 id=代码实现-2>代码实现</h4><p>链中调用工具</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic.v1 <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.tools <span style=color:#ff79c6>import</span> tool
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@tool
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>multiply</span>(first_int: <span style=color:#8be9fd;font-style:italic>int</span>, second_int: <span style=color:#8be9fd;font-style:italic>int</span>) <span style=color:#ff79c6>-&gt;</span> <span style=color:#8be9fd;font-style:italic>int</span>:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;Multiply two integers together.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> first_int <span style=color:#ff79c6>*</span> second_int
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    llm_with_tools <span style=color:#ff79c6>=</span> llm<span style=color:#ff79c6>.</span>bind_tools([multiply])
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 直接函数/工具调用</span>
</span></span><span style=display:flex><span>    msg <span style=color:#ff79c6>=</span> llm_with_tools<span style=color:#ff79c6>.</span>invoke(<span style=color:#f1fa8c>&#34;whats 5 times forty two&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(msg<span style=color:#ff79c6>.</span>tool_calls)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 链中调用工具</span>
</span></span><span style=display:flex><span>    chain <span style=color:#ff79c6>=</span> llm_with_tools <span style=color:#ff79c6>|</span> (<span style=color:#ff79c6>lambda</span> x: x<span style=color:#ff79c6>.</span>tool_calls[<span style=color:#bd93f9>0</span>][<span style=color:#f1fa8c>&#34;args&#34;</span>]) <span style=color:#ff79c6>|</span> multiply
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(chain<span style=color:#ff79c6>.</span>invoke(<span style=color:#f1fa8c>&#34;What&#39;s four times 23&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#ff79c6>[{</span><span style=color:#f1fa8c>&#39;name&#39;</span>: <span style=color:#f1fa8c>&#39;multiply&#39;</span>, <span style=color:#f1fa8c>&#39;args&#39;</span>: <span style=color:#ff79c6>{</span><span style=color:#f1fa8c>&#39;first_int&#39;</span>: 5, <span style=color:#f1fa8c>&#39;second_int&#39;</span>: 42<span style=color:#ff79c6>}</span>, <span style=color:#f1fa8c>&#39;id&#39;</span>: <span style=color:#f1fa8c>&#39;call_tYHGZbXVcZo2KTHTF9PGTEW3&#39;</span><span style=color:#ff79c6>}]</span>
</span></span><span style=display:flex><span><span style=color:#bd93f9>92</span>
</span></span></code></pre></div><p>Agent中调用工具</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic.v1 <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.tools <span style=color:#ff79c6>import</span> tool
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain <span style=color:#ff79c6>import</span> hub
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.agents <span style=color:#ff79c6>import</span> AgentExecutor, create_tool_calling_agent
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@tool
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>multiply</span>(first_int: <span style=color:#8be9fd;font-style:italic>int</span>, second_int: <span style=color:#8be9fd;font-style:italic>int</span>) <span style=color:#ff79c6>-&gt;</span> <span style=color:#8be9fd;font-style:italic>int</span>:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;Multiply two integers together.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> first_int <span style=color:#ff79c6>*</span> second_int
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@tool
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>add</span>(first_int: <span style=color:#8be9fd;font-style:italic>int</span>, second_int: <span style=color:#8be9fd;font-style:italic>int</span>) <span style=color:#ff79c6>-&gt;</span> <span style=color:#8be9fd;font-style:italic>int</span>:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;Add two integers.&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> first_int <span style=color:#ff79c6>+</span> second_int
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@tool
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>exponentiate</span>(base: <span style=color:#8be9fd;font-style:italic>int</span>, exponent: <span style=color:#8be9fd;font-style:italic>int</span>) <span style=color:#ff79c6>-&gt;</span> <span style=color:#8be9fd;font-style:italic>int</span>:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;Exponentiate the base to the exponent power.&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> base<span style=color:#ff79c6>**</span>exponent
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Get the prompt to use - can be replaced with any prompt that includes variables &#34;agent_scratchpad&#34; and &#34;input&#34;!</span>
</span></span><span style=display:flex><span>    prompt <span style=color:#ff79c6>=</span> hub<span style=color:#ff79c6>.</span>pull(<span style=color:#f1fa8c>&#34;hwchase17/openai-tools-agent&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    tools <span style=color:#ff79c6>=</span> [multiply, add, exponentiate]
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Construct the tool calling agent</span>
</span></span><span style=display:flex><span>    agent <span style=color:#ff79c6>=</span> create_tool_calling_agent(llm, tools, prompt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Create an agent executor by passing in the agent and tools</span>
</span></span><span style=display:flex><span>    agent_executor <span style=color:#ff79c6>=</span> AgentExecutor(agent<span style=color:#ff79c6>=</span>agent, tools<span style=color:#ff79c6>=</span>tools, verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    agent_executor<span style=color:#ff79c6>.</span>invoke(
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; Entering new AgentExecutor chain...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Invoking: <span style=color:#f1fa8c>`</span>exponentiate<span style=color:#f1fa8c>`</span> with <span style=color:#f1fa8c>`</span><span style=color:#ff79c6>{</span><span style=color:#f1fa8c>&#39;base&#39;</span>: 3, <span style=color:#f1fa8c>&#39;exponent&#39;</span>: 5<span style=color:#ff79c6>}</span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#bd93f9>243</span>
</span></span><span style=display:flex><span>Invoking: <span style=color:#f1fa8c>`</span>add<span style=color:#f1fa8c>`</span> with <span style=color:#f1fa8c>`</span><span style=color:#ff79c6>{</span><span style=color:#f1fa8c>&#39;first_int&#39;</span>: 12, <span style=color:#f1fa8c>&#39;second_int&#39;</span>: 3<span style=color:#ff79c6>}</span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#bd93f9>15</span>
</span></span><span style=display:flex><span>Invoking: <span style=color:#f1fa8c>`</span>multiply<span style=color:#f1fa8c>`</span> with <span style=color:#f1fa8c>`</span><span style=color:#ff79c6>{</span><span style=color:#f1fa8c>&#39;first_int&#39;</span>: 243, <span style=color:#f1fa8c>&#39;second_int&#39;</span>: 15<span style=color:#ff79c6>}</span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#bd93f9>3645</span>
</span></span><span style=display:flex><span>Invoking: <span style=color:#f1fa8c>`</span>exponentiate<span style=color:#f1fa8c>`</span> with <span style=color:#f1fa8c>`</span><span style=color:#ff79c6>{</span><span style=color:#f1fa8c>&#39;base&#39;</span>: 3645, <span style=color:#f1fa8c>&#39;exponent&#39;</span>: 2<span style=color:#ff79c6>}</span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>13286025The result of taking <span style=color:#bd93f9>3</span> to the fifth power and multiplying that by the sum of twelve and three, <span style=color:#ff79c6>then</span> squaring the whole result is 13,286,025.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span></code></pre></div><h3 id=查询分析>查询分析</h3><p><img src=https://python.langchain.com/v0.1/assets/images/query_analysis-cf7fe2eec43fce1e2e8feb1a16413fab.png alt>
使用查询分析可以在某些方面上提高查询的质量，借助LLM已经变成一种越来越流行的方式针对问答场景。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.document_loaders.youtube <span style=color:#ff79c6>import</span> YoutubeLoader
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic.v1 <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> datetime
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_chroma <span style=color:#ff79c6>import</span> Chroma
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> OpenAIEmbeddings
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_text_splitters <span style=color:#ff79c6>import</span> RecursiveCharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.prompts <span style=color:#ff79c6>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.runnables <span style=color:#ff79c6>import</span> RunnablePassthrough
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> typing <span style=color:#ff79c6>import</span> Optional
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.pydantic_v1 <span style=color:#ff79c6>import</span> BaseModel, Field
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> typing <span style=color:#ff79c6>import</span> List
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.documents <span style=color:#ff79c6>import</span> Document
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>Search</span>(BaseModel):
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;Search over a database of tutorial videos about a software library.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    query: <span style=color:#8be9fd;font-style:italic>str</span> <span style=color:#ff79c6>=</span> Field(
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>...</span>,
</span></span><span style=display:flex><span>        description<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;Similarity search query applied to video transcripts.&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    publish_year: Optional[<span style=color:#8be9fd;font-style:italic>int</span>] <span style=color:#ff79c6>=</span> Field(
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>None</span>, description<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;Year video was published&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    urls <span style=color:#ff79c6>=</span> [
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=HAn9vnJy6S4&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=dA1cHGACXCo&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=ZcEMLz27sL4&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=hvAPnpSfSGo&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=EhlPDL4QrWY&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=mmBo8nlu2j0&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=rQdibOsL1ps&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=28lC4fqukoc&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=es-9MgxB-uc&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=wLRHwKuKvOE&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=ObIltMaRJvY&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=DjuXACWYkkU&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;https://www.youtube.com/watch?v=o7C9ld6Ln-M&#34;</span>,
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    docs <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> url <span style=color:#ff79c6>in</span> urls:
</span></span><span style=display:flex><span>        docs<span style=color:#ff79c6>.</span>extend(YoutubeLoader<span style=color:#ff79c6>.</span>from_youtube_url(
</span></span><span style=display:flex><span>            url, add_video_info<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)<span style=color:#ff79c6>.</span>load())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Add some additional metadata: what year the video was published</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> doc <span style=color:#ff79c6>in</span> docs:
</span></span><span style=display:flex><span>        doc<span style=color:#ff79c6>.</span>metadata[<span style=color:#f1fa8c>&#34;publish_year&#34;</span>] <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>int</span>(
</span></span><span style=display:flex><span>            datetime<span style=color:#ff79c6>.</span>datetime<span style=color:#ff79c6>.</span>strptime(
</span></span><span style=display:flex><span>                doc<span style=color:#ff79c6>.</span>metadata[<span style=color:#f1fa8c>&#34;publish_date&#34;</span>], <span style=color:#f1fa8c>&#34;%Y-%m-</span><span style=color:#f1fa8c>%d</span><span style=color:#f1fa8c> %H:%M:%S&#34;</span>
</span></span><span style=display:flex><span>            )<span style=color:#ff79c6>.</span>strftime(<span style=color:#f1fa8c>&#34;%Y&#34;</span>)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    text_splitter <span style=color:#ff79c6>=</span> RecursiveCharacterTextSplitter(chunk_size<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2000</span>)
</span></span><span style=display:flex><span>    chunked_docs <span style=color:#ff79c6>=</span> text_splitter<span style=color:#ff79c6>.</span>split_documents(docs)
</span></span><span style=display:flex><span>    embeddings <span style=color:#ff79c6>=</span> OpenAIEmbeddings(model<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;text-embedding-3-small&#34;</span>,
</span></span><span style=display:flex><span>                                  api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>                                  base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    vectorstore <span style=color:#ff79c6>=</span> Chroma<span style=color:#ff79c6>.</span>from_documents(
</span></span><span style=display:flex><span>        chunked_docs,
</span></span><span style=display:flex><span>        embeddings,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># retrivael without query analysis</span>
</span></span><span style=display:flex><span>    search_results <span style=color:#ff79c6>=</span> vectorstore<span style=color:#ff79c6>.</span>similarity_search(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;how do I build a RAG agent&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(search_results[<span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>metadata[<span style=color:#f1fa8c>&#34;title&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(search_results[<span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>page_content[:<span style=color:#bd93f9>500</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># query analysis</span>
</span></span><span style=display:flex><span>    system <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;You are an expert at converting user questions into database queries. </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>    You have access to a database of tutorial videos about a software library for building LLM-powered applications. </span><span style=color:#f1fa8c>\
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c></span><span style=color:#f1fa8c>    Given a question, return a list of database queries optimized to retrieve the most relevant results.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    If there are acronyms or words you are not familiar with, do not try to rephrase them.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    prompt <span style=color:#ff79c6>=</span> ChatPromptTemplate<span style=color:#ff79c6>.</span>from_messages(
</span></span><span style=display:flex><span>        [
</span></span><span style=display:flex><span>            (<span style=color:#f1fa8c>&#34;system&#34;</span>, system),
</span></span><span style=display:flex><span>            (<span style=color:#f1fa8c>&#34;human&#34;</span>, <span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{question}</span><span style=color:#f1fa8c>&#34;</span>),
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    structured_llm <span style=color:#ff79c6>=</span> llm<span style=color:#ff79c6>.</span>with_structured_output(Search)
</span></span><span style=display:flex><span>    query_analyzer <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;question&#34;</span>: RunnablePassthrough()} <span style=color:#ff79c6>|</span> prompt <span style=color:#ff79c6>|</span> structured_llm
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(query_analyzer<span style=color:#ff79c6>.</span>invoke(<span style=color:#f1fa8c>&#34;how do I build a RAG agent&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>retrieval</span>(search: Search) <span style=color:#ff79c6>-&gt;</span> List[Document]:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> search<span style=color:#ff79c6>.</span>publish_year <span style=color:#ff79c6>is</span> <span style=color:#ff79c6>not</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>            <span style=color:#6272a4># This is syntax specific to Chroma,</span>
</span></span><span style=display:flex><span>            <span style=color:#6272a4># the vector database we are using.</span>
</span></span><span style=display:flex><span>            _filter <span style=color:#ff79c6>=</span> {<span style=color:#f1fa8c>&#34;publish_year&#34;</span>: {<span style=color:#f1fa8c>&#34;$eq&#34;</span>: search<span style=color:#ff79c6>.</span>publish_year}}
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>            _filter <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> vectorstore<span style=color:#ff79c6>.</span>similarity_search(search<span style=color:#ff79c6>.</span>query, <span style=color:#8be9fd;font-style:italic>filter</span><span style=color:#ff79c6>=</span>_filter)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    retrieval_chain <span style=color:#ff79c6>=</span> query_analyzer <span style=color:#ff79c6>|</span> retrieval
</span></span><span style=display:flex><span>    results <span style=color:#ff79c6>=</span> retrieval_chain<span style=color:#ff79c6>.</span>invoke(<span style=color:#f1fa8c>&#34;RAG tutorial published in 2023&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(results)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>OpenGPTs
</span></span><span style=display:flex><span>hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don&#39;t need to do a retrieval step when I said hi it didn&#39;t need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it&#39;s more focused there because this is also a simpler architecture so it&#39;s always
</span></span><span style=display:flex><span>/opt/anaconda3/envs/chatgpt/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.
</span></span><span style=display:flex><span>  warn_beta(
</span></span><span style=display:flex><span>query=&#39;build RAG agent&#39; publish_year=None
</span></span><span style=display:flex><span>[Document(page_content=&#34;capacity and conventional rag approaches that just strip the text out really miss a lot of this so let&#39;s try kind of how could we build a rag system over the visual content in in a slide deck um so to start off what I did was I took a slide deck and this is um uh data dog&#39;s Q3 earnings report I randomly chose it you know it was just like an interesting demonstration of like kind of complex uh you know financial information and figures and slide deck and I created a set of 10 questions and answer pairs about these slides this is like my evalve set um and this is really easy to do I can just create a CSV that has like my question and my answer in this case like my input output pairs um and it&#39;s just a set of questions that I devised myself I looked at the slides I said okay here&#39;s some interesting question answer pairs I put them in a CSV and I load these into Langs Smith now Langs Smith is Lang chain platform that supports durability and evaluations um and I create a data set for myself in Lang Smith and there&#39;s some links down here that show exactly how to do that but that&#39;s my starting point so I say okay here&#39;s my evaluation set I have the slide deck I built 10 question answer pairs from the slides now let&#39;s compare some approaches there might be two different ways to think about multimodal rag um so one is this notion of multimodal embeddings so we take our slides we extract them as images in every image we use multimodal embeddings to map them into this kind of this embedding space that is common between kind of text and and images um for that I use open clip embeddings um and so I now have an index in this case I use chroma that contains a bunch of images uh that have been embedded using open clip um at retrieval time I ask a question I use I basically take the natural language question embed it indeed with multimodal embeddings same ones similarity search just like normal retrieve images that are similar to my question pass the image to in this case uh my&#34;, metadata={&#39;author&#39;: &#39;LangChain&#39;, &#39;description&#39;: &#39;Unknown&#39;, &#39;length&#39;: 1833, &#39;publish_date&#39;: &#39;2023-12-20 00:00:00&#39;, &#39;publish_year&#39;: 2023, &#39;source&#39;: &#39;28lC4fqukoc&#39;, &#39;thumbnail_url&#39;: &#39;https://i.ytimg.com/vi/28lC4fqukoc/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGCkgWChyMA8=&amp;rs=AOn4CLCPeU4y3IyyG2C3XDHmIYh8efhGbQ&#39;, &#39;title&#39;: &#39;Getting Started with Multi-Modal LLMs&#39;, &#39;view_count&#39;: 3766}), Document(page_content=&#34;uh context for it um instead of uh instead of asking a question so here we can just add our second route so now we&#39;re going to have um two different sets of endpoints um and I can actually just show that off in the uh fast API doc so if I refresh this we&#39;ll see now that we have all the invoke batch stream and stream log uh calls for rag conversation which was the first example that we went over or first template that we went over um and we also now have these extraction open AI functions ones um which just taken a single string instead of both like a chat history and a question and so if we go to our playground um so this is going to be our rag conversation playground um but we can go to extraction openingi functions um and we&#39;re adding a little index so it&#39;s easier to to get to these links um so in this case if we look at the readme for extraction open AI functions um what this is going to do is it&#39;s going to um extract the title and author of papers um which uh we&#39;ll look at in a sec and we&#39;ll we&#39;ll try and customize it to extract something else um but we can actually just use the same article over here um just because it also has paper ERS and authors um so if we just paste in some section of this um we can see that it&#39;s able not reminders um we can see that it&#39;s able to extract out um those authors and papers that are kind of covered in this Tas do composition section um and let&#39;s actually go into that template um to see why it&#39;s it&#39;s doing just papers um in instead um so here we can see that we have just a prompt going into a model uh which has kind of some open AI functions um set on it and then we&#39;ll talk a little bit how about how we can design those ourselves um and then in the end it&#39;s just going to Output that papers key um which is just going to be a list of papers according to our kind of pedantic model here um and then we can see that it&#39;s extracting title an author because we um Define those as as the fields to extract so let&#39;s say um I don&#39;t know&#34;, metadata={&#39;author&#39;: &#39;LangChain&#39;, &#39;description&#39;: &#39;Unknown&#39;, &#39;length&#39;: 2441, &#39;publish_date&#39;: &#39;2023-11-02 00:00:00&#39;, &#39;publish_year&#39;: 2023, &#39;source&#39;: &#39;o7C9ld6Ln-M&#39;, &#39;thumbnail_url&#39;: &#39;https://i.ytimg.com/vi/o7C9ld6Ln-M/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&amp;rs=AOn4CLDf7gvV8D3I2UFy0UsA2Wh0qUhA-A&#39;, &#39;title&#39;: &#39;LangServe and LangChain Templates Webinar&#39;, &#39;view_count&#39;: 5000}), Document(page_content=&#34;and reason about what&#39;s going on so so that&#39;s maybe like a simple like mental model how to think about what&#39;s happening when you work with multimodal LMS um yeah let&#39;s talk a about use cases so Greg Cameron on Twitter kind of had this kind of nice visualization of a bunch of things that have been been shown with GPD 4V um a lot of people seen really cool demos with image captioning um extractions a really good one taking an image extracting elements text elements and so forth um recommendations so there&#39;s kind of like a lot of design applications um kind of suggestions about how to improve the visual Aesthetics of a scene of a of a of of like you know um of an object um and of course like interpretation this is like you know common in the rag context for example if you have like a you know collection of say we&#39;ll talk a little bit later to it a little bit later about slides um or about diagrams in documents you can of course use a vision model to reason about what&#39;s happening there in a question answer context um and this was like an intering demonstration of of extraction uh shown in the in the gbd uh 4V paper here uh actually this is a follow on to the GPD 4V model by Microsoft showing here are some interesting um explorations and they they talked about kind of extraction from complex documents um so let&#39;s actually walk through a demo to make this a little bit more concrete and I&#39;ll share kind of a bunch of code and and templates that can be easily reused later um so I think you know presentations like slide decks are a really good application for vision models because they&#39;re inherently kind of visual they have lots of kind of complex visual elements like like graphs uh tables figures and they&#39;re very common you know every nearly every organization uses slides in some capacity and conventional rag approaches that just strip the text out really miss a lot of this so let&#39;s try kind of how could we build a rag system over the visual content in in a slide deck um so&#34;, metadata={&#39;author&#39;: &#39;LangChain&#39;, &#39;description&#39;: &#39;Unknown&#39;, &#39;length&#39;: 1833, &#39;publish_date&#39;: &#39;2023-12-20 00:00:00&#39;, &#39;publish_year&#39;: 2023, &#39;source&#39;: &#39;28lC4fqukoc&#39;, &#39;thumbnail_url&#39;: &#39;https://i.ytimg.com/vi/28lC4fqukoc/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGCkgWChyMA8=&amp;rs=AOn4CLCPeU4y3IyyG2C3XDHmIYh8efhGbQ&#39;, &#39;title&#39;: &#39;Getting Started with Multi-Modal LLMs&#39;, &#39;view_count&#39;: 3766}), Document(page_content=&#34;this is the main thing I want to serve the thing that I copy has a bunch of extraneous things we can easily remove that um we can now do add routes app chain this is all useless stuff from before let&#39;s call This research assistant um and then we can do if we do this install SEC Starlet that&#39;s an easy fix no need to run that twice query run main install unicorn burun may now we can go here and we can add in research assistant playground and so now we get this thing what is the difference between L chain let&#39;s change it up what&#39;s the difference between L chain and open AI so this is a nice little this is all autogenerated we know that the input&#39;s question because we we know the internals of the chain that we wrote you can see the intermediate steps streams things automatically um Lang chain and open a are two prominent entities in the sphere each offering unique Frameworks and models so not exactly right we don&#39;t offer any models okay here we go open the eye provider of Link language models um Lang chain is a framework for language model applications cool so it gets those right general purpose versus chat focused um okay so it talks about the two different classes in Lang chain talks about our Integrations um developer platform um and conclusion and so we get a bunch of sources as well so that&#39;s pretty much it for this video um I&#39;ll post the code for this um in a in a simple gist or something um I&#39;ll also post uh the code for a more complex uh research assistant um oh let&#39;s maybe do one last thing let&#39;s maybe change this so instead of scraping the web it&#39;s using a different retriever of our choice and this is really interesting because uh you can now change it to do to do research over any corporate of data that you want so we&#39;ll change it we&#39;ll do some research over uh let&#39;s do some research over um over archive data all right so I&#39;ve done some basic setup I&#39;ve imported the archive retriever from L chain and I&#39;ve got uh I&#39;ve created the retriever class here what&#34;, metadata={&#39;author&#39;: &#39;LangChain&#39;, &#39;description&#39;: &#39;Unknown&#39;, &#39;length&#39;: 2620, &#39;publish_date&#39;: &#39;2023-11-16 00:00:00&#39;, &#39;publish_year&#39;: 2023, &#39;source&#39;: &#39;DjuXACWYkkU&#39;, &#39;thumbnail_url&#39;: &#39;https://i.ytimg.com/vi/DjuXACWYkkU/hq720.jpg&#39;, &#39;title&#39;: &#39;Building a Research Assistant from Scratch&#39;, &#39;view_count&#39;: 19059})]
</span></span></code></pre></div><h3 id=基于sqlcsv的问答>基于SQL/CSV的问答</h3><p>基于SQL语言的数据库问答，LangChain基于SQLAlchemy库提供了一系列的内置链和Agent实现此功能</p><h4 id=架构-2>架构</h4><p><img src=https://python.langchain.com/v0.1/assets/images/sql_usecase-d432701261f05ab69b38576093718cf3.png alt>
任何SQL链和Agent的步骤都是如下：</p><ul><li>把问题转换为SQL查询</li><li>执行SQL查询</li><li>使用查询结果回答问题</li></ul><h4 id=代码实现-3>代码实现</h4><p>创建Chinook db，并进行初始化</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#6272a4># 下载Chinook_Sqlite.sql</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># curl https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql -o Chinook_Sqlite.sql</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 创建Chinook.db</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># sqlite3 Chinook.db</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># .read Chinook_Sqlite.sql</span>
</span></span><span style=display:flex><span><span style=color:#6272a4># SELECT * FROM Artist LIMIT 10;</span>
</span></span><span style=display:flex><span>1|AC/DC
</span></span><span style=display:flex><span>2|Accept
</span></span><span style=display:flex><span>3|Aerosmith
</span></span><span style=display:flex><span>4|Alanis Morissette
</span></span><span style=display:flex><span>5|Alice In Chains
</span></span><span style=display:flex><span>6|Antônio Carlos Jobim
</span></span><span style=display:flex><span>7|Apocalyptica
</span></span><span style=display:flex><span>8|Audioslave
</span></span><span style=display:flex><span>9|BackBeat
</span></span><span style=display:flex><span>10|Billy Cobham
</span></span><span style=display:flex><span><span style=color:#6272a4># .exit</span>
</span></span></code></pre></div><p>内置链实现SQL查询</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic.v1 <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.utilities.sql_database <span style=color:#ff79c6>import</span> SQLDatabase
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.chains <span style=color:#ff79c6>import</span> create_sql_query_chain
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.tools.sql_database.tool <span style=color:#ff79c6>import</span> QuerySQLDataBaseTool
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> operator <span style=color:#ff79c6>import</span> itemgetter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.output_parsers <span style=color:#ff79c6>import</span> StrOutputParser
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.prompts <span style=color:#ff79c6>import</span> PromptTemplate
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.runnables <span style=color:#ff79c6>import</span> RunnablePassthrough
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    db <span style=color:#ff79c6>=</span> SQLDatabase<span style=color:#ff79c6>.</span>from_uri(<span style=color:#f1fa8c>&#34;sqlite:///Chinook.db&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(db<span style=color:#ff79c6>.</span>dialect)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(db<span style=color:#ff79c6>.</span>get_usable_table_names())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 问题转化为SQL查询</span>
</span></span><span style=display:flex><span>    chain <span style=color:#ff79c6>=</span> create_sql_query_chain(llm, db)
</span></span><span style=display:flex><span>    response <span style=color:#ff79c6>=</span> chain<span style=color:#ff79c6>.</span>invoke({<span style=color:#f1fa8c>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;How many employees are there&#34;</span>})
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(response)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 执行SQL查询</span>
</span></span><span style=display:flex><span>    execute_query <span style=color:#ff79c6>=</span> QuerySQLDataBaseTool(db<span style=color:#ff79c6>=</span>db)
</span></span><span style=display:flex><span>    write_query <span style=color:#ff79c6>=</span> create_sql_query_chain(llm, db)
</span></span><span style=display:flex><span>    chain <span style=color:#ff79c6>=</span> write_query <span style=color:#ff79c6>|</span> execute_query
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(chain<span style=color:#ff79c6>.</span>invoke({<span style=color:#f1fa8c>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;How many employees are there&#34;</span>}))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 回答问题</span>
</span></span><span style=display:flex><span>    answer_prompt <span style=color:#ff79c6>=</span> PromptTemplate<span style=color:#ff79c6>.</span>from_template(
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;&#34;&#34;Given the following user question, corresponding SQL query, and SQL result, answer the user question.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    Question: </span><span style=color:#f1fa8c>{question}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    SQL Query: </span><span style=color:#f1fa8c>{query}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    SQL Result: </span><span style=color:#f1fa8c>{result}</span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    Answer: &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    answer <span style=color:#ff79c6>=</span> answer_prompt <span style=color:#ff79c6>|</span> llm <span style=color:#ff79c6>|</span> StrOutputParser()
</span></span><span style=display:flex><span>    chain <span style=color:#ff79c6>=</span> (
</span></span><span style=display:flex><span>        RunnablePassthrough<span style=color:#ff79c6>.</span>assign(query<span style=color:#ff79c6>=</span>write_query)<span style=color:#ff79c6>.</span>assign(
</span></span><span style=display:flex><span>            result<span style=color:#ff79c6>=</span>itemgetter(<span style=color:#f1fa8c>&#34;query&#34;</span>) <span style=color:#ff79c6>|</span> execute_query
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>|</span> answer
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(chain<span style=color:#ff79c6>.</span>invoke({<span style=color:#f1fa8c>&#34;question&#34;</span>: <span style=color:#f1fa8c>&#34;How many employees are there&#34;</span>}))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sqlite
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span><span style=color:#f1fa8c>&#39;Album&#39;</span>, <span style=color:#f1fa8c>&#39;Artist&#39;</span>, <span style=color:#f1fa8c>&#39;Customer&#39;</span>, <span style=color:#f1fa8c>&#39;Employee&#39;</span>, <span style=color:#f1fa8c>&#39;Genre&#39;</span>, <span style=color:#f1fa8c>&#39;Invoice&#39;</span>, <span style=color:#f1fa8c>&#39;InvoiceLine&#39;</span>, <span style=color:#f1fa8c>&#39;MediaType&#39;</span>, <span style=color:#f1fa8c>&#39;Playlist&#39;</span>, <span style=color:#f1fa8c>&#39;PlaylistTrack&#39;</span>, <span style=color:#f1fa8c>&#39;Track&#39;</span><span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>SELECT COUNT<span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;EmployeeId&#34;</span><span style=color:#ff79c6>)</span> AS <span style=color:#f1fa8c>&#34;TotalEmployees&#34;</span> FROM <span style=color:#f1fa8c>&#34;Employee&#34;</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>[(</span>8,<span style=color:#ff79c6>)]</span>
</span></span><span style=display:flex><span>There are a total of <span style=color:#bd93f9>8</span> employees.
</span></span></code></pre></div><p>内置Agent实现SQL查询</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic.v1 <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.utilities.sql_database <span style=color:#ff79c6>import</span> SQLDatabase
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.agent_toolkits.sql.base <span style=color:#ff79c6>import</span> create_sql_agent
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://api.chatanywhere.com.cn/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    db <span style=color:#ff79c6>=</span> SQLDatabase<span style=color:#ff79c6>.</span>from_uri(<span style=color:#f1fa8c>&#34;sqlite:///Chinook.db&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(db<span style=color:#ff79c6>.</span>dialect)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(db<span style=color:#ff79c6>.</span>get_usable_table_names())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0.0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    agent_executor <span style=color:#ff79c6>=</span> create_sql_agent(
</span></span><span style=display:flex><span>        llm, db<span style=color:#ff79c6>=</span>db, agent_type<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;openai-tools&#34;</span>, verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    agent_executor<span style=color:#ff79c6>.</span>invoke(
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;input&#34;</span>: <span style=color:#f1fa8c>&#34;List the total sales per country. Which country&#39;s customers spent the most?&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sqlite
</span></span><span style=display:flex><span><span style=color:#ff79c6>[</span><span style=color:#f1fa8c>&#39;Album&#39;</span>, <span style=color:#f1fa8c>&#39;Artist&#39;</span>, <span style=color:#f1fa8c>&#39;Customer&#39;</span>, <span style=color:#f1fa8c>&#39;Employee&#39;</span>, <span style=color:#f1fa8c>&#39;Genre&#39;</span>, <span style=color:#f1fa8c>&#39;Invoice&#39;</span>, <span style=color:#f1fa8c>&#39;InvoiceLine&#39;</span>, <span style=color:#f1fa8c>&#39;MediaType&#39;</span>, <span style=color:#f1fa8c>&#39;Playlist&#39;</span>, <span style=color:#f1fa8c>&#39;PlaylistTrack&#39;</span>, <span style=color:#f1fa8c>&#39;Track&#39;</span><span style=color:#ff79c6>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Entering new SQL Agent Executor chain...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Invoking: <span style=color:#f1fa8c>`</span>sql_db_list_tables<span style=color:#f1fa8c>`</span> with <span style=color:#f1fa8c>`</span><span style=color:#ff79c6>{}</span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track
</span></span><span style=display:flex><span>Invoking: <span style=color:#f1fa8c>`</span>sql_db_schema<span style=color:#f1fa8c>`</span> with <span style=color:#f1fa8c>`</span><span style=color:#ff79c6>{</span><span style=color:#f1fa8c>&#39;table_names&#39;</span>: <span style=color:#f1fa8c>&#39;Customer, Invoice, InvoiceLine&#39;</span><span style=color:#ff79c6>}</span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>CREATE TABLE <span style=color:#f1fa8c>&#34;Customer&#34;</span> <span style=color:#ff79c6>(</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;CustomerId&#34;</span> INTEGER NOT NULL, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;FirstName&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>40<span style=color:#ff79c6>)</span> NOT NULL, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;LastName&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>20<span style=color:#ff79c6>)</span> NOT NULL, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;Company&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>80<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;Address&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>70<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;City&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>40<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;State&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>40<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;Country&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>40<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;PostalCode&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>10<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;Phone&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>24<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;Fax&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>24<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;Email&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>60<span style=color:#ff79c6>)</span> NOT NULL, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;SupportRepId&#34;</span> INTEGER, 
</span></span><span style=display:flex><span>        PRIMARY KEY <span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;CustomerId&#34;</span><span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        FOREIGN KEY<span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;SupportRepId&#34;</span><span style=color:#ff79c6>)</span> REFERENCES <span style=color:#f1fa8c>&#34;Employee&#34;</span> <span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;EmployeeId&#34;</span><span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>/*
</span></span><span style=display:flex><span><span style=color:#bd93f9>3</span> rows from Customer table:
</span></span><span style=display:flex><span>CustomerId      FirstName       LastName        Company Address City    State   Country PostalCode      Phone    Fax     Email   SupportRepId
</span></span><span style=display:flex><span><span style=color:#bd93f9>1</span>       Luís    Gonçalves       Embraer - Empresa Brasileira de Aeronáutica S.A.        Av. Brigadeiro Faria Lima, <span style=color:#bd93f9>2170</span>  São José dos Campos     SP      Brazil  12227-000       +55 <span style=color:#ff79c6>(</span>12<span style=color:#ff79c6>)</span> 3923-5555      +55 <span style=color:#ff79c6>(</span>12<span style=color:#ff79c6>)</span> 3923-5566       luisg@embraer.com.br    <span style=color:#bd93f9>3</span>
</span></span><span style=display:flex><span><span style=color:#bd93f9>2</span>       Leonie  Köhler  None    Theodor-Heuss-Straße <span style=color:#bd93f9>34</span> Stuttgart       None    Germany <span style=color:#bd93f9>70174</span>   +49 <span style=color:#bd93f9>0711</span> <span style=color:#bd93f9>2842222</span> None    leonekohler@surfeu.de   <span style=color:#bd93f9>5</span>
</span></span><span style=display:flex><span><span style=color:#bd93f9>3</span>       François        Tremblay        None    <span style=color:#bd93f9>1498</span> rue Bélanger       Montréal        QC      Canada  H2G 1A7  +1 <span style=color:#ff79c6>(</span>514<span style=color:#ff79c6>)</span> 721-4711       None    ftremblay@gmail.com     <span style=color:#bd93f9>3</span>
</span></span><span style=display:flex><span>*/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>CREATE TABLE <span style=color:#f1fa8c>&#34;Invoice&#34;</span> <span style=color:#ff79c6>(</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;InvoiceId&#34;</span> INTEGER NOT NULL, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;CustomerId&#34;</span> INTEGER NOT NULL, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;InvoiceDate&#34;</span> DATETIME NOT NULL, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;BillingAddress&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>70<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;BillingCity&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>40<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;BillingState&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>40<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;BillingCountry&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>40<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;BillingPostalCode&#34;</span> NVARCHAR<span style=color:#ff79c6>(</span>10<span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;Total&#34;</span> NUMERIC<span style=color:#ff79c6>(</span>10, 2<span style=color:#ff79c6>)</span> NOT NULL, 
</span></span><span style=display:flex><span>        PRIMARY KEY <span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;InvoiceId&#34;</span><span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        FOREIGN KEY<span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;CustomerId&#34;</span><span style=color:#ff79c6>)</span> REFERENCES <span style=color:#f1fa8c>&#34;Customer&#34;</span> <span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;CustomerId&#34;</span><span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>/*
</span></span><span style=display:flex><span><span style=color:#bd93f9>3</span> rows from Invoice table:
</span></span><span style=display:flex><span>InvoiceId       CustomerId      InvoiceDate     BillingAddress  BillingCity     BillingState    BillingCountry   BillingPostalCode       Total
</span></span><span style=display:flex><span><span style=color:#bd93f9>1</span>       <span style=color:#bd93f9>2</span>       2021-01-01 00:00:00     Theodor-Heuss-Straße <span style=color:#bd93f9>34</span> Stuttgart       None    Germany <span style=color:#bd93f9>70174</span>   1.98
</span></span><span style=display:flex><span><span style=color:#bd93f9>2</span>       <span style=color:#bd93f9>4</span>       2021-01-02 00:00:00     Ullevålsveien <span style=color:#bd93f9>14</span>        Oslo    None    Norway  <span style=color:#bd93f9>0171</span>    3.96
</span></span><span style=display:flex><span><span style=color:#bd93f9>3</span>       <span style=color:#bd93f9>8</span>       2021-01-03 00:00:00     Grétrystraat <span style=color:#bd93f9>63</span> Brussels        None    Belgium <span style=color:#bd93f9>1000</span>    5.94
</span></span><span style=display:flex><span>*/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>CREATE TABLE <span style=color:#f1fa8c>&#34;InvoiceLine&#34;</span> <span style=color:#ff79c6>(</span>
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;InvoiceLineId&#34;</span> INTEGER NOT NULL, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;InvoiceId&#34;</span> INTEGER NOT NULL, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;TrackId&#34;</span> INTEGER NOT NULL, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;UnitPrice&#34;</span> NUMERIC<span style=color:#ff79c6>(</span>10, 2<span style=color:#ff79c6>)</span> NOT NULL, 
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;Quantity&#34;</span> INTEGER NOT NULL, 
</span></span><span style=display:flex><span>        PRIMARY KEY <span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;InvoiceLineId&#34;</span><span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        FOREIGN KEY<span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;TrackId&#34;</span><span style=color:#ff79c6>)</span> REFERENCES <span style=color:#f1fa8c>&#34;Track&#34;</span> <span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;TrackId&#34;</span><span style=color:#ff79c6>)</span>, 
</span></span><span style=display:flex><span>        FOREIGN KEY<span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;InvoiceId&#34;</span><span style=color:#ff79c6>)</span> REFERENCES <span style=color:#f1fa8c>&#34;Invoice&#34;</span> <span style=color:#ff79c6>(</span><span style=color:#f1fa8c>&#34;InvoiceId&#34;</span><span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>/*
</span></span><span style=display:flex><span><span style=color:#bd93f9>3</span> rows from InvoiceLine table:
</span></span><span style=display:flex><span>InvoiceLineId   InvoiceId       TrackId UnitPrice       Quantity
</span></span><span style=display:flex><span><span style=color:#bd93f9>1</span>       <span style=color:#bd93f9>1</span>       <span style=color:#bd93f9>2</span>       0.99    <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span><span style=color:#bd93f9>2</span>       <span style=color:#bd93f9>1</span>       <span style=color:#bd93f9>4</span>       0.99    <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span><span style=color:#bd93f9>3</span>       <span style=color:#bd93f9>2</span>       <span style=color:#bd93f9>6</span>       0.99    <span style=color:#bd93f9>1</span>
</span></span><span style=display:flex><span>*/
</span></span><span style=display:flex><span>Invoking: <span style=color:#f1fa8c>`</span>sql_db_query<span style=color:#f1fa8c>`</span> with <span style=color:#f1fa8c>`</span><span style=color:#ff79c6>{</span><span style=color:#f1fa8c>&#39;query&#39;</span>: <span style=color:#f1fa8c>&#39;SELECT BillingCountry AS Country, SUM(Total) AS TotalSales FROM Invoice GROUP BY BillingCountry ORDER BY TotalSales DESC;&#39;</span><span style=color:#ff79c6>}</span><span style=color:#f1fa8c>`</span>
</span></span><span style=display:flex><span>responded: To find the total sales per country, we need to sum the total amount from the invoices <span style=color:#ff79c6>for</span> each country. Here is the query to achieve this:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f1fa8c>```</span>sql
</span></span><span style=display:flex><span>SELECT BillingCountry AS Country, SUM<span style=color:#ff79c6>(</span>Total<span style=color:#ff79c6>)</span> AS TotalSales
</span></span><span style=display:flex><span>FROM Invoice
</span></span><span style=display:flex><span>GROUP BY BillingCountry
</span></span><span style=display:flex><span>ORDER BY TotalSales DESC;
</span></span><span style=display:flex><span><span style=color:#f1fa8c>```</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>By running this query, we can determine which country<span style=color:#f1fa8c>&#39;s customers spent the most. Let me execute the query to provide you with the answer.
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>[(&#39;</span>USA<span style=color:#f1fa8c>&#39;, 523.0600000000003), (&#39;</span>Canada<span style=color:#f1fa8c>&#39;, 303.9599999999999), (&#39;</span>France<span style=color:#f1fa8c>&#39;, 195.09999999999994), (&#39;</span>Brazil<span style=color:#f1fa8c>&#39;, 190.09999999999997), (&#39;</span>Germany<span style=color:#f1fa8c>&#39;, 156.48), (&#39;</span>United Kingdom<span style=color:#f1fa8c>&#39;, 112.85999999999999), (&#39;</span>Czech Republic<span style=color:#f1fa8c>&#39;, 90.24000000000001), (&#39;</span>Portugal<span style=color:#f1fa8c>&#39;, 77.23999999999998), (&#39;</span>India<span style=color:#f1fa8c>&#39;, 75.25999999999999), (&#39;</span>Chile<span style=color:#f1fa8c>&#39;, 46.62), (&#39;</span>Ireland<span style=color:#f1fa8c>&#39;, 45.62), (&#39;</span>Hungary<span style=color:#f1fa8c>&#39;, 45.62), (&#39;</span>Austria<span style=color:#f1fa8c>&#39;, 42.62), (&#39;</span>Finland<span style=color:#f1fa8c>&#39;, 41.620000000000005), (&#39;</span>Netherlands<span style=color:#f1fa8c>&#39;, 40.62), (&#39;</span>Norway<span style=color:#f1fa8c>&#39;, 39.62), (&#39;</span>Sweden<span style=color:#f1fa8c>&#39;, 38.620000000000005), (&#39;</span>Poland<span style=color:#f1fa8c>&#39;, 37.620000000000005), (&#39;</span>Italy<span style=color:#f1fa8c>&#39;, 37.620000000000005), (&#39;</span>Denmark<span style=color:#f1fa8c>&#39;, 37.620000000000005), (&#39;</span>Australia<span style=color:#f1fa8c>&#39;, 37.620000000000005), (&#39;</span>Argentina<span style=color:#f1fa8c>&#39;, 37.620000000000005), (&#39;</span>Spain<span style=color:#f1fa8c>&#39;, 37.62), (&#39;</span>Belgium&#39;, 37.62<span style=color:#ff79c6>)]</span>The total sales per country are as follows:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>1. USA: <span style=color:#8be9fd;font-style:italic>$523</span>.06
</span></span><span style=display:flex><span>2. Canada: <span style=color:#8be9fd;font-style:italic>$303</span>.96
</span></span><span style=display:flex><span>3. France: <span style=color:#8be9fd;font-style:italic>$195</span>.10
</span></span><span style=display:flex><span>4. Brazil: <span style=color:#8be9fd;font-style:italic>$190</span>.10
</span></span><span style=display:flex><span>5. Germany: <span style=color:#8be9fd;font-style:italic>$156</span>.48
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Therefore, customers from the USA spent the most in total sales.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; Finished chain.
</span></span></code></pre></div><h3 id=获取外部系统的日志并分析>获取外部系统的日志并分析</h3><p>一个真实案例，利用langchain的代理agent来实现和外部系统交互，获取外部系统top10的日志并分析，汇总成邮件发送</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.globals <span style=color:#ff79c6>import</span> set_debug
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pydantic.v1 <span style=color:#ff79c6>import</span> SecretStr
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_core.tools <span style=color:#ff79c6>import</span> tool
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain <span style=color:#ff79c6>import</span> hub
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain.agents <span style=color:#ff79c6>import</span> AgentExecutor, create_tool_calling_agent
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> log <span style=color:#ff79c6>import</span> log_init, get_build_log,  email_content, extract_summary
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> notify <span style=color:#ff79c6>import</span> Notify
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>api_key <span style=color:#ff79c6>=</span> SecretStr(<span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>)
</span></span><span style=display:flex><span>openai_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;http://12.16.8.27:9997/v1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@tool
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>get_log</span>(rank: <span style=color:#8be9fd;font-style:italic>int</span>) <span style=color:#ff79c6>-&gt;</span> <span style=color:#8be9fd;font-style:italic>str</span>:
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;&#34;&#34;获取构建日志，参数rank表示速度排名，如rank=1表示获取构建速度最慢的日志&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> get_build_log(rank)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>(log_num: <span style=color:#8be9fd;font-style:italic>int</span> <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>10</span>, recipient: <span style=color:#8be9fd;font-style:italic>str</span> <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;</span>):
</span></span><span style=display:flex><span>    set_debug(<span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>    log_init(log_num)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    content <span style=color:#ff79c6>=</span> email_content(log_num)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        model_kwargs<span style=color:#ff79c6>=</span>{
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;stream_options&#34;</span>: {<span style=color:#f1fa8c>&#34;include_usage&#34;</span>: <span style=color:#ff79c6>True</span>},
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span>api_key,
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span>openai_url,
</span></span><span style=display:flex><span>        model<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;qwen1.5-72b-chat&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Get the prompt to use - can be replaced with any prompt that includes variables &#34;agent_scratchpad&#34; and &#34;input&#34;!</span>
</span></span><span style=display:flex><span>    prompt <span style=color:#ff79c6>=</span> hub<span style=color:#ff79c6>.</span>pull(<span style=color:#f1fa8c>&#34;hwchase17/openai-tools-agent&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    tools <span style=color:#ff79c6>=</span> [get_log, ]
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Construct the tool calling agent</span>
</span></span><span style=display:flex><span>    agent <span style=color:#ff79c6>=</span> create_tool_calling_agent(llm, tools, prompt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># Create an agent executor by passing in the agent and tools</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># stream_runnable=False，后端模型是vllm+qwen1.5-72b-int4，tool调用流式输出的话，结果会有问题，这里把流关闭了</span>
</span></span><span style=display:flex><span>    agent_executor <span style=color:#ff79c6>=</span> AgentExecutor(agent<span style=color:#ff79c6>=</span>agent, tools<span style=color:#ff79c6>=</span>tools, verbose<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>                                   stream_runnable<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    fm <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        `构建分析：
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        耗时排名...，服务：...（任务ID：...）
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        各阶段耗时：...
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        构建慢的主要原因：...
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        优化建议：...
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        ......（依次类推）
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        总结：......
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        TERMINATE`
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    message <span style=color:#ff79c6>=</span> (<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;我们以对话的方式分析构建日志。&#34;</span>
</span></span><span style=display:flex><span>               <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;根据现有工具，你按顺序每次提供一个日志的获取方法function_call给我，&#34;</span>
</span></span><span style=display:flex><span>               <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;我获取到日志后发送给你，不管获取成功或失败都计数，你继续把下一个的获取方法直接告诉我。&#34;</span>
</span></span><span style=display:flex><span>               <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;限制只有</span><span style=color:#f1fa8c>{</span>log_num<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>个日志，当超过日志数量限制时，按以下格式分别分析每个日志构建慢的原因：</span><span style=color:#f1fa8c>{</span>fm<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    result <span style=color:#ff79c6>=</span> agent_executor<span style=color:#ff79c6>.</span>invoke(
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;input&#34;</span>: message
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    output <span style=color:#ff79c6>=</span> result[<span style=color:#f1fa8c>&#34;output&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    analysis_result <span style=color:#ff79c6>=</span> extract_summary(output)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> <span style=color:#ff79c6>not</span> analysis_result:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> recipient:
</span></span><span style=display:flex><span>        content <span style=color:#ff79c6>+=</span> analysis_result
</span></span><span style=display:flex><span>        Notify()<span style=color:#ff79c6>.</span>send(
</span></span><span style=display:flex><span>            recipient, <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;构建时长TOP</span><span style=color:#f1fa8c>{</span>log_num<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>服务&#34;</span>, <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;&lt;html&gt;</span><span style=color:#f1fa8c>{</span>content<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&lt;/html&gt;&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;已发送邮件至</span><span style=color:#f1fa8c>{</span>recipient<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>。&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    ok <span style=color:#ff79c6>=</span> main(<span style=color:#bd93f9>10</span>, <span style=color:#f1fa8c>&#34;me&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;分析</span><span style=color:#f1fa8c>{</span><span style=color:#f1fa8c>&#39;成功&#39;</span> <span style=color:#ff79c6>if</span> ok <span style=color:#ff79c6>else</span> <span style=color:#f1fa8c>&#39;失败&#39;</span><span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>。&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># log.py: 用于解析日志</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> re
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> requests
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> datetime
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>XXX_TOKEN <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;xxx-132344553243211323&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>Ops</span>:
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> __init__(self):
</span></span><span style=display:flex><span>        self<span style=color:#ff79c6>.</span>base_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://xxx.com/ops/apis/v1/tenants/1234342346d5e&#34;</span>
</span></span><span style=display:flex><span>        self<span style=color:#ff79c6>.</span>headers <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;Authorization&#34;</span>: <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Bearer </span><span style=color:#f1fa8c>{</span>XXX_TOKEN<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>_get</span>(self, url, params<span style=color:#ff79c6>=</span><span style=color:#ff79c6>None</span>):
</span></span><span style=display:flex><span>        response <span style=color:#ff79c6>=</span> requests<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span>self<span style=color:#ff79c6>.</span>base_url<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>/</span><span style=color:#f1fa8c>{</span>url<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>, headers<span style=color:#ff79c6>=</span>self<span style=color:#ff79c6>.</span>headers, params<span style=color:#ff79c6>=</span>params)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> response<span style=color:#ff79c6>.</span>json()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 获取top10日志</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>get_top_build</span>(self, quantity<span style=color:#ff79c6>=</span><span style=color:#bd93f9>10</span>):
</span></span><span style=display:flex><span>        url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;dashboard/top_duration_builds&#34;</span>
</span></span><span style=display:flex><span>        end_date <span style=color:#ff79c6>=</span> datetime<span style=color:#ff79c6>.</span>datetime<span style=color:#ff79c6>.</span>now()<span style=color:#ff79c6>.</span>strftime(<span style=color:#f1fa8c>&#34;%Y-%m-</span><span style=color:#f1fa8c>%d</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>        start_date <span style=color:#ff79c6>=</span> (datetime<span style=color:#ff79c6>.</span>datetime<span style=color:#ff79c6>.</span>now() <span style=color:#ff79c6>-</span> datetime<span style=color:#ff79c6>.</span>timedelta(days<span style=color:#ff79c6>=</span><span style=color:#bd93f9>7</span>))<span style=color:#ff79c6>.</span>strftime(<span style=color:#f1fa8c>&#34;%Y-%m-</span><span style=color:#f1fa8c>%d</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>        params <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;start_date&#34;</span>: start_date,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;end_date&#34;</span>: end_date,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;quantity&#34;</span>: quantity
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> self<span style=color:#ff79c6>.</span>_get(url, params)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 获取任务详情</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>get_build_task</span>(self, project, service, iteration_id, tag):
</span></span><span style=display:flex><span>        url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;projects/</span><span style=color:#f1fa8c>{</span>project<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>/services/</span><span style=color:#f1fa8c>{</span>service<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>/build/tasks&#34;</span>
</span></span><span style=display:flex><span>        query <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;iteration_id&#34;</span>: iteration_id,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;tag&#34;</span>: tag
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> self<span style=color:#ff79c6>.</span>_get(url, query)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 获取任务对应的日志</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>get_build_log</span>(self, project, service, task_id):
</span></span><span style=display:flex><span>        url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;projects/</span><span style=color:#f1fa8c>{</span>project<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>/services/</span><span style=color:#f1fa8c>{</span>service<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>/build/history&#34;</span>
</span></span><span style=display:flex><span>        params <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;task_id&#34;</span>: task_id
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> self<span style=color:#ff79c6>.</span>_get(url, params)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 获取绘制成表格的内容</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>get_build_table</span>():
</span></span><span style=display:flex><span>    table <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> i, build <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>(builds):
</span></span><span style=display:flex><span>        project_manager <span style=color:#ff79c6>=</span> build<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;project_manager&#34;</span>, <span style=color:#f1fa8c>&#34;&#34;</span>)
</span></span><span style=display:flex><span>        pm_list <span style=color:#ff79c6>=</span> project_manager<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#34;,&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> <span style=color:#f1fa8c>&#34;op_superuser&#34;</span> <span style=color:#ff79c6>in</span> pm_list:
</span></span><span style=display:flex><span>            pm_list<span style=color:#ff79c6>.</span>remove(<span style=color:#f1fa8c>&#34;op_superuser&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> <span style=color:#f1fa8c>&#34;other_user&#34;</span> <span style=color:#ff79c6>in</span> pm_list:
</span></span><span style=display:flex><span>            pm_list<span style=color:#ff79c6>.</span>remove(<span style=color:#f1fa8c>&#34;other_user&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        duration <span style=color:#ff79c6>=</span> build<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;duration&#34;</span>, <span style=color:#bd93f9>0</span>) <span style=color:#ff79c6>/</span> <span style=color:#bd93f9>1000</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> duration <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>60</span>:
</span></span><span style=display:flex><span>            duration <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span><span style=color:#8be9fd;font-style:italic>int</span>(duration <span style=color:#ff79c6>//</span> <span style=color:#bd93f9>60</span>)<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>分</span><span style=color:#f1fa8c>{</span><span style=color:#8be9fd;font-style:italic>int</span>(duration <span style=color:#ff79c6>%</span> <span style=color:#bd93f9>60</span>)<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>秒&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>            duration <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span><span style=color:#8be9fd;font-style:italic>int</span>(duration)<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>秒&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        table<span style=color:#ff79c6>.</span>append({
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;耗时排名&#34;</span>: i <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;耗时&#34;</span>: duration,
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;名称&#34;</span>: build<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;pipeline_name&#34;</span>, <span style=color:#f1fa8c>&#34;&#34;</span>),
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;项目&#34;</span>: build<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;project_name&#34;</span>, <span style=color:#f1fa8c>&#34;&#34;</span>) <span style=color:#ff79c6>or</span> build<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;project_id&#34;</span>, <span style=color:#f1fa8c>&#34;&#34;</span>),
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;项目管理员&#34;</span>: <span style=color:#f1fa8c>&#34;,&#34;</span><span style=color:#ff79c6>.</span>join(pm_list),
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;任务ID&#34;</span>: build<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;pipeline_run_id&#34;</span>, <span style=color:#f1fa8c>&#34;&#34;</span>),
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;日期&#34;</span>: build<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;created_at&#34;</span>, <span style=color:#f1fa8c>&#34;&#34;</span>),
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;状态&#34;</span>: build<span style=color:#ff79c6>.</span>get(<span style=color:#f1fa8c>&#34;status&#34;</span>, <span style=color:#f1fa8c>&#34;&#34;</span>)
</span></span><span style=display:flex><span>        })
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> table
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 邮件模版</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>email_content</span>(log_num<span style=color:#ff79c6>=</span><span style=color:#bd93f9>10</span>):
</span></span><span style=display:flex><span>    content <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;&#34;&#34;各位领导好，以下是Ops平台本周构建时长TOP</span><span style=color:#f1fa8c>{</span>log_num<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>服务:&lt;br/&gt;&lt;br/&gt;&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    table_template <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &lt;table style=&#39;width: 1200px; border-collapse: collapse; border-spacing: 0; font-size: 12px; text-align: left;&#39;&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        &lt;thead style=&#34;background-color: rgb(81, 130, 187); color: #fff; border-bottom-width: 0;&#34;&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>            &lt;tr&gt;</span><span style=color:#f1fa8c>{ths}</span><span style=color:#f1fa8c>&lt;/tr&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        &lt;/thead&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>        &lt;tbody&gt;</span><span style=color:#f1fa8c>{tds}</span><span style=color:#f1fa8c>&lt;/tbody&gt;
</span></span></span><span style=display:flex><span><span style=color:#f1fa8c>    &lt;/table&gt;&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    table_data <span style=color:#ff79c6>=</span> get_build_table()
</span></span><span style=display:flex><span>    key_list <span style=color:#ff79c6>=</span> table_data[<span style=color:#bd93f9>0</span>]<span style=color:#ff79c6>.</span>keys()
</span></span><span style=display:flex><span>    ths <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    tds <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    width_map <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;耗时排名&#34;</span>: <span style=color:#f1fa8c>&#34;100px&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;耗时&#34;</span>: <span style=color:#f1fa8c>&#34;100px&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;名称&#34;</span>: <span style=color:#f1fa8c>&#34;300px&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;项目&#34;</span>: <span style=color:#f1fa8c>&#34;150px&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;项目管理员&#34;</span>: <span style=color:#f1fa8c>&#34;200px&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;任务ID&#34;</span>: <span style=color:#f1fa8c>&#34;150px&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;日期&#34;</span>: <span style=color:#f1fa8c>&#34;200px&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;状态&#34;</span>: <span style=color:#f1fa8c>&#34;100px&#34;</span>,
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> k <span style=color:#ff79c6>in</span> key_list:
</span></span><span style=display:flex><span>        ths<span style=color:#ff79c6>.</span>append(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;&lt;th style=&#39;border: 1px solid rgb(81, 130, 187); padding: 5px 10px;&#39;&gt;</span><span style=color:#f1fa8c>{</span>k<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&lt;/th&gt;&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> _, row <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>(table_data):
</span></span><span style=display:flex><span>        tds<span style=color:#ff79c6>.</span>append(<span style=color:#f1fa8c>&#34;&lt;tr style=&#39;border: 1px solid rgb(81, 130, 187);&#39;&gt;&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> k <span style=color:#ff79c6>in</span> key_list:
</span></span><span style=display:flex><span>            tds<span style=color:#ff79c6>.</span>append(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;&lt;td style=&#39;padding: 5px 10px; width: </span><span style=color:#f1fa8c>{</span>width_map<span style=color:#ff79c6>.</span>get(k, <span style=color:#f1fa8c>&#39;150px&#39;</span>)<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#39;&gt;</span><span style=color:#f1fa8c>{</span>row[k]<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&lt;/td&gt;&#34;</span>)
</span></span><span style=display:flex><span>        tds<span style=color:#ff79c6>.</span>append(<span style=color:#f1fa8c>&#34;&lt;/tr&gt;&#34;</span>)
</span></span><span style=display:flex><span>    content <span style=color:#ff79c6>+=</span> table_template<span style=color:#ff79c6>.</span>format(ths<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;&#34;</span><span style=color:#ff79c6>.</span>join(ths), tds<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;&#34;</span><span style=color:#ff79c6>.</span>join(tds))
</span></span><span style=display:flex><span>    content <span style=color:#ff79c6>+=</span> <span style=color:#f1fa8c>&#34;&lt;br/&gt;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> content
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 提取模型返回的内容</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>extract_summary</span>(result: <span style=color:#8be9fd;font-style:italic>str</span>):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 正则，匹配构建分析和TERMINATE之间的内容</span>
</span></span><span style=display:flex><span>    content <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;&#34;</span>
</span></span><span style=display:flex><span>    pattern <span style=color:#ff79c6>=</span> re<span style=color:#ff79c6>.</span>compile(<span style=color:#f1fa8c>r</span><span style=color:#f1fa8c>&#39;构建分析：(.*?)TERMINATE&#39;</span>, re<span style=color:#ff79c6>.</span>S)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>match</span> <span style=color:#ff79c6>=</span> pattern<span style=color:#ff79c6>.</span>search(result)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> <span style=color:#ff79c6>match</span>:
</span></span><span style=display:flex><span>        content <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>match</span><span style=color:#ff79c6>.</span>group(<span style=color:#bd93f9>0</span>)<span style=color:#ff79c6>.</span>replace(<span style=color:#f1fa8c>&#34;TERMINATE&#34;</span>, <span style=color:#f1fa8c>&#34;&#34;</span>)<span style=color:#ff79c6>.</span>strip()<span style=color:#ff79c6>.</span>replace(<span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, <span style=color:#f1fa8c>&#34;&lt;br/&gt;&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>        pattern <span style=color:#ff79c6>=</span> re<span style=color:#ff79c6>.</span>compile(<span style=color:#f1fa8c>r</span><span style=color:#f1fa8c>&#39;构建分析：(.*?)&#39;</span>, re<span style=color:#ff79c6>.</span>S)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>match</span> <span style=color:#ff79c6>=</span> pattern<span style=color:#ff79c6>.</span>search(result)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> <span style=color:#ff79c6>match</span>:
</span></span><span style=display:flex><span>            content <span style=color:#ff79c6>=</span> result<span style=color:#ff79c6>.</span>strip()<span style=color:#ff79c6>.</span>replace(<span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>, <span style=color:#f1fa8c>&#34;&lt;br/&gt;&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> content
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>log_init</span>(quantity<span style=color:#ff79c6>=</span><span style=color:#bd93f9>10</span>):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>global</span> z
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>global</span> builds
</span></span><span style=display:flex><span>    z <span style=color:#ff79c6>=</span> Ops()
</span></span><span style=display:flex><span>    builds <span style=color:#ff79c6>=</span> z<span style=color:#ff79c6>.</span>get_top_build(quantity)[<span style=color:#f1fa8c>&#34;data&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>cut_log</span>(log):
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 1.删除Progress Timestamp(compileStamp) 和 Progress Timestamp(buildStamp) 之间，并且前缀不是&gt;的内容</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 2.清除#编号开头中间的行，只保留相同编号的第一行和最后一行</span>
</span></span><span style=display:flex><span>    compile_stamp <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;Progress Timestamp(compileStamp)&#34;</span>
</span></span><span style=display:flex><span>    build_stamp <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;Progress Timestamp(buildStamp)&#34;</span>
</span></span><span style=display:flex><span>    push_stamp <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;Progress Timestamp(pushStamp)&#34;</span>
</span></span><span style=display:flex><span>    completed_stamp <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;Progress Timestamp(completedStamp)&#34;</span>
</span></span><span style=display:flex><span>    lines <span style=color:#ff79c6>=</span> log<span style=color:#ff79c6>.</span>split(<span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#39;</span>)
</span></span><span style=display:flex><span>    new_lines <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    is_compile <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>    is_build <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>    stage_map <span style=color:#ff79c6>=</span> {}
</span></span><span style=display:flex><span>    build_idx <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> idx, line <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>enumerate</span>(lines):
</span></span><span style=display:flex><span>        is_build_stamp <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> compile_stamp <span style=color:#ff79c6>in</span> line:
</span></span><span style=display:flex><span>            is_compile <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>            is_build <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>            line <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;开始编译：</span><span style=color:#f1fa8c>{</span>line<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>elif</span> build_stamp <span style=color:#ff79c6>in</span> line:
</span></span><span style=display:flex><span>            is_compile <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>            is_build <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>            is_build_stamp <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>            line <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;开始构建Dockerfile：</span><span style=color:#f1fa8c>{</span>line<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>elif</span> push_stamp <span style=color:#ff79c6>in</span> line:
</span></span><span style=display:flex><span>            line <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;开始推送：</span><span style=color:#f1fa8c>{</span>line<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>elif</span> completed_stamp <span style=color:#ff79c6>in</span> line:
</span></span><span style=display:flex><span>            line <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;构建完成：</span><span style=color:#f1fa8c>{</span>line<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>elif</span> is_compile <span style=color:#ff79c6>and</span> <span style=color:#ff79c6>not</span> is_build <span style=color:#ff79c6>and</span> <span style=color:#ff79c6>not</span> line<span style=color:#ff79c6>.</span>startswith(<span style=color:#f1fa8c>&#34;&gt;&#34;</span>):
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>continue</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>elif</span> line<span style=color:#ff79c6>.</span>strip() <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span> <span style=color:#ff79c6>or</span> <span style=color:#ff79c6>not</span> line<span style=color:#ff79c6>.</span>strip():
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>continue</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>match</span> <span style=color:#ff79c6>=</span> re<span style=color:#ff79c6>.</span><span style=color:#ff79c6>match</span>(<span style=color:#f1fa8c>r</span><span style=color:#f1fa8c>&#39;#(\d+) &#39;</span>, line)
</span></span><span style=display:flex><span>            <span style=color:#ff79c6>if</span> <span style=color:#ff79c6>match</span>:
</span></span><span style=display:flex><span>                stage <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>match</span><span style=color:#ff79c6>.</span>group(<span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>if</span> stage <span style=color:#ff79c6>not</span> <span style=color:#ff79c6>in</span> stage_map:
</span></span><span style=display:flex><span>                    stage_map[stage] <span style=color:#ff79c6>=</span> [line]
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>elif</span> <span style=color:#8be9fd;font-style:italic>len</span>(stage_map[stage]) <span style=color:#ff79c6>&gt;=</span> <span style=color:#bd93f9>1</span>:
</span></span><span style=display:flex><span>                    <span style=color:#ff79c6>if</span> <span style=color:#8be9fd;font-style:italic>len</span>(lines) <span style=color:#ff79c6>&lt;=</span> lines<span style=color:#ff79c6>.</span>index(line) <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span> <span style=color:#ff79c6>or</span> <span style=color:#ff79c6>not</span> lines[lines<span style=color:#ff79c6>.</span>index(line) <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>1</span>]<span style=color:#ff79c6>.</span>startswith(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#39;#</span><span style=color:#f1fa8c>{</span>stage<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c> &#39;</span>):
</span></span><span style=display:flex><span>                        stage_map[stage]<span style=color:#ff79c6>.</span>append(line)
</span></span><span style=display:flex><span>                <span style=color:#ff79c6>continue</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        new_lines<span style=color:#ff79c6>.</span>append(line)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> is_build_stamp:
</span></span><span style=display:flex><span>            build_idx <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>len</span>(new_lines)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    stage_list <span style=color:#ff79c6>=</span> stage_map<span style=color:#ff79c6>.</span>keys()
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 排序</span>
</span></span><span style=display:flex><span>    stage_list <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>sorted</span>(stage_list, key<span style=color:#ff79c6>=</span><span style=color:#ff79c6>lambda</span> x: <span style=color:#8be9fd;font-style:italic>int</span>(x))
</span></span><span style=display:flex><span>    stage_lines <span style=color:#ff79c6>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> stage <span style=color:#ff79c6>in</span> stage_list:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> line <span style=color:#ff79c6>in</span> stage_map[stage]:
</span></span><span style=display:flex><span>            stage_lines<span style=color:#ff79c6>.</span>append(line)
</span></span><span style=display:flex><span>    new_lines[build_idx:build_idx] <span style=color:#ff79c6>=</span> stage_lines
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> <span style=color:#f1fa8c>&#39;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#39;</span><span style=color:#ff79c6>.</span>join(new_lines)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>get_build_log</span>(rank: <span style=color:#8be9fd;font-style:italic>int</span>) <span style=color:#ff79c6>-&gt;</span> <span style=color:#8be9fd;font-style:italic>str</span>:
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> rank <span style=color:#ff79c6>&gt;</span> <span style=color:#8be9fd;font-style:italic>len</span>(builds):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;not found build log for rank </span><span style=color:#f1fa8c>{</span>rank<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>    build <span style=color:#ff79c6>=</span> builds[rank <span style=color:#ff79c6>-</span> <span style=color:#bd93f9>1</span>]
</span></span><span style=display:flex><span>    project <span style=color:#ff79c6>=</span> build[<span style=color:#f1fa8c>&#34;project_id&#34;</span>]
</span></span><span style=display:flex><span>    iteration_id <span style=color:#ff79c6>=</span> build[<span style=color:#f1fa8c>&#34;iteration_id&#34;</span>]
</span></span><span style=display:flex><span>    service <span style=color:#ff79c6>=</span> build[<span style=color:#f1fa8c>&#34;service_id&#34;</span>]
</span></span><span style=display:flex><span>    tag <span style=color:#ff79c6>=</span> build[<span style=color:#f1fa8c>&#34;pipeline_run_id&#34;</span>]
</span></span><span style=display:flex><span>    task <span style=color:#ff79c6>=</span> z<span style=color:#ff79c6>.</span>get_build_task(project, service, iteration_id, tag)[<span style=color:#f1fa8c>&#34;data&#34;</span>][<span style=color:#f1fa8c>&#34;items&#34;</span>][<span style=color:#bd93f9>0</span>]
</span></span><span style=display:flex><span>    task_id <span style=color:#ff79c6>=</span> task[<span style=color:#f1fa8c>&#34;id&#34;</span>]
</span></span><span style=display:flex><span>    log <span style=color:#ff79c6>=</span> z<span style=color:#ff79c6>.</span>get_build_log(project, service, task_id)[<span style=color:#f1fa8c>&#34;data&#34;</span>]
</span></span><span style=display:flex><span>    log <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;服务：</span><span style=color:#f1fa8c>{</span>build[<span style=color:#f1fa8c>&#39;pipeline_name&#39;</span>]<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c> (任务ID：</span><span style=color:#f1fa8c>{</span>tag<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>，耗时排名</span><span style=color:#f1fa8c>{</span>rank<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>)的日志：</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>{</span>log<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    log <span style=color:#ff79c6>=</span> cut_log(log)
</span></span><span style=display:flex><span>    <span style=color:#6272a4># # 按行遍历，如果&#39;#编号&#39;开头的行，只保留相同&#39;#编号&#39;的第一行和最后一行</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># lines = log.split(&#39;\n&#39;)</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># new_lines = []</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># stage_map = {}</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># for line in lines:</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#     match = re.match(r&#39;#(\d+) &#39;, line)</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#     if match:</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#         stage = match.group(1)</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#         if line.startswith(f&#34;{stage} ...&#34;):</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#             continue</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#         if stage not in stage_map:</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#             stage_map[stage] = 1</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#             new_lines.append(line)</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#         elif stage_map[stage] &gt;= 1:</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#             if len(lines) &lt;= lines.index(line) + 1 or not lines[lines.index(line) + 1].startswith(f&#39;#{stage} &#39;):</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#                 new_lines.append(line)</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#                 stage_map[stage] = 2</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#         continue</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#     else:</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>#         new_lines.append(line)</span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># log = &#39;\n&#39;.join(new_lines)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> rank <span style=color:#ff79c6>&gt;=</span> <span style=color:#8be9fd;font-style:italic>len</span>(builds):
</span></span><span style=display:flex><span>        log <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;```</span><span style=color:#f1fa8c>{</span>log<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>```</span><span style=color:#f1fa8c>\n\n</span><span style=color:#f1fa8c>这是最后一条构建日志了，请帮我分析构建慢的原因&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>        log <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;```</span><span style=color:#f1fa8c>{</span>log<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>```</span><span style=color:#f1fa8c>\n\n</span><span style=color:#f1fa8c>这是第</span><span style=color:#f1fa8c>{</span>rank<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>个日志内容，请继续给我第</span><span style=color:#f1fa8c>{</span>rank<span style=color:#ff79c6>+</span><span style=color:#bd93f9>1</span><span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>个日志的获取方法&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> log
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    log_init(<span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(get_build_log(<span style=color:#bd93f9>1</span>))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># notify.py: 调用外部服务用于发送邮件</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> requests
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> log <span style=color:#ff79c6>import</span> XXX_TOKEN
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>Notify</span>:
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> __init__(self):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 外部发邮件服务域名</span>
</span></span><span style=display:flex><span>        self<span style=color:#ff79c6>.</span>base_url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;https://xxx.com/api/v1/message&#34;</span>
</span></span><span style=display:flex><span>        self<span style=color:#ff79c6>.</span>headers <span style=color:#ff79c6>=</span> {
</span></span><span style=display:flex><span>            <span style=color:#f1fa8c>&#34;Authorization&#34;</span>: <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Bearer </span><span style=color:#f1fa8c>{</span>XXX_TOKEN<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>email</span>(self, body):
</span></span><span style=display:flex><span>        url <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>{</span>self<span style=color:#ff79c6>.</span>base_url<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>/smtp&#34;</span>
</span></span><span style=display:flex><span>        requests<span style=color:#ff79c6>.</span>post(url, headers<span style=color:#ff79c6>=</span>self<span style=color:#ff79c6>.</span>headers, json<span style=color:#ff79c6>=</span>body)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>send</span>(self, recipient, subject, message, method<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;email&#34;</span>):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> method <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;email&#34;</span>:
</span></span><span style=display:flex><span>            self<span style=color:#ff79c6>.</span>email({
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;usernames&#34;</span>: recipient,
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;subject&#34;</span>: subject,
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;type&#34;</span>: <span style=color:#f1fa8c>&#34;text/html&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#f1fa8c>&#34;body&#34;</span>: message
</span></span><span style=display:flex><span>            })
</span></span></code></pre></div><h2 id=token计算>Token计算</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> time
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_openai <span style=color:#ff79c6>import</span> ChatOpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> langchain_community.callbacks <span style=color:#ff79c6>import</span> get_openai_callback
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>IS_OLLAMA <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 初始化OpenAI LLM</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> IS_OLLAMA:
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;https://openllm.xxx.com/v1&#34;</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;ollama&#34;</span>,
</span></span><span style=display:flex><span>        model<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;qwen:7b&#34;</span>,
</span></span><span style=display:flex><span>        max_tokens<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2000</span>,
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span><span style=color:#ff79c6>else</span>:
</span></span><span style=display:flex><span>    llm <span style=color:#ff79c6>=</span> ChatOpenAI(
</span></span><span style=display:flex><span>        base_url<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;http://xxx:9000/v1&#34;</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;ollama&#34;</span>,
</span></span><span style=display:flex><span>        model<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;Qwen1.5-7B-Chat&#34;</span>,
</span></span><span style=display:flex><span>        max_tokens<span style=color:#ff79c6>=</span><span style=color:#bd93f9>2000</span>,
</span></span><span style=display:flex><span>        temperature<span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 启动回调函数</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>with</span> get_openai_callback() <span style=color:#ff79c6>as</span> cb:
</span></span><span style=display:flex><span>    start_time <span style=color:#ff79c6>=</span> time<span style=color:#ff79c6>.</span>time()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 生成一些文本</span>
</span></span><span style=display:flex><span>    output <span style=color:#ff79c6>=</span> llm<span style=color:#ff79c6>.</span>invoke(<span style=color:#f1fa8c>&#34;作为一个旅游资深博主，请写一篇关于巴厘岛的文章，不少于2000字。&#34;</span>)
</span></span><span style=display:flex><span>    end_time <span style=color:#ff79c6>=</span> time<span style=color:#ff79c6>.</span>time()
</span></span><span style=display:flex><span>    elapsed_time <span style=color:#ff79c6>=</span> end_time <span style=color:#ff79c6>-</span> start_time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># 获取生成的token数量</span>
</span></span><span style=display:flex><span>    total_tokens <span style=color:#ff79c6>=</span> cb<span style=color:#ff79c6>.</span>total_tokens
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4># 计算token生成速率</span>
</span></span><span style=display:flex><span>token_rate <span style=color:#ff79c6>=</span> total_tokens <span style=color:#ff79c6>/</span> elapsed_time
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;生成了 </span><span style=color:#f1fa8c>{</span>total_tokens<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c> 个token, 用时 </span><span style=color:#f1fa8c>{</span>elapsed_time<span style=color:#f1fa8c>:</span><span style=color:#f1fa8c>.2f</span><span style=color:#f1fa8c>}</span><span style=color:#f1fa8c> 秒&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Token生成速率为 </span><span style=color:#f1fa8c>{</span>token_rate<span style=color:#f1fa8c>:</span><span style=color:#f1fa8c>.2f</span><span style=color:#f1fa8c>}</span><span style=color:#f1fa8c> tokens/秒&#34;</span>)
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>生成了 286 个token, 用时 8.75 秒
Token生成速率为 32.70 tokens/秒
</code></pre><h1 id=向量数据库>向量数据库</h1><h2 id=milvus>Milvus</h2><h3 id=简介-2>简介</h3><p>Milvus是一款云原生向量数据库，于2019年开源，具备高可用、高性能、易扩展的特点，用于海量向量数据的实时召回。它基于FAISS、ANNOY、HNSW等向量搜索库构建，核心是解决稠密向量相似度检索的问题。在向量检索库的基础上，Milvus支持数据分区分片、数据持久化、增量数据摄取、标量向量混合查询、TimeTravel等功能，同时大幅优化了向量检索的性能，可满足任何向量检索场景的应用需求。通常，建议用户使用Kubernetes部署Milvus，以获得最佳可用性和弹性。</p><ul><li>Milvus部署：<a href=https://milvus.io/docs/install-overview.md>https://milvus.io/docs/install-overview.md</a></li><li>Milvus训练营：<a href=https://github.com/milvus-io/bootcamp>https://github.com/milvus-io/bootcamp</a></li><li>向量搜索的相似性度量指标：<a href=https://zilliz.com/blog/similarity-metrics-for-vector-search>https://zilliz.com/blog/similarity-metrics-for-vector-search</a></li></ul><h3 id=attu>Attu</h3><p>Attu是一个用于Milvus的图形化管理工具，Attu中的功能模块包括 Overview（概览）、Collection（集合）、Vector Search（向量搜索）和 System View（系统视图）等。</p><p>docker部署方式</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run -p 8000:3000 -e <span style=color:#8be9fd;font-style:italic>HOST_URL</span><span style=color:#ff79c6>=</span>http://<span style=color:#ff79c6>{</span> your machine IP <span style=color:#ff79c6>}</span>:8000 -e <span style=color:#8be9fd;font-style:italic>MILVUS_URL</span><span style=color:#ff79c6>={</span>your machine IP<span style=color:#ff79c6>}</span>:19530 zilliz/attu:latest
</span></span></code></pre></div><h3 id=milvus文本检索>Milvus文本检索</h3><p>采用milvus+embedding模型+reranker模型检索文本的示例代码，分为两个代码片段：插入代码和查询代码</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 插入代码d.py</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> openai <span style=color:#ff79c6>import</span> OpenAI
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> pymilvus <span style=color:#ff79c6>import</span> MilvusClient
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>EMBEDDING_MODEL_NAME <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;bge-large-zh-v1.5&#34;</span>  <span style=color:#6272a4># Which model to use, please check https://platform.openai.com/docs/guides/embeddings for available models</span>
</span></span><span style=display:flex><span>RERANKER_MODEL_NAME <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;bge-reranker-large&#34;</span>
</span></span><span style=display:flex><span>DIMENSION <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1024</span>  <span style=color:#6272a4># Dimension of vector embedding</span>
</span></span><span style=display:flex><span>BASE_URL <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;http://&lt;MODEL-IP&gt;:9997&#34;</span>
</span></span><span style=display:flex><span>API_KEY <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;sk-xxx&#34;</span>
</span></span><span style=display:flex><span>MILVUS_URI <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;http://&lt;MILVUS-IP&gt;:19530&#34;</span>
</span></span><span style=display:flex><span>MILVUS_DB_NAME <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;default&#34;</span>
</span></span><span style=display:flex><span>COLLECTION_NAME <span style=color:#ff79c6>=</span> <span style=color:#f1fa8c>&#34;demo_collection&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>openai_client <span style=color:#ff79c6>=</span> OpenAI(api_key<span style=color:#ff79c6>=</span>API_KEY, base_url<span style=color:#ff79c6>=</span>BASE_URL <span style=color:#ff79c6>+</span> <span style=color:#f1fa8c>&#34;/v1&#34;</span>)
</span></span><span style=display:flex><span>milvus_client <span style=color:#ff79c6>=</span> MilvusClient(uri<span style=color:#ff79c6>=</span>MILVUS_URI, db_name<span style=color:#ff79c6>=</span>MILVUS_DB_NAME)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>create_embeddings</span>(client, documents):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>try</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> [
</span></span><span style=display:flex><span>            vec<span style=color:#ff79c6>.</span>embedding <span style=color:#ff79c6>for</span> vec <span style=color:#ff79c6>in</span> client<span style=color:#ff79c6>.</span>embeddings<span style=color:#ff79c6>.</span>create(
</span></span><span style=display:flex><span>                <span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span>documents,
</span></span><span style=display:flex><span>                model<span style=color:#ff79c6>=</span>EMBEDDING_MODEL_NAME)<span style=color:#ff79c6>.</span>data
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>except</span> Exception <span style=color:#ff79c6>as</span> e:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Error creating embeddings: </span><span style=color:#f1fa8c>{</span>e<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>insert_into_milvus</span>(milvus_client, collection_name, data):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>try</span>:
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>if</span> milvus_client<span style=color:#ff79c6>.</span>has_collection(collection_name<span style=color:#ff79c6>=</span>collection_name):
</span></span><span style=display:flex><span>            milvus_client<span style=color:#ff79c6>.</span>drop_collection(collection_name<span style=color:#ff79c6>=</span>collection_name)
</span></span><span style=display:flex><span>        milvus_client<span style=color:#ff79c6>.</span>create_collection(
</span></span><span style=display:flex><span>            collection_name<span style=color:#ff79c6>=</span>collection_name,
</span></span><span style=display:flex><span>            dimension<span style=color:#ff79c6>=</span>DIMENSION,
</span></span><span style=display:flex><span>            consistency_level<span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#34;Strong&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> milvus_client<span style=color:#ff79c6>.</span>insert(collection_name<span style=color:#ff79c6>=</span>collection_name, data<span style=color:#ff79c6>=</span>data)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>except</span> Exception <span style=color:#ff79c6>as</span> e:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Error inserting data into Milvus: </span><span style=color:#f1fa8c>{</span>e<span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#ff79c6>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    docs <span style=color:#ff79c6>=</span> [
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;查询考勤的工具，只能用于查询个人考勤记录。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;待办管理的工具，只能用于创建或查询待办列表。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;查询内存使用率的工具，只能用查询容器集群。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;采购邮件格式优化的工具，只能用于优化采购内容的邮件格式。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;申请网络代理的工具，只能用于http代理上网。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;申请虚拟机的工具，只能用于申请虚拟机资源。&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;申请应用虚拟机的工具，只能用于申请dify虚拟机资源&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#f1fa8c>&#34;查询cmdb的工具，只能用于查询跟cmdb相关的数据&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#6272a4># &#34;默认通用的工具，适合一切。&#34;</span>
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    vectors <span style=color:#ff79c6>=</span> create_embeddings(openai_client, docs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4># We can store the id, vector representation, raw text and labels such as &#34;subject&#34; in this case in Milvus.</span>
</span></span><span style=display:flex><span>    data <span style=color:#ff79c6>=</span> [
</span></span><span style=display:flex><span>        {<span style=color:#f1fa8c>&#34;id&#34;</span>: i, <span style=color:#f1fa8c>&#34;vector&#34;</span>: vectors[i], <span style=color:#f1fa8c>&#34;text&#34;</span>: docs[i], <span style=color:#f1fa8c>&#34;subject&#34;</span>: <span style=color:#f1fa8c>&#34;大模型工具&#34;</span>}
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#8be9fd;font-style:italic>len</span>(docs))
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    res <span style=color:#ff79c6>=</span> insert_into_milvus(milvus_client, COLLECTION_NAME, data)
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(res)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>往milvus向量数据库的demo_collection集合插入了8条数据</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 查询代码e.py</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> xinference.client <span style=color:#ff79c6>import</span> Client <span style=color:#ff79c6>as</span> xClient
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> d <span style=color:#ff79c6>import</span> EMBEDDING_MODEL_NAME, RERANKER_MODEL_NAME, \
</span></span><span style=display:flex><span>    openai_client, milvus_client, COLLECTION_NAME, BASE_URL
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>main</span>():
</span></span><span style=display:flex><span>    queries <span style=color:#ff79c6>=</span> [<span style=color:#f1fa8c>&#34;申请代理&#34;</span>, <span style=color:#f1fa8c>&#34;申请虚机&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    query_vectors <span style=color:#ff79c6>=</span> [
</span></span><span style=display:flex><span>        vec<span style=color:#ff79c6>.</span>embedding
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> vec <span style=color:#ff79c6>in</span> openai_client<span style=color:#ff79c6>.</span>embeddings<span style=color:#ff79c6>.</span>create(
</span></span><span style=display:flex><span>            <span style=color:#8be9fd;font-style:italic>input</span><span style=color:#ff79c6>=</span>queries, model<span style=color:#ff79c6>=</span>EMBEDDING_MODEL_NAME)<span style=color:#ff79c6>.</span>data
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    origin_result <span style=color:#ff79c6>=</span> milvus_client<span style=color:#ff79c6>.</span>search(
</span></span><span style=display:flex><span>        collection_name<span style=color:#ff79c6>=</span>COLLECTION_NAME,  <span style=color:#6272a4># target collection</span>
</span></span><span style=display:flex><span>        data<span style=color:#ff79c6>=</span>query_vectors,  <span style=color:#6272a4># query vectors</span>
</span></span><span style=display:flex><span>        limit<span style=color:#ff79c6>=</span><span style=color:#bd93f9>10</span>,  <span style=color:#6272a4># number of returned entities</span>
</span></span><span style=display:flex><span>        output_fields<span style=color:#ff79c6>=</span>[<span style=color:#f1fa8c>&#34;text&#34;</span>, <span style=color:#f1fa8c>&#34;subject&#34;</span>],  <span style=color:#6272a4># specifies fields to be returned</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    xclient <span style=color:#ff79c6>=</span> xClient(BASE_URL)
</span></span><span style=display:flex><span>    rerank_model <span style=color:#ff79c6>=</span> xclient<span style=color:#ff79c6>.</span>get_model(RERANKER_MODEL_NAME)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    results <span style=color:#ff79c6>=</span> rerank_model<span style=color:#ff79c6>.</span>rerank(
</span></span><span style=display:flex><span>            [doc[<span style=color:#f1fa8c>&#34;entity&#34;</span>][<span style=color:#f1fa8c>&#34;text&#34;</span>] <span style=color:#ff79c6>for</span> doc <span style=color:#ff79c6>in</span> origin_result[<span style=color:#bd93f9>0</span>]],
</span></span><span style=display:flex><span>            queries[<span style=color:#bd93f9>0</span>],
</span></span><span style=display:flex><span>            return_documents<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>,
</span></span><span style=display:flex><span>            top_n<span style=color:#ff79c6>=</span><span style=color:#bd93f9>3</span>,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> q <span style=color:#ff79c6>in</span> queries:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;Query:&#34;</span>, q)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> result <span style=color:#ff79c6>in</span> results[<span style=color:#f1fa8c>&#39;results&#39;</span>]:
</span></span><span style=display:flex><span>            <span style=color:#8be9fd;font-style:italic>print</span>(result)
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>&#34;</span><span style=color:#f1fa8c>\n</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>利用向量数据库相似检索出最相关的数据后，再利用rerank模型进行排序，最终返回最相关的数据</p><h1 id=大模型运行的gpu显存计算公式>大模型运行的GPU显存计算公式</h1><pre tabindex=0><code>M = (P*4B) / (32/Q) * 1.2
</code></pre><table><thead><tr><th>Symbol</th><th>Description</th></tr></thead><tbody><tr><td>M</td><td>GPU memory expressed in Gigabyte</td></tr><tr><td>P</td><td>The amount of parameters in the model. E.g. a 7B model has 7 billion parameters.</td></tr><tr><td>4B</td><td>4 bytes, expressing the bytes used for each parameter</td></tr><tr><td>32</td><td>There are 32 bits in 4 bytes</td></tr><tr><td>Q</td><td>The amount of bits that should be used for loading the model. E.g. 16 bits, 8 bits or 4 bits.</td></tr><tr><td>1.2</td><td>Represents a 20% overhead of loading additional things in GPU memory.</td></tr></tbody></table><p>示例1：运行llama 70B 16bit的大模型需要多少显存?</p><pre tabindex=0><code>M = (70B*4B) / (32/16) * 1.2 = 168GB
</code></pre><p>根据计算结果，需要至少2张A100(80GB显存)</p><p>示例2：运行llama 70B 4bit量化的大模型需要多少显存?</p><pre tabindex=0><code>M = (70B*4B) / (32/4) * 1.2 = 42GB
</code></pre><p>根据计算结果，需要2张L4就可以运行(24GB显存)</p><h1 id=参考链接>参考链接</h1><ul><li><a href=https://datawhalechina.github.io/prompt-engineering-for-developers/#/README>面向开发者的LLM入门教程</a></li><li><a href=https://github.com/xusenlinzy/api-for-open-llm/blob/master/docs/SCRIPT.md>https://github.com/xusenlinzy/api-for-open-llm/blob/master/docs/SCRIPT.md</a></li><li><a href=https://zhuanlan.zhihu.com/p/641999400>https://zhuanlan.zhihu.com/p/641999400</a></li><li><a href=https://zhuanlan.zhihu.com/p/111235300>NVidia Docker介绍</a></li><li><a href=https://python.langchain.com/docs/get_started/introduction>https://python.langchain.com/docs/get_started/introduction</a></li><li><a href=https://www.chenshaowen.com/blog/llama-cpp-that-is-a-llm-deployment-tool.html>llama.cpp部署实践</a></li><li><a href=https://new.qq.com/rain/a/20240221A06BMB00>大模型在研发效率提升方面的应用与实践</a></li><li><a href=https://opencompass.org.cn/doc>OpenCompass教程</a></li></ul><div class="entry-shang text-center"><p>「真诚赞赏，手留余香」</p><button class="zs show-zs btn btn-bred">赞赏支持</button></div><div class=zs-modal-bg></div><div class=zs-modal-box><div class=zs-modal-head><button type=button class=close>×</button>
<span class=author><a href=https://www.iceyao.com.cn/><img src=/img/favicon.png>爱折腾的工程师</a></span><p class=tip><i></i><span>真诚赞赏，手留余香</span></p></div><div class=zs-modal-body><div class=zs-modal-btns><button class="btn btn-blink" data-num=2>2元</button>
<button class="btn btn-blink" data-num=5>5元</button>
<button class="btn btn-blink" data-num=10>10元</button>
<button class="btn btn-blink" data-num=50>50元</button>
<button class="btn btn-blink" data-num=100>100元</button>
<button class="btn btn-blink" data-num=1>任意金额</button></div><div class=zs-modal-pay><button class="btn btn-bred" id=pay-text>2元</button><p>使用<span id=pay-type>微信</span>扫描二维码完成支付</p><img src=/img/reward/wechat-2.png id=pay-image></div></div><div class=zs-modal-footer><label><input type=radio name=zs-type value=wechat class=zs-type checked><span><span class=zs-wechat><img src=/img/reward/wechat-btn.png></span></label>
<label><input type=radio name=zs-type value=alipay class=zs-type class=zs-alipay><img src=/img/reward/alipay-btn.png></span></label></div></div><script type=text/javascript src=/js/reward.js></script><hr><ul class=pager><li class=previous><a href=/2023/07/19/gitlab-ee-readnotes/ data-toggle=tooltip data-placement=top title=Gitlab极狐版体验笔记>&larr;
Previous Post</a></li><li class=next><a href=/2024/03/21/python-knowledge/ data-toggle=tooltip data-placement=top title=Python知识点大杂烩>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=yaoice/yaoice.github.io data-repo-id=R_kgDOJnxqVg data-category=General data-category-id=DIC_kwDOJnxqVs4CWwUs data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=en crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/devops title=devops>devops
</a><a href=/tags/go title=go>go
</a><a href=/tags/k8s title=k8s>k8s
</a><a href=/tags/llm title=llm>llm
</a><a href=/tags/openstack title=openstack>openstack
</a><a href=/tags/tkestack title=tkestack>tkestack
</a><a href=/tags/%E7%BB%83%E8%BD%A6 title=练车>练车</a></div></section></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:yao3690093@gmail.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat.jpeg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/yaoice><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title=爱折腾的工程师><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; 爱折腾的工程师 2024</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script>(function(){var t,e=document.createElement("script"),n=window.location.protocol.split(":")[0];n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script><script>var _baId="92c175994ded75a3cd2074bc1123e2be",_hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="//hm.baidu.com/hm.js?"+_baId,e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){t=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),n=$(this).text(),i=$('<a href="'+o+'" rel="nofollow">'+n+"</a>"),s=$('<li class="'+t+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script></body></html>