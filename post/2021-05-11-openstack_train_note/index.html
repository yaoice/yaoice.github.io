<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <meta property="og:site_name" content="爱折腾的工程师">
    <meta property="og:type" content="article">

    
    <meta property="og:image" content="https://www.iceyao.com.cn/img/post-bg-unix-linux.jpg">
    <meta property="twitter:image" content="https://www.iceyao.com.cn/img/post-bg-unix-linux.jpg" />
    

    
    <meta name="title" content="OpenStack Train实践（纯操作）" />
    <meta property="og:title" content="OpenStack Train实践（纯操作）" />
    <meta property="twitter:title" content="OpenStack Train实践（纯操作）" />
    

    
    <meta name="description" content="iceyao，程序员, 开源爱好者，生活探险家 | 这里是iceyao的博客，与你一起发现更大的世界。">
    <meta property="og:description" content="iceyao，程序员, 开源爱好者，生活探险家 | 这里是iceyao的博客，与你一起发现更大的世界。" />
    <meta property="twitter:description" content="iceyao，程序员, 开源爱好者，生活探险家 | 这里是iceyao的博客，与你一起发现更大的世界。" />
    

    
    <meta property="twitter:card" content="summary" />
    
    

    <meta name="keyword"  content="iceyao, IceYao&#39;s Blog, 博客, 个人网站, 互联网, Web, 云原生, PaaS, Istio, Kubernetes, 微服务, Microservice">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>OpenStack Train实践（纯操作） | 爱折腾的工程师 | IceYao&#39;s Blog</title>

    <link rel="canonical" href="/post/2021-05-11-openstack_train_note/">

    
    
    
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    
    <link rel="stylesheet" href="/css/hugo-theme-cleanwhite.min.css">

    
    <link rel="stylesheet" href="/css/zanshang.css">

    
    <link rel="stylesheet" href="/css/font-awesome.all.min.css">

    
    

    
    <script src="/js/jquery.min.js"></script>

    
    <script src="/js/bootstrap.min.js"></script>

    
    <script src="/js/hux-blog.min.js"></script>

    
    <script src="/js/lazysizes.min.js"></script>

    
    

</head>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-9J7CKFVPPM"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-9J7CKFVPPM', { 'anonymize_ip': false });
}
</script>






<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
        
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">爱折腾的工程师</a>
        </div>

        
        
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">All Posts</a>
                    </li>
                    
                        
                    
                    
		    
                        <li><a href="/archive//">ARCHIVE</a></li>
                    
                        <li><a href="/notes//">NOTES</a></li>
                    
                        <li><a href="/about//">ABOUT</a></li>
                    
		            <li>
                        <a href="/search"><i class="fa fa-search"></i></a>
		           </li>
                </ul>
            </div>
        </div>
        
    </div>
    
</nav>
<script>
    
    
    
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        
            $navbar.className = " ";
            
            setTimeout(function(){
                
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>




<style type="text/css">
    header.intro-header {
        background-image: url('/img/post-bg-unix-linux.jpg')
    }
</style>

<header class="intro-header" >

    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/openstack" title="OpenStack">
                            OpenStack
                        </a>
                        
                    </div>
                    <h1>OpenStack Train实践（纯操作）</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        
                            Posted by 
                            
                                    爱折腾的工程师
                             
                            on 
                            Tuesday, May 11, 2021
                            
                            
                            
                            
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>




<article>
    <div class="container">
        <div class="row">

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                
                <h2 id="1-环境">1. 环境</h2>
<ul>
<li>
<p>系统：CentOS 7.8</p>
</li>
<li>
<p>kernel: 3.10.0-1127.el7.x86_64</p>
</li>
<li>
<p>OpenStack: Train版</p>
</li>
<li>
<p>Controller Node（172.16.88.245）: 48 processor, 376 GB memory</p>
</li>
<li>
<p>Compute Node（172.16.88.246）: 48 processor, 376 GB memory</p>
</li>
</ul>
<h2 id="2-openstack">2. OpenStack</h2>
<h3 id="21-openstack架构">2.1 OpenStack架构</h3>
<p>概念架构：

  <img src="https://docs.openstack.org/install-guide/_images/openstack_kilo_conceptual_arch.png" alt="">

</p>
<p>逻辑架构：

  <img src="https://docs.openstack.org/install-guide/_images/openstack-arch-kilo-logical-v1.png" alt="">

</p>
<h3 id="22-controller-node">2.2 Controller Node</h3>
<h4 id="221-静态域名解析">2.2.1 静态域名解析</h4>
<pre tabindex="0"><code># vim /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

172.16.88.245 controller1
172.16.88.246 computer1
</code></pre><h4 id="222-ntp">2.2.2 NTP</h4>
<p>安装chrony</p>
<pre tabindex="0"><code># yum install -y chrony
</code></pre><p>编辑chrony配置</p>
<pre tabindex="0"><code># vim /etc/chrony.conf
# Allow NTP client access from local network.
allow 172.16.0.0/16
</code></pre><p>启动chrony</p>
<pre tabindex="0"><code># systemctl enable chronyd.service
# systemctl restart chronyd.service
</code></pre><p>查看NTP同步情况</p>
<pre tabindex="0"><code># chronyc sources
210 Number of sources = 4
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^- undefined.hostname.local&gt;     2   7   377    66  +3915us[+3601us] +/-  108ms
^- ntp6.flashdance.cx            2   6   376   324   +835us[ +754us] +/-  155ms
^- ntp.xtom.nl                   2   7    17   121  -3981us[-4273us] +/-   96ms
^* 119.28.206.193                2   7   373     2   -715us[-1030us] +/-   18ms
</code></pre><h4 id="223-yum源">2.2.3 Yum源</h4>
<p>官方提示说明，CentOS 7最高支持到OpenStack Train版本，U版、V版得使用CentOS 8</p>
<pre tabindex="0"><code># yum install -y centos-release-openstack-train
</code></pre><h4 id="224-openstackclient">2.2.4 OpenStackClient</h4>
<pre tabindex="0"><code># yum install -y python-openstackclient
</code></pre><h4 id="225-mariadb">2.2.5 Mariadb</h4>
<p>安装mariadb包</p>
<pre tabindex="0"><code># yum install mariadb mariadb-server python2-PyMySQL
</code></pre><p>编辑mariadb配置</p>
<pre tabindex="0"><code># vim /etc/my.cnf.d/openstack.cnf
[mysqld]
bind-address = 10.0.0.11

default-storage-engine = innodb
innodb_file_per_table = on
max_connections = 10000
collation-server = utf8_general_ci
character-set-server = utf8

key_buffer_size = '64M'
max_heap_table_size = '64M'
tmp_table_size = '64M'
innodb_buffer_pool_size = '8192M'
</code></pre><p>修改mariadb systemd启动脚本，因为mariadb有默认打开文件数限制</p>
<pre tabindex="0"><code>[root@controller1 data(keystone_admin)]# vim /usr/lib/systemd/system/mariadb.service
[Service]
...
LimitNOFILE=10000
LimitNPROC=10000
</code></pre><p>启动mariadb</p>
<pre tabindex="0"><code># systemctl daemon-reload
# systemctl enable mariadb.service
# systemctl start mariadb.service
</code></pre><p>查看mariadb连接数是否生效(还是没到10000)</p>
<pre tabindex="0"><code>[root@controller1 data(keystone_admin)]# mysql -uroot -popenstack -e &quot;show variables like '%connections%';&quot;
+-----------------------+-------+
| Variable_name         | Value |
+-----------------------+-------+
| extra_max_connections | 1     |
| max_connections       | 9570  |
| max_user_connections  | 0     |
+-----------------------+-------+
</code></pre><p>初始化mariadb，按照提示，输入合适的root密码(这里root密码为openstack)</p>
<pre tabindex="0"><code># mysql_secure_installation

NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB
      SERVERS IN PRODUCTION USE!  PLEASE READ EACH STEP CAREFULLY!

In order to log into MariaDB to secure it, we'll need the current
password for the root user.  If you've just installed MariaDB, and
you haven't set the root password yet, the password will be blank,
so you should just press enter here.

Enter current password for root (enter for none):
OK, successfully used password, moving on...

Setting the root password ensures that nobody can log into the MariaDB
root user without the proper authorisation.

Set root password? [Y/n] y
New password:
Re-enter new password:
Password updated successfully!
Reloading privilege tables..
 ... Success!


By default, a MariaDB installation has an anonymous user, allowing anyone
to log into MariaDB without having to have a user account created for
them.  This is intended only for testing, and to make the installation
go a bit smoother.  You should remove them before moving into a
production environment.

Remove anonymous users? [Y/n] y
 ... Success!

Normally, root should only be allowed to connect from 'localhost'.  This
ensures that someone cannot guess at the root password from the network.

Disallow root login remotely? [Y/n] n
 ... skipping.

By default, MariaDB comes with a database named 'test' that anyone can
access.  This is also intended only for testing, and should be removed
before moving into a production environment.

Remove test database and access to it? [Y/n] y
 - Dropping test database...
 ... Success!
 - Removing privileges on test database...
 ... Success!

Reloading the privilege tables will ensure that all changes made so far
will take effect immediately.

Reload privilege tables now? [Y/n] y
 ... Success!

Cleaning up...

All done!  If you've completed all of the above steps, your MariaDB
installation should now be secure.

Thanks for using MariaDB!
</code></pre><h4 id="226-message-queue">2.2.6 Message queue</h4>
<p>安装rabbitmq</p>
<pre tabindex="0"><code># yum install -y rabbitmq-server
</code></pre><p>启动rabbitmq服务</p>
<pre tabindex="0"><code># systemctl enable rabbitmq-server.service
# systemctl start rabbitmq-server.service
</code></pre><p>创建rabbitmq OpenStack授权用户, 并设置权限</p>
<pre tabindex="0"><code># rabbitmqctl add_user openstack openstack
# rabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;
</code></pre><h4 id="227-memcached">2.2.7 Memcached</h4>
<p>安装memcached包</p>
<pre tabindex="0"><code># yum install -y memcached python-memcached
</code></pre><p>修改memcached配置</p>
<pre tabindex="0"><code># vim /etc/sysconfig/memcached
MAXCONN=&quot;4096&quot;
CACHESIZE=&quot;512&quot;
OPTIONS=&quot;-l 127.0.0.1,::1,controller1&quot;
</code></pre><p>启动memcached</p>
<pre tabindex="0"><code># systemctl enable memcached.service
# systemctl start memcached.service
</code></pre><h4 id="228-etcd">2.2.8 Etcd</h4>
<p>安装etcd包</p>
<pre tabindex="0"><code># yum install etcd -y
</code></pre><p>修改etcd配置</p>
<pre tabindex="0"><code># vim  /etc/etcd/etcd.conf
#[Member]
ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;
ETCD_LISTEN_PEER_URLS=&quot;http://172.16.88.245:2380&quot;
ETCD_LISTEN_CLIENT_URLS=&quot;http://172.16.88.245:2379&quot;
ETCD_NAME=&quot;controller&quot;
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;http://172.16.88.245:2380&quot;
ETCD_ADVERTISE_CLIENT_URLS=&quot;http://172.16.88.245:2379&quot;
ETCD_INITIAL_CLUSTER=&quot;controller=http://172.16.88.245:2380&quot;
ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster-01&quot;
ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;
</code></pre><p>启动etcd</p>
<pre tabindex="0"><code># systemctl enable etcd
# systemctl start etcd
</code></pre><h4 id="229-keystone">2.2.9 KeyStone</h4>
<p>初始化keystone数据库</p>
<pre tabindex="0"><code># mysql -u root -popenstack
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 18
Server version: 10.3.20-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE keystone;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \
    -&gt; IDENTIFIED BY 'keystone';
Query OK, 0 rows affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \
    -&gt; IDENTIFIED BY 'keystone';
Query OK, 0 rows affected (0.000 sec)
</code></pre><p>安装keystone包</p>
<pre tabindex="0"><code># yum install openstack-keystone httpd mod_wsgi -y
</code></pre><p>修改keystone配置</p>
<pre tabindex="0"><code># vim /etc/keystone/keystone.conf
[database]
# ...
connection = mysql+pymysql://keystone:keystone@controller1/keystone
connection_recycle_time = 10
max_overflow = 1000
max_pool_size = 1
max_retries = -1

[cache]
backend = oslo_cache.memcache_pool
enabled = True
memcache_servers = controller1:11211

[token]
# ...
provider = fernet

[identity]
max_password_length = 128
password_hash_algorithm = pbkdf2_sha512
password_hash_rounds = 1
</code></pre><p>同步keystone数据库表</p>
<pre tabindex="0"><code># /bin/sh -c &quot;keystone-manage db_sync&quot; keystone
</code></pre><p>初始化fernet</p>
<pre tabindex="0"><code># keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
# keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
</code></pre><p>引导identity服务，设置admin密码</p>
<pre tabindex="0"><code># keystone-manage bootstrap --bootstrap-password admin \
  --bootstrap-admin-url http://controller1:5000/v3/ \
  --bootstrap-internal-url http://controller1:5000/v3/ \
  --bootstrap-public-url http://controller1:5000/v3/ \
  --bootstrap-region-id RegionOne
</code></pre><p>配置httpd代理keystone</p>
<pre tabindex="0"><code># vim /etc/httpd/conf/httpd.conf
ServerName controller1
</code></pre><p>创建httpd keystone wsgi配置文件</p>
<pre tabindex="0"><code># ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
</code></pre><p>启动httpd服务</p>
<pre tabindex="0"><code># systemctl enable httpd.service
# systemctl start httpd.service
</code></pre><p>验证keystone服务</p>
<pre tabindex="0"><code># vim keystonerc
export OS_USERNAME=admin
export OS_PASSWORD=admin
export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://controller1:5000/v3
export OS_IDENTITY_API_VERSION=3
export PS1='[\u@\h \W(keystone_admin)]\$ '
</code></pre><pre tabindex="0"><code># source keystonerc
# openstack endpoint list
+----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+
| ID                               | Region    | Service Name | Service Type | Enabled | Interface | URL                         |
+----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+
| 9e1766bdac0e4fdd9678eaf2d06a0046 | RegionOne | keystone     | identity     | True    | admin     | http://controller1:5000/v3/ |
| c17a4cb493944ca1a62aa2d632905072 | RegionOne | keystone     | identity     | True    | public    | http://controller1:5000/v3/ |
| f60ba6d5ebee4db2927cb2ec52eb8209 | RegionOne | keystone     | identity     | True    | internal  | http://controller1:5000/v3/ |
+----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+
# openstack token issue
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field      | Value                                                                                                                                                                                   |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| expires    | 2021-05-11T09:29:19+0000                                                                                                                                                                |
| id         | gAAAAABgmkBf1XpUG3MinL7rwTVIz6SEtJf-AsWYsIBpdnoCFb8cLjBjhQAHc09Hqnh7d9GK-t_Igl-XCsctKi8GRrgGm2N4Vebx9VwvRk_u9glioKOa0ZW7z0nLRjy5BoMVhUx8MHQGb7fqUcXsr8wyVkhzMhlWzRFyuaMvFk2Z9n0y1UfFMPg |
| project_id | 6f8b202e14144ed3bb1a414d308bdfd9                                                                                                                                                        |
| user_id    | 09e15386565341c8bdc89ca0555ab756                                                                                                                                                        |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
</code></pre><p>创建service project</p>
<pre tabindex="0"><code># openstack project create --domain default --description &quot;Service Project&quot; service
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Service Project                  |
| domain_id   | default                          |
| enabled     | True                             |
| id          | a7667c721672424398b29ea0ef3a7c37 |
| is_domain   | False                            |
| name        | service                          |
| options     | {}                               |
| parent_id   | default                          |
| tags        | []                               |
+-------------+----------------------------------+
</code></pre><h4 id="2210-glance">2.2.10 Glance</h4>
<p>初始化glance数据库</p>
<pre tabindex="0"><code># mysql -u root -popenstack
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 30
Server version: 10.3.20-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE glance;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \
    -&gt;   IDENTIFIED BY 'glance';
Query OK, 0 rows affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \
    -&gt;   IDENTIFIED BY 'glance';
Query OK, 0 rows affected (0.000 sec)
</code></pre><p>创建glance认证user</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack user create --domain default --password glance glance
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | default                          |
| enabled             | True                             |
| id                  | 633502b6cc1f4c41a3c61102535933d7 |
| name                | glance                           |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
</code></pre><p>赋予glance用户admin权限，并添加到service项目</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack role add --project service --user glance admin
</code></pre><p>创建glance service</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack service create --name glance --description &quot;OpenStack Image&quot; image
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Image                  |
| enabled     | True                             |
| id          | 591601b6c50f44a99ee879a1cb666480 |
| name        | glance                           |
| type        | image                            |
+-------------+----------------------------------+
</code></pre><p>创建glance endpoint</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack endpoint create --region RegionOne image public http://controller1:9292
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 4b7e8fb9e51b4896a4da31bf807fcb49 |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 591601b6c50f44a99ee879a1cb666480 |
| service_name | glance                           |
| service_type | image                            |
| url          | http://controller1:9292          |
+--------------+----------------------------------+

[root@controller1 ~]# openstack endpoint create --region RegionOne image internal http://controller1:9292
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 6c26f26e246e41bc98deb2e5fe27e020 |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 591601b6c50f44a99ee879a1cb666480 |
| service_name | glance                           |
| service_type | image                            |
| url          | http://controller1:9292          |
+--------------+----------------------------------+

[root@controller1 ~]# openstack endpoint create --region RegionOne image admin http://controller1:9292
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | d862a7872a1f46bbb0e8f7a6ca011d5f |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 591601b6c50f44a99ee879a1cb666480 |
| service_name | glance                           |
| service_type | image                            |
| url          | http://controller1:9292          |
+--------------+----------------------------------+
</code></pre><p>安装glance包</p>
<pre tabindex="0"><code>[root@controller1 ~]# yum install -y openstack-glance
</code></pre><p>编辑glance配置</p>
<pre tabindex="0"><code>[root@controller1 ~]# vim /etc/glance/glance-api.conf
[database]
# ...
connection = mysql+pymysql://glance:glance@controller1/glance
connection_recycle_time = 10
max_overflow = 1000
max_pool_size = 1
max_retries = -1

[keystone_authtoken]
# ...
www_authenticate_uri  = http://controller1:5000
auth_url = http://controller1:5000
memcached_servers = controller1:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = glance
password = glance

[paste_deploy]
# ...
flavor = keystone

[glance_store]
# ...
stores = file,http
default_store = file
filesystem_store_datadir = /var/lib/glance/images/
</code></pre><p>同步glance数据库表</p>
<pre tabindex="0"><code>[root@controller1 ~]# /bin/sh -c &quot;glance-manage db_sync&quot; glance
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
/usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1280, u&quot;Name 'alembic_version_pkc' ignored for PRIMARY key.&quot;)
  result = self._query(query)
INFO  [alembic.runtime.migration] Running upgrade  -&gt; liberty, liberty initial
INFO  [alembic.runtime.migration] Running upgrade liberty -&gt; mitaka01, add index on created_at and updated_at columns of 'images' table
INFO  [alembic.runtime.migration] Running upgrade mitaka01 -&gt; mitaka02, update metadef os_nova_server
INFO  [alembic.runtime.migration] Running upgrade mitaka02 -&gt; ocata_expand01, add visibility to images
INFO  [alembic.runtime.migration] Running upgrade ocata_expand01 -&gt; pike_expand01, empty expand for symmetry with pike_contract01
INFO  [alembic.runtime.migration] Running upgrade pike_expand01 -&gt; queens_expand01
INFO  [alembic.runtime.migration] Running upgrade queens_expand01 -&gt; rocky_expand01, add os_hidden column to images table
INFO  [alembic.runtime.migration] Running upgrade rocky_expand01 -&gt; rocky_expand02, add os_hash_algo and os_hash_value columns to images table
INFO  [alembic.runtime.migration] Running upgrade rocky_expand02 -&gt; train_expand01, empty expand for symmetry with train_contract01
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
Upgraded database to: train_expand01, current revision(s): train_expand01
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
Database migration is up to date. No migration needed.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade mitaka02 -&gt; ocata_contract01, remove is_public from images
INFO  [alembic.runtime.migration] Running upgrade ocata_contract01 -&gt; pike_contract01, drop glare artifacts tables
INFO  [alembic.runtime.migration] Running upgrade pike_contract01 -&gt; queens_contract01
INFO  [alembic.runtime.migration] Running upgrade queens_contract01 -&gt; rocky_contract01
INFO  [alembic.runtime.migration] Running upgrade rocky_contract01 -&gt; rocky_contract02
INFO  [alembic.runtime.migration] Running upgrade rocky_contract02 -&gt; train_contract01
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
Upgraded database to: train_contract01, current revision(s): train_contract01
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
Database is synced successfully.
</code></pre><p>启动glance服务</p>
<pre tabindex="0"><code># chown -R glance:glance /var/log/glance/
# systemctl enable openstack-glance-api.service
# systemctl start openstack-glance-api.service
</code></pre><p>验证glance服务</p>
<pre tabindex="0"><code># openstack image list
</code></pre><p>上传cirros镜像</p>
<pre tabindex="0"><code>[root@controller1 data]# wget -c http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img

[root@controller1 data]# glance image-create --name &quot;cirros&quot; \
&gt;   --file cirros-0.4.0-x86_64-disk.img \
&gt;   --disk-format qcow2 --container-format bare \
&gt;   --visibility public
+------------------+----------------------------------------------------------------------------------+
| Property         | Value                                                                            |
+------------------+----------------------------------------------------------------------------------+
| checksum         | 443b7623e27ecf03dc9e01ee93f67afe                                                 |
| container_format | bare                                                                             |
| created_at       | 2021-05-11T12:11:19Z                                                             |
| disk_format      | qcow2                                                                            |
| id               | 7abfcd41-c55c-4344-ad52-6e80716f9aca                                             |
| min_disk         | 0                                                                                |
| min_ram          | 0                                                                                |
| name             | cirros                                                                           |
| os_hash_algo     | sha512                                                                           |
| os_hash_value    | 6513f21e44aa3da349f248188a44bc304a3653a04122d8fb4535423c8e1d14cd6a153f735bb0982e |
|                  | 2161b5b5186106570c17a9e58b64dd39390617cd5a350f78                                 |
| os_hidden        | False                                                                            |
| owner            | 6f8b202e14144ed3bb1a414d308bdfd9                                                 |
| protected        | False                                                                            |
| size             | 12716032                                                                         |
| status           | active                                                                           |
| tags             | []                                                                               |
| updated_at       | 2021-05-11T12:11:20Z                                                             |
| virtual_size     | Not available                                                                    |
| visibility       | public                                                                           |
+------------------+----------------------------------------------------------------------------------+
</code></pre><h4 id="2211-placement">2.2.11 Placement</h4>
<p>初始化placement数据库</p>
<pre tabindex="0"><code>[root@controller1 data]# mysql -u root -popenstack
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 46
Server version: 10.3.20-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE placement;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'localhost' \
    -&gt;   IDENTIFIED BY 'placement';
Query OK, 0 rows affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'%' \
    -&gt;   IDENTIFIED BY 'placement';
Query OK, 0 rows affected (0.000 sec)
</code></pre><p>创建placement用户</p>
<pre tabindex="0"><code>[root@controller1 data]# openstack user create --domain default --password placement placement
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | default                          |
| enabled             | True                             |
| id                  | 710b09d606d74141886809a2d7ec3d2d |
| name                | placement                        |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
</code></pre><p>添加placement用户到service项目</p>
<pre tabindex="0"><code>[root@controller1 data]# openstack role add --project service --user placement admin
</code></pre><p>创建placement服务</p>
<pre tabindex="0"><code>[root@controller1 data]# openstack service create --name placement \
&gt;   --description &quot;Placement API&quot; placement
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Placement API                    |
| enabled     | True                             |
| id          | 792ab077eebe4178b751c76219242450 |
| name        | placement                        |
| type        | placement                        |
+-------------+----------------------------------+
</code></pre><p>创建placement endpoint</p>
<pre tabindex="0"><code>[root@controller1 data]# openstack endpoint create --region RegionOne placement public http://controller1:8778
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 3479e2d9ec454572ab899710aaf2290f |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 792ab077eebe4178b751c76219242450 |
| service_name | placement                        |
| service_type | placement                        |
| url          | http://controller1:8778          |
+--------------+----------------------------------+
[root@controller1 data]# openstack endpoint create --region RegionOne placement internal http://controller1:8778
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | b9ff6c18366e48e3955036edcc47517f |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 792ab077eebe4178b751c76219242450 |
| service_name | placement                        |
| service_type | placement                        |
| url          | http://controller1:8778          |
+--------------+----------------------------------+
[root@controller1 data]# openstack endpoint create --region RegionOne placement admin http://controller1:8778
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | ae0b1860ed10459c8bcef228f273013e |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 792ab077eebe4178b751c76219242450 |
| service_name | placement                        |
| service_type | placement                        |
| url          | http://controller1:8778          |
+--------------+----------------------------------+
</code></pre><p>安装openstack-placement-api包</p>
<pre tabindex="0"><code>[root@controller1 data]# yum install openstack-placement-api -y
</code></pre><p>编辑placement配置</p>
<pre tabindex="0"><code>[root@controller1 data]# vim /etc/placement/placement.conf
[placement_database]
connection = mysql+pymysql://placement:placement@controller1/placement

[api]
# ...
auth_strategy = keystone

[keystone_authtoken]
# ...
auth_url = http://controller:5000/v3
memcached_servers = controller:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = placement
password = placement
</code></pre><p>同步placement数据库表</p>
<pre tabindex="0"><code>[root@controller1 data]# /bin/sh -c &quot;placement-manage db sync&quot; placement
/usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1280, u&quot;Name 'alembic_version_pkc' ignored for PRIMARY key.&quot;)
  result = self._query(query)
</code></pre><p>编辑httpd placement配置文件</p>
<pre tabindex="0"><code>[root@controller1 ~]# vim /etc/httpd/conf.d/00-placement-api.conf
&lt;VirtualHost *:8778&gt;
......
  &lt;Directory /usr/bin&gt;
      &lt;IfVersion &gt;= 2.4&gt;
          Require all granted
      &lt;/IfVersion&gt;
      &lt;IfVersion &lt; 2.4&gt;
          Order allow,deny
          Allow from all
      &lt;/IfVersion&gt;
  &lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre><p>启动placement-api服务</p>
<pre tabindex="0"><code># systemctl restart httpd
</code></pre><p>验证placement服务</p>
<pre tabindex="0"><code>[root@controller1 data]# placement-status upgrade check
+----------------------------------+
| Upgrade Check Results            |
+----------------------------------+
| Check: Missing Root Provider IDs |
| Result: Success                  |
| Details: None                    |
+----------------------------------+
| Check: Incomplete Consumers      |
| Result: Success                  |
| Details: None                    |
+----------------------------------+
</code></pre><h4 id="2212-nova">2.2.12 Nova</h4>
<p>初始化nova数据库</p>
<pre tabindex="0"><code>[root@controller1 data]# mysql -u root -popenstack
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 52
Server version: 10.3.20-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE nova_api;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]&gt; CREATE DATABASE nova;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]&gt; CREATE DATABASE nova_cell0;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' \
    -&gt;   IDENTIFIED BY 'nova';
Query OK, 0 rows affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' \
    -&gt;   IDENTIFIED BY 'nova';
Query OK, 0 rows affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \
    -&gt;   IDENTIFIED BY 'nova';
Query OK, 0 rows affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \
    -&gt;   IDENTIFIED BY 'nova';
Query OK, 0 rows affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' \
    -&gt;   IDENTIFIED BY 'nova';
Query OK, 0 rows affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' \
    -&gt;   IDENTIFIED BY 'nova';
Query OK, 0 rows affected (0.000 sec)
</code></pre><p>创建nova用户</p>
<pre tabindex="0"><code>[root@controller1 data]# openstack user create --domain default --password nova nova
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | default                          |
| enabled             | True                             |
| id                  | 994991c181c0455c9aef3ed1ae62f610 |
| name                | nova                             |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
</code></pre><p>赋予nova用户admin权限，并添加到service项目</p>
<pre tabindex="0"><code>[root@controller1 data]# openstack role add --project service --user nova admin
</code></pre><p>创建nova服务</p>
<pre tabindex="0"><code>[root@controller1 data]# openstack service create --name nova \
&gt;   --description &quot;OpenStack Compute&quot; compute
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Compute                |
| enabled     | True                             |
| id          | 32459289053c49bd91fb375410771202 |
| name        | nova                             |
| type        | compute                          |
+-------------+----------------------------------+
</code></pre><p>创建nova endpoint</p>
<pre tabindex="0"><code>[root@controller1 data]# openstack endpoint create --region RegionOne \
&gt;   compute public http://controller1:8774/v2.1
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 4bd23d114cac4a85a2e4b055dc0cbf52 |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 32459289053c49bd91fb375410771202 |
| service_name | nova                             |
| service_type | compute                          |
| url          | http://controller1:8774/v2.1     |
+--------------+----------------------------------+
[root@controller1 data]# openstack endpoint create --region RegionOne \
&gt;   compute internal http://controller1:8774/v2.1
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | a2f182824d444fd5ac5bfba72aa55967 |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 32459289053c49bd91fb375410771202 |
| service_name | nova                             |
| service_type | compute                          |
| url          | http://controller1:8774/v2.1     |
+--------------+----------------------------------+
[root@controller1 data]# openstack endpoint create --region RegionOne \
&gt;   compute admin http://controller1:8774/v2.1
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 97c7e07b1df84addaf0a26b5d964bf82 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 32459289053c49bd91fb375410771202 |
| service_name | nova                             |
| service_type | compute                          |
| url          | http://controller1:8774/v2.1     |
+--------------+----------------------------------+
</code></pre><p>安装nova包</p>
<pre tabindex="0"><code>[root@controller1 data]# yum install openstack-nova-api openstack-nova-conductor   openstack-nova-novncproxy openstack-nova-scheduler
</code></pre><p>编辑nova配置</p>
<pre tabindex="0"><code>[root@controller1 data]# vim /etc/nova/nova.conf
[DEFAULT]
# ...
enabled_apis = osapi_compute,metadata
transport_url = rabbit://openstack:openstack@controller1:5672/
my_ip = 172.16.88.245
use_neutron = true
firewall_driver = nova.virt.firewall.NoopFirewallDriver

[cache]
backend = oslo_cache.memcache_pool
enabled = True
memcache_servers = controller1:11211

[api_database]
# ...
connection = mysql+pymysql://nova:nova@controller1/nova_api
connection_recycle_time = 10
max_overflow = 1000
max_pool_size = 1
max_retries = -1

[database]
# ...
connection = mysql+pymysql://nova:nova@controller1/nova
connection_recycle_time = 10
max_overflow = 1000
max_pool_size = 1
max_retries = -1

[api]
# ...
auth_strategy = keystone

[keystone_authtoken]
# ...
www_authenticate_uri = http://controller1:5000/
auth_url = http://controller1:5000/
memcached_servers = controller1:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = nova
password = nova

[vnc]
enabled = true
# ...
server_listen = 172.16.88.245
server_proxyclient_address = 172.16.88.245
novncproxy_base_url = http://controller1:6080/vnc_auto.html

[glance]
# ...
api_servers = http://controller1:9292

[oslo_concurrency]
# ...
lock_path = /var/lib/nova/tmp

[placement]
# ...
region_name = RegionOne
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://controller1:5000/v3
username = placement
password = placement
</code></pre><p>同步nova-api数据库表</p>
<pre tabindex="0"><code>[root@controller1 data]# /bin/sh -c &quot;nova-manage api_db sync&quot; nova
</code></pre><p>注册cell0数据库</p>
<pre tabindex="0"><code>[root@controller1 ~]# /bin/sh -c &quot;nova-manage cell_v2 map_cell0&quot; nova
Cell0 is already setup
</code></pre><p>创建cell1 cell</p>
<pre tabindex="0"><code>[root@controller1 ~]# /bin/sh -c &quot;nova-manage cell_v2 create_cell --name=cell1 --verbose&quot; nova
a16bf78a-8657-4717-9037-f14428c7eceb
</code></pre><p>同步nova数据库表</p>
<pre tabindex="0"><code>[root@controller1 ~]# /bin/sh -c &quot;nova-manage db sync&quot; nova
/usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1831, u'Duplicate index `block_device_mapping_instance_uuid_virtual_name_device_name_idx`. This is deprecated and will be disallowed in a future release')
  result = self._query(query)
/usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1831, u'Duplicate index `uniq_instances0uuid`. This is deprecated and will be disallowed in a future release')
  result = self._query(query)
</code></pre><p>验证nova cell0、cell1是否注册成功</p>
<pre tabindex="0"><code>[root@controller1 ~]# /bin/sh -c &quot;nova-manage cell_v2 list_cells&quot; nova
+-------+--------------------------------------+-------------------------------------------+--------------------------------------------------+----------+
|  Name |                 UUID                 |               Transport URL               |               Database Connection                | Disabled |
+-------+--------------------------------------+-------------------------------------------+--------------------------------------------------+----------+
| cell0 | 00000000-0000-0000-0000-000000000000 |                   none:/                  | mysql+pymysql://nova:****@controller1/nova_cell0 |  False   |
| cell1 | a16bf78a-8657-4717-9037-f14428c7eceb | rabbit://openstack:****@controller1:5672/ |    mysql+pymysql://nova:****@controller1/nova    |  False   |
+-------+--------------------------------------+-------------------------------------------+--------------------------------------------------+----------+
</code></pre><p>启动nova服务</p>
<pre tabindex="0"><code># systemctl enable \
    openstack-nova-api.service \
    openstack-nova-scheduler.service \
    openstack-nova-conductor.service \
    openstack-nova-novncproxy.service
# systemctl start \
    openstack-nova-api.service \
    openstack-nova-scheduler.service \
    openstack-nova-conductor.service \
    openstack-nova-novncproxy.service
</code></pre><p>待添加完计算节点后，可以执行以下操作进行服务验证：
1、查看nova服务列表</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack compute service list
+----+----------------+-------------+----------+---------+-------+----------------------------+
| ID | Binary         | Host        | Zone     | Status  | State | Updated At                 |
+----+----------------+-------------+----------+---------+-------+----------------------------+
|  1 | nova-conductor | controller1 | internal | enabled | up    | 2021-05-12T03:37:43.000000 |
|  3 | nova-scheduler | controller1 | internal | enabled | up    | 2021-05-12T03:37:49.000000 |
|  8 | nova-compute   | computer1   | nova     | enabled | up    | 2021-05-12T03:37:51.000000 |
| 11 | nova-compute   | controller1 | nova     | enabled | up    | 2021-05-12T03:37:52.000000 |
+----+----------------+-------------+----------+---------+-------+----------------------------+
</code></pre><p>2、查看API endpoints列表</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack catalog list
+-----------+-----------+------------------------------------------+
| Name      | Type      | Endpoints                                |
+-----------+-----------+------------------------------------------+
| keystone  | identity  | RegionOne                                |
|           |           |   admin: http://controller1:5000/v3/     |
|           |           | RegionOne                                |
|           |           |   public: http://controller1:5000/v3/    |
|           |           | RegionOne                                |
|           |           |   internal: http://controller1:5000/v3/  |
|           |           |                                          |
| nova      | compute   | RegionOne                                |
|           |           |   public: http://controller1:8774/v2.1   |
|           |           | RegionOne                                |
|           |           |   admin: http://controller1:8774/v2.1    |
|           |           | RegionOne                                |
|           |           |   internal: http://controller1:8774/v2.1 |
|           |           |                                          |
| glance    | image     | RegionOne                                |
|           |           |   public: http://controller1:9292        |
|           |           | RegionOne                                |
|           |           |   internal: http://controller1:9292      |
|           |           | RegionOne                                |
|           |           |   admin: http://controller1:9292         |
|           |           |                                          |
| placement | placement | RegionOne                                |
|           |           |   public: http://controller1:8778        |
|           |           | RegionOne                                |
|           |           |   admin: http://controller1:8778         |
|           |           | RegionOne                                |
|           |           |   internal: http://controller1:8778      |
|           |           |                                          |
+-----------+-----------+------------------------------------------+
</code></pre><p>3、查看镜像列表</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack image list
+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| 7abfcd41-c55c-4344-ad52-6e80716f9aca | cirros | active |
+--------------------------------------+--------+--------+
</code></pre><p>4、校验cells和placement API是否工作正常</p>
<pre tabindex="0"><code>[root@controller1 ~]# nova-status upgrade check
+--------------------------------+
| Upgrade Check Results          |
+--------------------------------+
| Check: Cells v2                |
| Result: Success                |
| Details: None                  |
+--------------------------------+
| Check: Placement API           |
| Result: Success                |
| Details: None                  |
+--------------------------------+
| Check: Ironic Flavor Migration |
| Result: Success                |
| Details: None                  |
+--------------------------------+
| Check: Cinder API              |
| Result: Success                |
| Details: None                  |
+--------------------------------+
</code></pre><h4 id="2213-neutron">2.2.13 Neutron</h4>
<p>初始化neutron数据库</p>
<pre tabindex="0"><code>[root@controller1 ~]# mysql -u root -popenstack
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 7396
Server version: 10.3.20-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE neutron;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' \
    -&gt;   IDENTIFIED BY 'neutron';
Query OK, 0 rows affected (0.001 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' \
    -&gt;   IDENTIFIED BY 'neutron';
Query OK, 0 rows affected (0.000 sec)
</code></pre><p>创建neutron用户</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack user create --domain default --password neutron neutron
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | default                          |
| enabled             | True                             |
| id                  | a8c22e883b3a44f2b2b93bda0bfbc1c2 |
| name                | neutron                          |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
</code></pre><p>授予neutron用户admin权限，并添加到service项目</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack role add --project service --user neutron admin
</code></pre><p>创建neutron服务</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack service create --name neutron \
&gt;   --description &quot;OpenStack Networking&quot; network

+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Networking             |
| enabled     | True                             |
| id          | 8986d22b17954afa8c8616862c1b61d3 |
| name        | neutron                          |
| type        | network                          |
+-------------+----------------------------------+
</code></pre><p>创建neutron endpoint</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack endpoint create --region RegionOne network public http://controller1:9696
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | cb46fdc6723747059ff51e2b458c1ebf |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 8986d22b17954afa8c8616862c1b61d3 |
| service_name | neutron                          |
| service_type | network                          |
| url          | http://controller1:9696          |
+--------------+----------------------------------+

[root@controller1 ~]# openstack endpoint create --region RegionOne network internal http://controller1:9696
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | e6e96552557e4c668fcfead75afeb4a8 |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 8986d22b17954afa8c8616862c1b61d3 |
| service_name | neutron                          |
| service_type | network                          |
| url          | http://controller1:9696          |
+--------------+----------------------------------+

[root@controller1 ~]# openstack endpoint create --region RegionOne network admin http://controller1:9696
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 4e2841ebfbe14bf4b8c24a5cbc5edf85 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 8986d22b17954afa8c8616862c1b61d3 |
| service_name | neutron                          |
| service_type | network                          |
| url          | http://controller1:9696          |
+--------------+----------------------------------+
</code></pre><p>安装neutron包</p>
<pre tabindex="0"><code>[root@controller1 ~]# yum install -y openstack-neutron openstack-neutron-ml2 \
    openstack-neutron-linuxbridge ebtables
</code></pre><p>编辑neutron配置文件（使用linuxbridge）</p>
<pre tabindex="0"><code># vim /etc/neutron/neutron.conf
[database]
# ...
connection = mysql+pymysql://neutron:NEUTRON_DBPASS@controller/neutron
connection_recycle_time = 10
max_overflow = 1000
max_pool_size = 1
max_retries = -1

[DEFAULT]
# ...
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = true
transport_url = rabbit://openstack:openstack@controller1:5672/
auth_strategy = keystone
notify_nova_on_port_status_changes = true
notify_nova_on_port_data_changes = true

[keystone_authtoken]
# ...
www_authenticate_uri = http://controller1:5000
auth_url = http://controller1:5000
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = neturon

[nova]
# ...
auth_url = http://controller1:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = nova

[oslo_concurrency]
# ...
lock_path = /var/lib/neutron/tmp
</code></pre><p>编辑ml2配置文件</p>
<pre tabindex="0"><code># vim /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
# ...
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan

mechanism_drivers = linuxbridge,l2population
extension_drivers = port_security

[ml2_type_flat]
# ...
flat_networks = provider

[ml2_type_vxlan]
# ...
vni_ranges = 1:1000

[securitygroup]
# ...
enable_ipset = true
</code></pre><p>编辑linuxbridge配置文件</p>
<pre tabindex="0"><code># vim /etc/neutron/plugins/ml2/linuxbridge_agent.ini
[linux_bridge]
physical_interface_mappings = provider:em3

[vxlan]
enable_vxlan = true
local_ip = 172.16.88.245
l2_population = true

[securitygroup]
# ...
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
</code></pre><p>编辑sysctl.conf配置</p>
<pre tabindex="0"><code># vim /etc/sysctl.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1

# sysctl -p
</code></pre><p>编辑l3配置文件</p>
<pre tabindex="0"><code>[root@controller1 ~]# vim /etc/neutron/l3_agent.ini
[DEFAULT]
interface_driver = linuxbridge
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = true
</code></pre><p>编辑metadata配置文件</p>
<pre tabindex="0"><code>[root@controller1 ~]# vim /etc/neutron/metadata_agent.ini
[DEFAULT]
# ...
nova_metadata_host = controller1
metadata_proxy_shared_secret = METADATA_SECRET
</code></pre><p>编辑nova配置，nova与neutron交互</p>
<pre tabindex="0"><code>[root@controller1 ~]# vim /etc/nova/nova.conf
[neutron]
# ...
auth_url = http://controller1:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = neutron
service_metadata_proxy = true
metadata_proxy_shared_secret = METADATA_SECRET
</code></pre><p>创建软链接，neutron-server systemd启动脚步读取的是<code>/etc/neutron/plugin.ini</code>配置文件</p>
<pre tabindex="0"><code>[root@controller1 ~]# ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
</code></pre><p>同步neutron数据库表</p>
<pre tabindex="0"><code>[root@controller1 ~]# /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf \
&gt;   --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head&quot; neutron
</code></pre><p>重启nova api服务</p>
<pre tabindex="0"><code>[root@controller1 ~]# systemctl restart openstack-nova-api.service
</code></pre><p>启动neutron服务</p>
<pre tabindex="0"><code>[root@controller1 ~]# systemctl enable neutron-server.service \
&gt;   neutron-linuxbridge-agent.service neutron-dhcp-agent.service \
&gt;   neutron-metadata-agent.service \
&gt;   neutron-l3-agent.service

[root@controller1 ~]# systemctl start neutron-server.service \
&gt;   neutron-linuxbridge-agent.service neutron-dhcp-agent.service \
&gt;   neutron-metadata-agent.service \
&gt;   neutron-l3-agent.service
</code></pre><h4 id="2214-cinder">2.2.14 Cinder</h4>
<p>初始化cinder数据库</p>
<pre tabindex="0"><code>[root@controller1 ~]# mysql -u root -popenstack
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 26397
Server version: 10.3.20-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]&gt; CREATE DATABASE cinder;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' \
    -&gt;   IDENTIFIED BY 'cinder';
Query OK, 0 rows affected (0.001 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' \
    -&gt;   IDENTIFIED BY 'cinder';
Query OK, 0 rows affected (0.000 sec)
</code></pre><p>创建cinder用户</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack user create --domain default --password cinder cinder

+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | default                          |
| enabled             | True                             |
| id                  | 58104291fa634ea2a72b28864d2700a9 |
| name                | cinder                           |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
</code></pre><p>授予cindder用户admin权限，并添加到service项目</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack role add --project service --user cinder admin
</code></pre><p>创建cinder v2服务</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack service create --name cinderv2 \
&gt;   --description &quot;OpenStack Block Storage&quot; volumev2
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Block Storage          |
| enabled     | True                             |
| id          | 7ce339c0607247c3977588117cc55c93 |
| name        | cinderv2                         |
| type        | volumev2                         |
+-------------+----------------------------------+
</code></pre><p>创建cinder v3服务</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack service create --name cinderv3 \
&gt;   --description &quot;OpenStack Block Storage&quot; volumev3
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Block Storage          |
| enabled     | True                             |
| id          | f29317ffd14640468a052452d727f94c |
| name        | cinderv3                         |
| type        | volumev3                         |
+-------------+----------------------------------+
</code></pre><p>创建cinder v2 endpoint</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack endpoint create --region RegionOne \
volumev2 public http://controller1:8776/v2/%\(project_id\)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | 16552ff4fb3846b790f63337f85fed5a          |
| interface    | public                                    |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 7ce339c0607247c3977588117cc55c93          |
| service_name | cinderv2                                  |
| service_type | volumev2                                  |
| url          | http://controller1:8776/v2/%(project_id)s |
+--------------+-------------------------------------------+

[root@controller1 ~]# openstack endpoint create --region RegionOne \
volumev2 internal http://controller1:8776/v2/%\(project_id\)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | 278352f982b849f4a9c5982218b01db4          |
| interface    | internal                                  |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 7ce339c0607247c3977588117cc55c93          |
| service_name | cinderv2                                  |
| service_type | volumev2                                  |
| url          | http://controller1:8776/v2/%(project_id)s |
+--------------+-------------------------------------------+

[root@controller1 ~]# openstack endpoint create --region RegionOne \
volumev2 admin http://controller1:8776/v2/%\(project_id\)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | 4acda7fa4ce849e7a48858607eb35f40          |
| interface    | admin                                     |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 7ce339c0607247c3977588117cc55c93          |
| service_name | cinderv2                                  |
| service_type | volumev2                                  |
| url          | http://controller1:8776/v2/%(project_id)s |
+--------------+-------------------------------------------+
</code></pre><p>创建cinder v3 endpoint</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack endpoint create --region RegionOne \
volumev3 public http://controller1:8776/v3/%\(project_id\)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | ed063de851e34eafbee64efcd8061d2b          |
| interface    | public                                    |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | f29317ffd14640468a052452d727f94c          |
| service_name | cinderv3                                  |
| service_type | volumev3                                  |
| url          | http://controller1:8776/v3/%(project_id)s |
+--------------+-------------------------------------------+

[root@controller1 ~]# openstack endpoint create --region RegionOne \
volumev3 internal http://controller1:8776/v3/%\(project_id\)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | 8b98c04c6fc7444da90cbc2692e735a1          |
| interface    | internal                                  |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | f29317ffd14640468a052452d727f94c          |
| service_name | cinderv3                                  |
| service_type | volumev3                                  |
| url          | http://controller1:8776/v3/%(project_id)s |
+--------------+-------------------------------------------+

[root@controller1 ~]# openstack endpoint create --region RegionOne \
volumev3 admin http://controller1:8776/v3/%\(project_id\)s
+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | d7d98fee7cf44ac6bdaf6edaae3456a0          |
| interface    | admin                                     |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | f29317ffd14640468a052452d727f94c          |
| service_name | cinderv3                                  |
| service_type | volumev3                                  |
| url          | http://controller1:8776/v3/%(project_id)s |
+--------------+-------------------------------------------+
</code></pre><p>安装cinder安装包</p>
<pre tabindex="0"><code>[root@controller1 ~]# yum install openstack-cinder -y
</code></pre><p>编辑cinder配置文件</p>
<pre tabindex="0"><code>[root@controller1 ~]# vim /etc/cinder/cinder.conf
[DEFAULT]
transport_url = rabbit://openstack:openstack@controller1:5672/
auth_strategy = keystone
my_ip = 172.16.88.245

[database]
connection = mysql+pymysql://cinder:cinder@controller1/cinder
connection_recycle_time = 10
max_overflow = 1000
max_pool_size = 1
max_retries = -1

[keystone_authtoken]
www_authenticate_uri = http://controller1:5000
auth_url = http://controller1:5000
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = cinder
password = cinder

[oslo_concurrency]
lock_path = /var/lib/cinder/tmp
</code></pre><p>同步cinder数据库表</p>
<pre tabindex="0"><code>[root@controller1 ~]#  /bin/sh -c &quot;cinder-manage db sync&quot; cinder
Deprecated: Option &quot;logdir&quot; from group &quot;DEFAULT&quot; is deprecated. Use option &quot;log-dir&quot; from group &quot;DEFAULT&quot;.
</code></pre><p>配置nova使用cinder</p>
<pre tabindex="0"><code>[root@controller1 ~]# vim /etc/nova/nova.conf
[cinder]
os_region_name = RegionOne
</code></pre><p>重启nova-api服务</p>
<pre tabindex="0"><code>[root@controller1 ~]# systemctl restart openstack-nova-api.service
</code></pre><p>启动cinder服务</p>
<pre tabindex="0"><code>[root@controller1 ~]# systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service
Created symlink from /etc/systemd/system/multi-user.target.wants/openstack-cinder-api.service to /usr/lib/systemd/system/openstack-cinder-api.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/openstack-cinder-scheduler.service to /usr/lib/systemd/system/openstack-cinder-scheduler.service.

[root@controller1 ~]# systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service
</code></pre><p>配置storage节点（以LVM为例）</p>
<pre tabindex="0"><code># yum install lvm2 device-mapper-persistent-data openstack-cinder targetcli python-keystone
</code></pre><p>启动lvm元数据服务</p>
<pre tabindex="0"><code># systemctl enable lvm2-lvmetad.service
# systemctl start lvm2-lvmetad.service
</code></pre><p>模拟LVM（节点上没有多余的盘）</p>
<pre tabindex="0"><code>[root@controller1 data(keystone_admin)]# fallocate -l 2T cinder-volumes
[root@controller1 data(keystone_admin)]# losetup -f cinder-volumes
[root@controller1 data(keystone_admin)]# losetup -a
/dev/loop0: [64768]:567 (/data/cinder-volumes)
</code></pre><p>编辑lvm配置文件, 允许/dev/loop0设备</p>
<pre tabindex="0"><code>[root@controller1 data(keystone_admin)]# vim /etc/lvm/lvm.conf
filter = [ &quot;a|loop0|&quot;, &quot;r|.*|&quot; ]
</code></pre><pre tabindex="0"><code>[root@controller1 data(keystone_admin)]# vim /etc/cinder/cinder.conf
[DEFAULT]
# ...
enabled_backends = lvm
[lvm]
volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
volume_group = cinder-volumes
target_protocol = iscsi
target_helper = lioadm
</code></pre><p>启动cinder-volume服务</p>
<pre tabindex="0"><code>[root@controller1 data(keystone_admin)]# systemctl enable openstack-cinder-volume.service target.service
[root@controller1 data(keystone_admin)]# systemctl start openstack-cinder-volume.service target.service
</code></pre><p>验证cinder服务</p>
<pre tabindex="0"><code>[root@controller1 data(keystone_admin)]# openstack volume service list
+------------------+-----------------+------+---------+-------+----------------------------+
| Binary           | Host            | Zone | Status  | State | Updated At                 |
+------------------+-----------------+------+---------+-------+----------------------------+
| cinder-scheduler | controller1     | nova | enabled | up    | 2021-05-20T11:28:20.000000 |
| cinder-volume    | controller1@lvm | nova | enabled | up    | 2021-05-20T11:28:21.000000 |
+------------------+-----------------+------+---------+-------+----------------------------+
</code></pre><p>cinder-volume对接ceph(待验证)</p>
<pre tabindex="0"><code># 创建cinder pool
[root@ceph-1 ~]# ceph osd pool create cinder 128
</code></pre><p>创建ceph cinder用户</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=cinder'
[client.cinder]
	key = AQBIVKZgGHE2MxAAAYlmJJaoe4/u6AalEnuSAQ==
</code></pre><p>创建<code>/etc/cinder/ceph.client.cinder.keyring</code>文件，拷贝/etc/ceph目录到glance-api、cinder-volume、cinder-backup、nova-compute节点上</p>
<p>增加rbd配置，配置cinder多后端</p>
<pre tabindex="0"><code>[root@controller1 ~]# vim /etc/cinder/cinder.conf
[DEFAULT]
enabled_backends=lvm,rbd
default_volume_type = lvm

[rbd]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
rbd_pool = rbd
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_flatten_volume_from_snapshot = false
rbd_max_clone_depth = 5
rbd_store_chunk_size = 4
rados_connect_timeout = -1
glance_api_version = 2
rbd_user = rbd
rbd_secret_uuid = aa03e7e8-6fcc-443f-94aa-ac169bfd0fd5
</code></pre><p>创建lvm和rbd类型</p>
<pre tabindex="0"><code>[root@controller1 data(keystone_admin)]# cinder type-create lvm
+--------------------------------------+------+-------------+-----------+
| ID                                   | Name | Description | Is_Public |
+--------------------------------------+------+-------------+-----------+
| e55deb0f-5b79-4d51-93e1-cf5b9123d2b7 | lvm  | -           | True      |
+--------------------------------------+------+-------------+-----------+
[root@controller1 data(keystone_admin)]# cinder type-create rbd
+--------------------------------------+------+-------------+-----------+
| ID                                   | Name | Description | Is_Public |
+--------------------------------------+------+-------------+-----------+
| 99f79886-3f9a-4db5-8410-903738fd3c0b | rbd  | -           | True      |
+--------------------------------------+------+-------------+-----------+

[root@controller1 data(keystone_admin)]# cinder type-key lvm set volume_backend_name=lvm
[root@controller1 data(keystone_admin)]# cinder type-key rbd set volume_backend_name=rbd
</code></pre><h4 id="2215-horizon">2.2.15 Horizon</h4>
<p>安装horizon包</p>
<pre tabindex="0"><code>[root@controller1 ~]# yum install -y openstack-dashboard
</code></pre><p>编辑horizon配置文件</p>
<pre tabindex="0"><code>[root@controller1 ~]# vim /etc/openstack-dashboard/local_settings
COMPRESS_OFFLINE = True
WEBROOT = '/dashboard/'
OPENSTACK_HOST = &quot;controller1&quot;
ALLOWED_HOSTS = ['*']

SESSION_ENGINE = 'django.contrib.sessions.backends.cache'

CACHES = {
    'default': {
         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
         'LOCATION': 'controller1:11211',
    }
}

OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True
OPENSTACK_API_VERSIONS = {
    &quot;identity&quot;: 3,
    &quot;image&quot;: 2,
    &quot;volume&quot;: 3,
}

OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &quot;Default&quot;
OPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;member&quot;

OPENSTACK_KEYSTONE_BACKEND = {
    'name': 'native',
    'can_edit_user': True,
    'can_edit_group': True,
    'can_edit_project': True,
    'can_edit_domain': True,
    'can_edit_role': True,
}

OPENSTACK_HYPERVISOR_FEATURES = {
    'can_set_mount_point': False,
    'can_set_password': False,
    'requires_keypair': False,
    'enable_quotas': True
}

OPENSTACK_CINDER_FEATURES = {
    'enable_backup': True,
}

TIME_ZONE = &quot;Asia/Shanghai&quot;
# Path to directory containing policy.json files
POLICY_FILES_PATH = '/etc/openstack-dashboard'
</code></pre><p>编辑horizon httpd配置文件</p>
<pre tabindex="0"><code>[root@controller1 ~]# vim /etc/httpd/conf.d/openstack-dashboard.conf
</code></pre><p>重启httpd服务</p>
<pre tabindex="0"><code>[root@controller1 ~]# systemctl status httpd.service
</code></pre><p>登录<code>http://&lt;controller-ip&gt;/dashboard</code>访问</p>
<p>tips: 界面logo修改路径</p>
<pre tabindex="0"><code>[root@controller1 img]# ll /usr/share/openstack-dashboard/static/dashboard/img/logo-splash.svg
-rw-r--r-- 1 root root 9632 May 13 20:48 /usr/share/openstack-dashboard/static/dashboard/img/logo-splash.svg
[root@controller1 img]# ll /usr/share/openstack-dashboard/static/dashboard/img/logo.svg
-rw-r--r-- 1 root root 9632 May 13 20:50 /usr/share/openstack-dashboard/static/dashboard/img/logo.svg
</code></pre><h3 id="23-compute-node">2.3 Compute Node</h3>
<h4 id="231-静态域名解析">2.3.1 静态域名解析</h4>
<pre tabindex="0"><code># vim /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

172.16.88.245 controller1
172.16.88.246 computer1
</code></pre><h4 id="232-ntp">2.3.2 NTP</h4>
<p>安装chrony</p>
<pre tabindex="0"><code># yum install -y chrony
</code></pre><p>编辑chrony配置(移除原来的server块区域)</p>
<pre tabindex="0"><code># vim /etc/chrony.conf
server controller1 iburst
</code></pre><p>启动chrony</p>
<pre tabindex="0"><code># systemctl enable chronyd.service
# systemctl restart chronyd.service
</code></pre><p>查看NTP同步情况</p>
<pre tabindex="0"><code># chronyc sources
210 Number of sources = 1
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^* controller1                   3   6   177    40   +111us[ +207us] +/-   16ms
</code></pre><h4 id="233-yum源">2.3.3 Yum源</h4>
<p>安装OpenStack train源</p>
<pre tabindex="0"><code># yum install -y centos-release-openstack-train
</code></pre><h4 id="234-nova">2.3.4 Nova</h4>
<p>编辑nova配置</p>
<pre tabindex="0"><code>[root@controller1 data]# vim /etc/nova/nova.conf
[DEFAULT]
# ...
enabled_apis = osapi_compute,metadata
transport_url = rabbit://openstack:openstack@controller1:5672/
my_ip = 172.16.88.245
use_neutron = true
firewall_driver = nova.virt.firewall.NoopFirewallDriver
cpu_allocation_ratio=4.0
ram_allocation_ratio=1.0
disk_allocation_ratio=1.0
reserved_host_memory_mb=4096

[api]
# ...
auth_strategy = keystone

[keystone_authtoken]
# ...
www_authenticate_uri = http://controller1:5000/
auth_url = http://controller1:5000/
memcached_servers = controller1:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = nova
password = nova

[vnc]
enabled = true
# ...
server_listen = 172.16.88.245
server_proxyclient_address = 172.16.88.245
novncproxy_base_url = http://controller1:6080/vnc_auto.html

[glance]
# ...
api_servers = http://controller1:9292

[oslo_concurrency]
# ...
lock_path = $state_path/tmp

[placement]
# ...
region_name = RegionOne
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://controller1:5000/v3
username = placement
password = placement
</code></pre><p>查看节点是否开启VT硬件加速</p>
<pre tabindex="0"><code>[root@computer1 ~]# egrep -c '(vmx|svm)' /proc/cpuinfo
48
</code></pre><p>如果返回为0的话，即没有开启VT，需要配置<code>virt_type=qemu</code></p>
<p>启动nova compute服务</p>
<pre tabindex="0"><code># systemctl enable libvirtd.service openstack-nova-compute.service
# systemctl start libvirtd.service openstack-nova-compute.service
</code></pre><p>查看nova compute服务</p>
<pre tabindex="0"><code>[root@controller1 ~]# openstack compute service list --service nova-compute
+----+--------------+-------------+------+---------+-------+----------------------------+
| ID | Binary       | Host        | Zone | Status  | State | Updated At                 |
+----+--------------+-------------+------+---------+-------+----------------------------+
|  8 | nova-compute | computer1   | nova | enabled | up    | 2021-05-12T01:43:01.000000 |
|  9 | nova-compute | controller1 | nova | enabled | up    | 2021-05-12T01:42:54.000000 |
+----+--------------+-------------+------+---------+-------+----------------------------+
</code></pre><p>注册新计算节点到cell数据库（controller节点上操作）</p>
<pre tabindex="0"><code>[root@controller1 ~]# /bin/sh -c &quot;nova-manage cell_v2 discover_hosts --verbose&quot; nova
</code></pre><p>也可以使用cell自动发现功能，通过设置</p>
<pre tabindex="0"><code># vim /etc/nova/nova.conf
[scheduler]
discover_hosts_in_cells_interval = 300
</code></pre><h4 id="235-neutron">2.3.5 Neutron</h4>
<p>安装neutron包</p>
<pre tabindex="0"><code>[root@computer1 ~]# yum install openstack-neutron-linuxbridge ebtables ipset -y
</code></pre><p>编辑neutron配置文件</p>
<pre tabindex="0"><code>[root@computer1 ~]# vim /etc/neutron/neutron.conf
[DEFAULT]
transport_url = rabbit://openstack:openstack@controller1:5672/
auth_strategy = keystone

[keystone_authtoken]
# ...
www_authenticate_uri = http://controller1:5000
auth_url = http://controller1:5000
memcached_servers = controller1:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = neutron

[oslo_concurrency]
# ...
lock_path = /var/lib/neutron/tmp
</code></pre><p>编辑ml2配置文件</p>
<pre tabindex="0"><code>[root@computer1 ~]# vim /etc/neutron/plugins/ml2/linuxbridge_agent.ini
[linux_bridge]
physical_interface_mappings = provider:em3

[vxlan]
enable_vxlan = true
local_ip = 172.16.88.246
l2_population = true

[securitygroup]
# ...
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
</code></pre><p>编辑sysctl.conf配置</p>
<pre tabindex="0"><code># vim /etc/sysctl.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1

# sysctl -p
</code></pre><p>编辑nova配置，nova与neutron交互</p>
<pre tabindex="0"><code>[root@controller1 ~]# vim /etc/nova/nova.conf
[neutron]
# ...
auth_url = http://controller1:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = neutron
</code></pre><p>重启nova compute服务</p>
<pre tabindex="0"><code>[root@computer1 ~]# systemctl restart openstack-nova-compute.service
</code></pre><p>启动linux-bridge-agent服务</p>
<pre tabindex="0"><code>[root@computer1 ~]# systemctl enable neutron-linuxbridge-agent.service
[root@computer1 ~]# systemctl start neutron-linuxbridge-agent.service
</code></pre><p>验证neutron agent服务列表</p>
<pre tabindex="0"><code>[root@controller1 ~]#  openstack network agent list
+--------------------------------------+--------------------+-------------+-------------------+-------+-------+---------------------------+
| ID                                   | Agent Type         | Host        | Availability Zone | Alive | State | Binary                    |
+--------------------------------------+--------------------+-------------+-------------------+-------+-------+---------------------------+
| 2ec756b4-0313-4ca2-b77f-4c8ae82f4034 | DHCP agent         | controller1 | nova              | :-)   | UP    | neutron-dhcp-agent        |
| 97e383a7-c06e-4f3b-8352-3d54b89ad591 | L3 agent           | controller1 | nova              | :-)   | UP    | neutron-l3-agent          |
| a95ab80a-dd80-49bb-ac86-6eeb31264483 | Linux bridge agent | computer1   | None              | :-)   | UP    | neutron-linuxbridge-agent |
| e10466ac-63f7-4e5d-a982-5f94e3e2701c | Metadata agent     | controller1 | None              | :-)   | UP    | neutron-metadata-agent    |
| f88161bc-12bc-40b1-8337-a1f55e8679b6 | Linux bridge agent | controller1 | None              | :-)   | UP    | neutron-linuxbridge-agent |
+--------------------------------------+--------------------+-------------+-------------------+-------+-------+---------------------------+
</code></pre><h2 id="3-ceph">3. Ceph</h2>
<h3 id="31-cephadm">3.1 cephadm</h3>
<p>Cephadm使用容器和systemd安装并管理Ceph集群，并与CLI和仪表板GUI紧密集成。</p>
<ul>
<li>cephadm仅支持Octopus和更高版本。</li>
<li>cephadm与新的业务流程API完全集成，并完全支持新的CLI和仪表板功能来管理集群部署。</li>
<li>cephadm需要容器支持（podman或docker）和Python 3。</li>
</ul>
<blockquote>
<p>cephadm部署和管理Ceph集群。它是通过SSH将管理器守护程序连接到主机来实现的。管理器守护程序能够添加，删除和更新Ceph容器。
cephadm不依赖于Ansible，Rook和Salt等外部配置工具。</p>
</blockquote>
<blockquote>
<p>cephadm管理Ceph集群的整个生命周期。此生命周期从引导过程开始，当cephadm在单个节点上创建一个微小的Ceph群集时。该群集由一台监视器和一台管理器组成。
cephadm然后使用业务流程界面（“第2天”命令）扩展集群，添加所有主机并配置所有Ceph守护程序和服务。可以通过Ceph命令行界面（CLI）或通过仪表板（GUI）来执行此生命周期的管理。</p>
</blockquote>
<blockquote>
<p>cephadm是Ceph v15.2.0（Octopus）版本中的新增功能，并且不支持Ceph的旧版本。</p>
</blockquote>
<h3 id="32-部署ceph集群">3.2 部署ceph集群</h3>
<h4 id="321-部署环境">3.2.1 部署环境</h4>
<ul>
<li>内核版本：3.10.0-1127.el7.x86_64</li>
<li>操作系统：CentOS Linux release 7.8.2003 (Core)</li>
</ul>
<p>节点列表，每个节点都有一块额外盘/dev/vdb</p>
<pre tabindex="0"><code>172.16.80.45 ceph-1 [ceph-admin,ceph-mon,ceph-mgr,ceph-osd]
172.16.80.185 ceph-2 [ceph-mon,ceph-mgr,ceph-osd]
172.16.80.213 ceph-3 [ceph-mon,ceph-mgr,ceph-osd]
172.16.80.203 ceph-4 [ceph-osd]
172.16.80.90 ceph-5  [ceph-osd]
</code></pre><h4 id="322-前提条件">3.2.2 前提条件</h4>
<p>每个节点都需要做的前置条件</p>
<p>1、安装python3环境</p>
<pre tabindex="0"><code># yum install y wget -zlib-devel \
    bzip2-devel \
    openssl-devel \
    ncurses-devel \
    sqlite-devel \
    readline-devel \
    tk-devel \
    gcc \
    make

# wget -c https://www.python.org/ftp/python/3.9.5/Python-3.9.5.tar.xz
# tar -xvJf  Python-3.9.5.tar.xz
# ./configure prefix=/usr/local/Python3
# make &amp;&amp; make install

# ln -s /usr/local/Python3/bin/python3 /usr/bin/python3
# ln -s /usr/local/Python3/bin/pip3 /usr/bin/pip3
</code></pre><p>2、安装docker</p>
<pre tabindex="0"><code># yum-config-manager --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

# yum install -y docker-ce
# systemctl enable docker
# systemctl start docker
</code></pre><p>3、安装chrony
4、安装systemd
5、安装lvm2</p>
<h4 id="323-安装cephadm">3.2.3 安装cephadm</h4>
<p>ceph-admin节点上执行</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm
[root@ceph-1 ~]# chmod +x cephadm
[root@ceph-1 ~]# ./cephadm add-repo --release octopus
[root@ceph-1 ~]# ./cephadm install

[root@ceph-1 ~]# which cephadm
/usr/sbin/cephadm
</code></pre><h4 id="324-初始化新集群">3.2.4 初始化新集群</h4>
<pre tabindex="0"><code>[root@ceph-1 ~]# cephadm bootstrap --mon-ip 172.16.80.45
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit chronyd.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit chronyd.service is enabled and running
Host looks OK
Cluster fsid: 789b1ff2-b910-11eb-a538-fa163e24ae71
Verifying IP 172.16.80.45 port 3300 ...
Verifying IP 172.16.80.45 port 6789 ...
Mon IP 172.16.80.45 is in CIDR network 172.16.80.0/24
Pulling container image docker.io/ceph/ceph:v15...
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Assimilating anything we can from ceph.conf...
Generating new minimal ceph.conf...
Restarting the monitor...
Setting mon public_network...
Creating mgr...
Verifying port 9283 ...
Wrote keyring to /etc/ceph/ceph.client.admin.keyring
Wrote config to /etc/ceph/ceph.conf
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/10)...
mgr not available, waiting (2/10)...
mgr not available, waiting (3/10)...
mgr not available, waiting (4/10)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for Mgr epoch 5...
Mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to to /etc/ceph/ceph.pub
Adding key to root@localhost's authorized_keys...
Adding host ceph-1...
Deploying mon service with default placement...
Deploying mgr service with default placement...
Deploying crash service with default placement...
Enabling mgr prometheus module...
Deploying prometheus service with default placement...
Deploying grafana service with default placement...
Deploying node-exporter service with default placement...
Deploying alertmanager service with default placement...
Enabling the dashboard module...
Waiting for the mgr to restart...
Waiting for Mgr epoch 13...
Mgr epoch 13 is available
Generating a dashboard self-signed certificate...
Creating initial admin user...
Fetching dashboard port number...
Ceph Dashboard is now available at:

	     URL: https://ceph-1:8443/
	    User: admin
	Password: 85ermr0hjd

You can access the Ceph CLI with:

	sudo /usr/sbin/cephadm shell --fsid 789b1ff2-b910-11eb-a538-fa163e24ae71 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/master/mgr/telemetry/

Bootstrap complete.
</code></pre><p>浏览器登录<code>https://ceph-1:8443/</code>，首次登录会提示修改密码</p>
<p>查看所有组件运行状态</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph orch ps
NAME                  HOST    STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                            IMAGE ID      CONTAINER ID
alertmanager.ceph-1   ceph-1  running (8s)   5s ago     19s  0.20.0   docker.io/prom/alertmanager:v0.20.0   0881eb8f169f  fb2ba2adee1d
crash.ceph-1          ceph-1  running (17s)  5s ago     18s  15.2.12  docker.io/ceph/ceph:v15               c717e215da21  f32945a238d8
grafana.ceph-1        ceph-1  running (7s)   5s ago     16s  6.7.4    docker.io/ceph/ceph-grafana:6.7.4     ae5c36c3d3cd  6606b21f0d06
mgr.ceph-1.qvbenz     ceph-1  running (68s)  5s ago     68s  15.2.12  docker.io/ceph/ceph:v15               c717e215da21  1f7835d9a3f8
mon.ceph-1            ceph-1  running (68s)  5s ago     71s  15.2.12  docker.io/ceph/ceph:v15               c717e215da21  200e7545429b
node-exporter.ceph-1  ceph-1  running (15s)  5s ago     15s  0.18.1   docker.io/prom/node-exporter:v0.18.1  e5a616e4b9cf  7f728ef3eb9b
prometheus.ceph-1     ceph-1  running (14s)  5s ago     14s  2.18.1   docker.io/prom/prometheus:v2.18.1     de242295e225  313e5790bad6
</code></pre><h4 id="325-启用ceph-cli">3.2.5 启用ceph cli</h4>
<p>ceph shell会进入装好ceph软件包的容器中, 设置个alias，方便使用</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ./cephadm shell -- ceph status

[root@ceph-1 ~]# vim /etc/bashrc
alias ceph='cephadm shell -- ceph'
</code></pre><p>也可以在宿主机上安装ceph-common(可选)</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# cephadm add-repo --release octopus
[root@ceph-1 ~]# cephadm install ceph-common
</code></pre><h4 id="326-注册节点">3.2.6 注册节点</h4>
<p>把第一个mon节点上产生的ssh公钥拷贝到新节点上</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@172.16.80.185
[root@ceph-1 ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@172.16.80.213
</code></pre><p>注册新节点, 会自动扩展成monitor和manager</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph orch host add  ceph-2
[root@ceph-1 ~]# ceph orch host add  ceph-3
</code></pre><p>查看ceph纳管的所有节点</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph orch host ls
HOST    ADDR    LABELS  STATUS
ceph-1  ceph-1
ceph-2  ceph-2
ceph-3  ceph-3
</code></pre><p>移除注册节点</p>
<pre tabindex="0"><code># ceph orch host rm &lt;host&gt;
</code></pre><h4 id="327-部署mon可选">3.2.7 部署mon(可选)</h4>
<p>上述注册节点的步骤会自动部署mon，所以以下步骤可跳过。</p>
<p>一个典型的ceph集群一般有3个或5个monitor进程，如果集群规模达到5个节点及以上的话，建议部署5个monitor进程</p>
<p>设置ceph public_network网络</p>
<pre tabindex="0"><code># ceph config set mon public_network *&lt;mon-cidr-network&gt;*
</code></pre><p>默认情况下，向群集中添加新主机时，cephadm会自动最多添加5个mon, 可修改默认mon数量</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph orch apply mon 3
Scheduled mon update...
</code></pre><p>部署mon到指定的节点上</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph orch apply mon *&lt;host1,host2,host3,...&gt;*
</code></pre><p>禁止自动部署mon</p>
<pre tabindex="0"><code># ceph orch apply mon --unmanaged
</code></pre><h4 id="328-部署osd">3.2.8 部署osd</h4>
<p>再注册两个节点</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@172.16.80.203
[root@ceph-1 ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@172.16.80.90

[root@ceph-1 ~]# ceph orch host add  ceph-4
[root@ceph-1 ~]# ceph orch host add  ceph-5
</code></pre><p>自动添加所有可用设备(貌似没有效果，故手动指定添加osd)</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph orch apply osd --all-available-devices
</code></pre><p>指定节点指定设备创建osd</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph orch daemon add osd ceph-1:/dev/vdb
[root@ceph-1 ~]# ceph orch daemon add osd ceph-2:/dev/vdb
[root@ceph-1 ~]# ceph orch daemon add osd ceph-3:/dev/vdb
[root@ceph-1 ~]# ceph orch daemon add osd ceph-4:/dev/vdb
[root@ceph-1 ~]# ceph orch daemon add osd ceph-5:/dev/vdb
</code></pre><p>显示集群主机上所有的存储设备清单</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph orch device ls
Hostname  Path      Type  Serial                Size   Health   Ident  Fault  Available
ceph-1    /dev/vdb  hdd   d190d163-ff43-4e8b-8  21.4G  Unknown  N/A    N/A    No
ceph-2    /dev/vdb  hdd   6541642c-a7c0-46e4-9  21.4G  Unknown  N/A    N/A    No
ceph-3    /dev/vdb  hdd   67db6403-5d8d-417f-8  21.4G  Unknown  N/A    N/A    No
ceph-4    /dev/vdb  hdd   931038cb-7a6e-4317-b  21.4G  Unknown  N/A    N/A    No
ceph-5    /dev/vdb  hdd   0a25d009-f18f-49ba-9  21.4G  Unknown  N/A    N/A    No
</code></pre><p>查看osd pool列表，默认有一个device_health_metrics的pool</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
</code></pre><p>验证块存储rbd挂载</p>
<pre tabindex="0"><code>#1.创建一个test的pool
[root@ceph-1 ~]# ceph osd pool create test 64 64
pool 'test' created

#2.创建块设备test1,容量为1G
[root@ceph-1 ~]# rbd create test/test1 --size 1024

#3.查看块设备test1详情
[root@ceph-1 ~]# rbd info test/test1
rbd image 'test1':
	size 1 GiB in 256 objects
	order 22 (4 MiB objects)
	snapshot_count: 0
	id: 3a209142feac
	block_name_prefix: rbd_data.3a209142feac
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features:
	flags:
	create_timestamp: Fri May 21 02:15:19 2021
	access_timestamp: Fri May 21 02:15:19 2021
	modify_timestamp: Fri May 21 02:15:19 2021

#4.映射块设备test1，默认内核不支持rbd一些特性
[root@ceph-1 ~]# rbd map test/test1
rbd: sysfs write failed
RBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable test/test1 object-map fast-diff deep-flatten&quot;.
In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.
rbd: map failed: (6) No such device or address

[root@ceph-1 ~]# rbd feature disable test/test1 object-map fast-diff deep-flatten

[root@ceph-1 ~]# rbd map test/test1
/dev/rbd0

#5.格式化并挂载
[root@ceph-1 ~]# mkfs.xfs /dev/rbd0
meta-data=/dev/rbd0              isize=512    agcount=8, agsize=32768 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=262144, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
[root@ceph-1 ~]# mount /dev/rbd0 /mnt/

#6.测试读写
[root@ceph-1 ~]# echo &quot;test&quot; &gt; /mnt/test
[root@ceph-1 ~]# cat /mnt/test
test

#7.清理环境
[root@ceph-1 ~]# umount /mnt/
[root@ceph-1 ~]# rbd unmap test/test1
[root@ceph-1 ~]# rbd rm test/test1
Removing image: 100% complete...done.
</code></pre><h4 id="329-部署mds">3.2.9 部署mds</h4>
<p>指定节点部署mds</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph orch apply mds cephfs --placement=&quot;3 ceph-1 ceph-2 ceph-3&quot;
Scheduled mds.cephfs update...
</code></pre><p>cephfs需要两个存储池，一个存储文件，另一个存储文件的元数据</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph osd pool create cephfs_data 128 128
pool 'cephfs_data' created
[root@ceph-1 ~]# ceph osd pool create cephfs_metadata 128 128
pool 'cephfs_metadata' created
</code></pre><p>手动创建cephfs</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph fs new cephfs cephfs_metadata cephfs_data
new fs with metadata pool 4 and data pool 3
</code></pre><p>查看cephfs</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
</code></pre><p>挂载cephfs</p>
<pre tabindex="0"><code>[root@ceph-3 ~]# mkdir -p /mnt/cephfs/
[root@ceph-3 ~]# mount -t ceph 172.16.80.45:/ /mnt/cephfs -o name=admin,secret=AQBrBKZgarHxAxAAQm2TL1KM3gkW7zr5A2yZgw==
[root@ceph-3 ~]# mount |grep cephfs
172.16.80.45:/ on /mnt/cephfs type ceph (rw,relatime,name=admin,secret=&lt;hidden&gt;,acl,wsize=16777216)
</code></pre><h4 id="3210-部署rgws">3.2.10 部署rgws</h4>
<p>cephadm将radosgw部署为管理特定领域和区域的守护程序的集合，创建rgw的realm、zonegroup、zone(&ndash;default设置默认值)</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# radosgw-admin realm create --rgw-realm=testrgw --default
{
    &quot;id&quot;: &quot;359ff5f9-db95-493a-91d8-90c4b626c848&quot;,
    &quot;name&quot;: &quot;testrgw&quot;,
    &quot;current_period&quot;: &quot;a0c7dc86-9070-4ffc-affd-d8e953df6949&quot;,
    &quot;epoch&quot;: 1
}

[root@ceph-1 ~]# radosgw-admin zonegroup create --rgw-zonegroup=testzone --master --default
{
    &quot;id&quot;: &quot;3029b614-c16a-4086-9db4-f1a2ba356c0b&quot;,
    &quot;name&quot;: &quot;testzone&quot;,
    &quot;api_name&quot;: &quot;testzone&quot;,
    &quot;is_master&quot;: &quot;true&quot;,
    &quot;endpoints&quot;: [],
    &quot;hostnames&quot;: [],
    &quot;hostnames_s3website&quot;: [],
    &quot;master_zone&quot;: &quot;&quot;,
    &quot;zones&quot;: [],
    &quot;placement_targets&quot;: [],
    &quot;default_placement&quot;: &quot;&quot;,
    &quot;realm_id&quot;: &quot;359ff5f9-db95-493a-91d8-90c4b626c848&quot;,
    &quot;sync_policy&quot;: {
        &quot;groups&quot;: []
    }
}

[root@ceph-1 ~]# radosgw-admin zone create --rgw-zonegroup=testzone --rgw-zone=zone1 --master --default
{
    &quot;id&quot;: &quot;0f3caf93-05bc-49cd-8fd3-31fce9dbab48&quot;,
    &quot;name&quot;: &quot;zone1&quot;,
    &quot;domain_root&quot;: &quot;zone1.rgw.meta:root&quot;,
    &quot;control_pool&quot;: &quot;zone1.rgw.control&quot;,
    &quot;gc_pool&quot;: &quot;zone1.rgw.log:gc&quot;,
    &quot;lc_pool&quot;: &quot;zone1.rgw.log:lc&quot;,
    &quot;log_pool&quot;: &quot;zone1.rgw.log&quot;,
    &quot;intent_log_pool&quot;: &quot;zone1.rgw.log:intent&quot;,
    &quot;usage_log_pool&quot;: &quot;zone1.rgw.log:usage&quot;,
    &quot;roles_pool&quot;: &quot;zone1.rgw.meta:roles&quot;,
    &quot;reshard_pool&quot;: &quot;zone1.rgw.log:reshard&quot;,
    &quot;user_keys_pool&quot;: &quot;zone1.rgw.meta:users.keys&quot;,
    &quot;user_email_pool&quot;: &quot;zone1.rgw.meta:users.email&quot;,
    &quot;user_swift_pool&quot;: &quot;zone1.rgw.meta:users.swift&quot;,
    &quot;user_uid_pool&quot;: &quot;zone1.rgw.meta:users.uid&quot;,
    &quot;otp_pool&quot;: &quot;zone1.rgw.otp&quot;,
    &quot;system_key&quot;: {
        &quot;access_key&quot;: &quot;&quot;,
        &quot;secret_key&quot;: &quot;&quot;
    },
    &quot;placement_pools&quot;: [
        {
            &quot;key&quot;: &quot;default-placement&quot;,
            &quot;val&quot;: {
                &quot;index_pool&quot;: &quot;zone1.rgw.buckets.index&quot;,
                &quot;storage_classes&quot;: {
                    &quot;STANDARD&quot;: {
                        &quot;data_pool&quot;: &quot;zone1.rgw.buckets.data&quot;
                    }
                },
                &quot;data_extra_pool&quot;: &quot;zone1.rgw.buckets.non-ec&quot;,
                &quot;index_type&quot;: 0
            }
        }
    ],
    &quot;realm_id&quot;: &quot;359ff5f9-db95-493a-91d8-90c4b626c848&quot;
}
</code></pre><p>指定节点创建rgw</p>
<pre tabindex="0"><code># ceph orch apply rgw &lt;realm_name&gt; &lt;zone_name&gt; [&lt;subcluster&gt;] [&lt;port: Update the number of RGW instances for the given zone int&gt;] [--ssl] [&lt;placement&gt;] [--dry-run] [plain|json|json-pretty|yaml] [--unmanaged]
[root@ceph-1 ~]# ceph orch apply rgw testrgw mr-1 --placement=&quot;2 ceph-4 ceph-5&quot;
Scheduled rgw.testrgw.mr-1 update...
</code></pre><p>配置ceph dashboard能够查看rgw集群信息，因为rgw有自己的一套账号体系，在rgw中创建一个dashboard的账号</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# radosgw-admin user create --uid=rgw --display-name=rgw --system
{
    &quot;user_id&quot;: &quot;rgw&quot;,
    &quot;display_name&quot;: &quot;rgw&quot;,
    &quot;email&quot;: &quot;&quot;,
    &quot;suspended&quot;: 0,
    &quot;max_buckets&quot;: 1000,
    &quot;subusers&quot;: [],
    &quot;keys&quot;: [
        {
            &quot;user&quot;: &quot;rgw&quot;,
            &quot;access_key&quot;: &quot;45FW7W30MHJOH8633464&quot;,
            &quot;secret_key&quot;: &quot;DIvMiQl3AzllCSyEEn8TuoDFCKaHFt7QqGMXzqnU&quot;
        }
    ],
    &quot;swift_keys&quot;: [],
    &quot;caps&quot;: [],
    &quot;op_mask&quot;: &quot;read, write, delete&quot;,
    &quot;system&quot;: &quot;true&quot;,
    &quot;default_placement&quot;: &quot;&quot;,
    &quot;default_storage_class&quot;: &quot;&quot;,
    &quot;placement_tags&quot;: [],
    &quot;bucket_quota&quot;: {
        &quot;enabled&quot;: false,
        &quot;check_on_raw&quot;: false,
        &quot;max_size&quot;: -1,
        &quot;max_size_kb&quot;: 0,
        &quot;max_objects&quot;: -1
    },
    &quot;user_quota&quot;: {
        &quot;enabled&quot;: false,
        &quot;check_on_raw&quot;: false,
        &quot;max_size&quot;: -1,
        &quot;max_size_kb&quot;: 0,
        &quot;max_objects&quot;: -1
    },
    &quot;temp_url_keys&quot;: [],
    &quot;type&quot;: &quot;rgw&quot;,
    &quot;mfa_ids&quot;: []
}
</code></pre><pre tabindex="0"><code>[root@ceph-1 ~]# echo -n &quot;45FW7W30MHJOH8633464&quot; &gt; file-containing-access-key
[root@ceph-1 ~]# echo -n &quot;DIvMiQl3AzllCSyEEn8TuoDFCKaHFt7QqGMXzqnU&quot; &gt; file-containing-secret-key

[root@ceph-1 ~]# ceph dashboard set-rgw-api-access-key -i file-containing-access-key
Option RGW_API_ACCESS_KEY updated
[root@ceph-1 ~]# ceph dashboard set-rgw-api-secret-key -i file-containing-secret-key
Option RGW_API_SECRET_KEY updated
</code></pre><p>配置完后，ceph dashboard上就可以操作Object Gateway了</p>
<p>安装s3cmd命令行客户端</p>
<pre tabindex="0"><code>[root@ceph-3 ~]# yum install s3cmd -y
</code></pre><p>配置s3cmd（172.16.80.90:80是radosgw监听的地址端口）</p>
<pre tabindex="0"><code>[root@ceph-3 ~]# s3cmd --configure

Enter new values or accept defaults in brackets with Enter.
Refer to user manual for detailed description of all options.

Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.
Access Key: 45FW7W30MHJOH8633464
Secret Key: DIvMiQl3AzllCSyEEn8TuoDFCKaHFt7QqGMXzqnU
Default Region [US]:

Use &quot;s3.amazonaws.com&quot; for S3 Endpoint and not modify it to the target Amazon S3.
S3 Endpoint [s3.amazonaws.com]: 172.16.80.90:80

Use &quot;%(bucket)s.s3.amazonaws.com&quot; to the target Amazon S3. &quot;%(bucket)s&quot; and &quot;%(location)s&quot; vars can be used
if the target S3 system supports dns based buckets.
DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: 172.16.80.90:80/%(bucket)s

Encryption password is used to protect your files from reading
by unauthorized persons while in transfer to S3
Encryption password:
Path to GPG program [/usr/bin/gpg]:

When using secure HTTPS protocol all communication with Amazon S3
servers is protected from 3rd party eavesdropping. This method is
slower than plain HTTP, and can only be proxied with Python 2.7 or newer
Use HTTPS protocol [Yes]: no

On some networks all internet access must go through a HTTP proxy.
Try setting it here if you can't connect to S3 directly
HTTP Proxy server name:

New settings:
  Access Key: 45FW7W30MHJOH8633464
  Secret Key: DIvMiQl3AzllCSyEEn8TuoDFCKaHFt7QqGMXzqnU
  Default Region: US
  S3 Endpoint: 172.16.80.90:80
  DNS-style bucket+hostname:port template for accessing a bucket: 172.16.80.90:80/%(bucket)s
  Encryption password:
  Path to GPG program: /usr/bin/gpg
  Use HTTPS protocol: False
  HTTP Proxy server name:
  HTTP Proxy server port: 0

Test access with supplied credentials? [Y/n] y
Please wait, attempting to list all buckets...
Success. Your access key and secret key worked fine :-)

Now verifying that encryption works...
Not configured. Never mind.

Save settings? [y/N] y
Configuration saved to '/root/.s3cfg'
</code></pre><p>启用signature_v2</p>
<pre tabindex="0"><code># vim /root/.s3cfg
signature_v2 = True
</code></pre><p>验证rgw</p>
<pre tabindex="0"><code>#1.创建buckets
[root@ceph-3 ~]# s3cmd mb s3://test
Bucket 's3://test/' created

#2.查看buckets
[root@ceph-3 ~]# s3cmd ls
2021-05-21 12:19  s3://test

#3.上传文件到buckets
[root@ceph-3 ~]# s3cmd put /var/log/messages s3://test/messages
upload: '/var/log/messages' -&gt; 's3://test/messages'  [part 1 of 3, 15MB] [1 of 1]
 15728640 of 15728640   100% in    2s     6.02 MB/s  done
upload: '/var/log/messages' -&gt; 's3://test/messages'  [part 2 of 3, 15MB] [1 of 1]
 15728640 of 15728640   100% in    0s    28.72 MB/s  done
upload: '/var/log/messages' -&gt; 's3://test/messages'  [part 3 of 3, 5MB] [1 of 1]
 5290039 of 5290039   100% in    0s    16.92 MB/s  done

#4. 上传目录到buckets
[root@ceph-3 ~]# s3cmd put /etc/yum.repos.d/ --recursive  --recursive s3://test/yum/
upload: '/etc/yum.repos.d/CentOS-Base.repo' -&gt; 's3://test/yum/CentOS-Base.repo'  [1 of 11]
 1664 of 1664   100% in    0s    57.66 KB/s  done
upload: '/etc/yum.repos.d/CentOS-CR.repo' -&gt; 's3://test/yum/CentOS-CR.repo'  [2 of 11]
 1309 of 1309   100% in    0s    22.59 KB/s  done
upload: '/etc/yum.repos.d/CentOS-Debuginfo.repo' -&gt; 's3://test/yum/CentOS-Debuginfo.repo'  [3 of 11]
 649 of 649   100% in    0s    10.49 KB/s  done
upload: '/etc/yum.repos.d/CentOS-Media.repo' -&gt; 's3://test/yum/CentOS-Media.repo'  [4 of 11]
 630 of 630   100% in    0s    10.95 KB/s  done
upload: '/etc/yum.repos.d/CentOS-Sources.repo' -&gt; 's3://test/yum/CentOS-Sources.repo'  [5 of 11]
 1331 of 1331   100% in    0s    22.93 KB/s  done
upload: '/etc/yum.repos.d/CentOS-Vault.repo' -&gt; 's3://test/yum/CentOS-Vault.repo'  [6 of 11]
 7577 of 7577   100% in    0s   461.08 KB/s  done
upload: '/etc/yum.repos.d/CentOS-fasttrack.repo' -&gt; 's3://test/yum/CentOS-fasttrack.repo'  [7 of 11]
 314 of 314   100% in    0s     5.26 KB/s  done
upload: '/etc/yum.repos.d/CentOS-x86_64-kernel.repo' -&gt; 's3://test/yum/CentOS-x86_64-kernel.repo'  [8 of 11]
 616 of 616   100% in    0s    10.64 KB/s  done
upload: '/etc/yum.repos.d/docker-ce.repo' -&gt; 's3://test/yum/docker-ce.repo'  [9 of 11]
 1919 of 1919   100% in    0s   122.17 KB/s  done
upload: '/etc/yum.repos.d/epel-testing.repo' -&gt; 's3://test/yum/epel-testing.repo'  [10 of 11]
 1050 of 1050   100% in    0s    18.65 KB/s  done
upload: '/etc/yum.repos.d/epel.repo' -&gt; 's3://test/yum/epel.repo'  [11 of 11]
 951 of 951   100% in    0s    16.75 KB/s  done

#5.查看buckets中的文件
[root@ceph-3 ~]# s3cmd ls s3://test/
                          DIR  s3://test/yum/
2021-05-21 12:31     36747319  s3://test/messages

#6.下载文件
[root@ceph-3 ~]# s3cmd get s3://test/messages msg.bak
download: 's3://test/messages' -&gt; 'msg.bak'  [1 of 1]
 36747319 of 36747319   100% in    0s   123.63 MB/s  done
WARNING: MD5 signatures do not match: computed=d8b57ce97cc383266c68c86abe25efe3, received=83622893dbb2a16958423e3d97c74269

#7.下载目录
[root@ceph-3 ~]# mkdir -p test
[root@ceph-3 ~]# s3cmd get --recursive s3://test/yum/ test/
download: 's3://test/yum/CentOS-Base.repo' -&gt; 'test/CentOS-Base.repo'  [1 of 11]
 1664 of 1664   100% in    0s   215.95 KB/s  done
download: 's3://test/yum/CentOS-CR.repo' -&gt; 'test/CentOS-CR.repo'  [2 of 11]
 1309 of 1309   100% in    0s    29.41 KB/s  done
download: 's3://test/yum/CentOS-Debuginfo.repo' -&gt; 'test/CentOS-Debuginfo.repo'  [3 of 11]
 649 of 649   100% in    0s    14.67 KB/s  done
download: 's3://test/yum/CentOS-Media.repo' -&gt; 'test/CentOS-Media.repo'  [4 of 11]
 630 of 630   100% in    0s    13.91 KB/s  done
download: 's3://test/yum/CentOS-Sources.repo' -&gt; 'test/CentOS-Sources.repo'  [5 of 11]
 1331 of 1331   100% in    0s    29.97 KB/s  done
download: 's3://test/yum/CentOS-Vault.repo' -&gt; 'test/CentOS-Vault.repo'  [6 of 11]
 7577 of 7577   100% in    0s   581.35 KB/s  done
download: 's3://test/yum/CentOS-fasttrack.repo' -&gt; 'test/CentOS-fasttrack.repo'  [7 of 11]
 314 of 314   100% in    0s     7.01 KB/s  done
download: 's3://test/yum/CentOS-x86_64-kernel.repo' -&gt; 'test/CentOS-x86_64-kernel.repo'  [8 of 11]
 616 of 616   100% in    0s    13.64 KB/s  done
download: 's3://test/yum/docker-ce.repo' -&gt; 'test/docker-ce.repo'  [9 of 11]
 1919 of 1919   100% in    0s   150.19 KB/s  done
download: 's3://test/yum/epel-testing.repo' -&gt; 'test/epel-testing.repo'  [10 of 11]
 1050 of 1050   100% in    0s    24.02 KB/s  done
download: 's3://test/yum/epel.repo' -&gt; 'test/epel.repo'  [11 of 11]
 951 of 951   100% in    0s    21.62 KB/s  done

[root@ceph-3 ~]# ll test/
total 48
-rw-r--r--. 1 root root 1664 May 21 12:31 CentOS-Base.repo
-rw-r--r--. 1 root root 1309 May 21 12:31 CentOS-CR.repo
-rw-r--r--. 1 root root  649 May 21 12:31 CentOS-Debuginfo.repo
-rw-r--r--. 1 root root  314 May 21 12:31 CentOS-fasttrack.repo
-rw-r--r--. 1 root root  630 May 21 12:31 CentOS-Media.repo
-rw-r--r--. 1 root root 1331 May 21 12:31 CentOS-Sources.repo
-rw-r--r--. 1 root root 7577 May 21 12:31 CentOS-Vault.repo
-rw-r--r--. 1 root root  616 May 21 12:31 CentOS-x86_64-kernel.repo
-rw-r--r--. 1 root root 1919 May 21 12:31 docker-ce.repo
-rw-r--r--. 1 root root  951 May 21 12:31 epel.repo
-rw-r--r--. 1 root root 1050 May 21 12:31 epel-testing.repo

#8.bucket上传文件后自动新建一个mr-1.rgw.buckets.data的存储池
[root@ceph-1 ~]# ceph osd lspools
1 device_health_metrics
2 cinder
3 cephfs_data
4 cephfs_metadata
5 test
6 nfs-ganesha
7 .rgw.root
8 mr-1.rgw.log
9 mr-1.rgw.control
10 mr-1.rgw.meta
11 mr-1.rgw.buckets.index
12 mr-1.rgw.buckets.non-ec
13 mr-1.rgw.buckets.data

#9.查看mr-1.rgw.buckets.data存储池中的内容
[root@ceph-1 ~]# rados -p  mr-1.rgw.buckets.data ls
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_yum/CentOS-Debuginfo.repo
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_yum/CentOS-x86_64-kernel.repo
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2__shadow_messages.2~uulO03Jct94mKYXEd4oQs-8MLKQIPj4.3_1
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2__multipart_messages.2~uulO03Jct94mKYXEd4oQs-8MLKQIPj4.3
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2__multipart_messages.2~uulO03Jct94mKYXEd4oQs-8MLKQIPj4.1
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2__shadow_messages.2~uulO03Jct94mKYXEd4oQs-8MLKQIPj4.2_1
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_yum/CentOS-CR.repo
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_yum/CentOS-Base.repo
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_yum/CentOS-Vault.repo
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_yum/CentOS-fasttrack.repo
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2__shadow_messages.2~uulO03Jct94mKYXEd4oQs-8MLKQIPj4.1_3
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2__shadow_messages.2~uulO03Jct94mKYXEd4oQs-8MLKQIPj4.2_2
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2__shadow_messages.2~uulO03Jct94mKYXEd4oQs-8MLKQIPj4.1_1
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2__shadow_messages.2~uulO03Jct94mKYXEd4oQs-8MLKQIPj4.1_2
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_messages
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_yum/CentOS-Media.repo
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2__shadow_messages.2~uulO03Jct94mKYXEd4oQs-8MLKQIPj4.2_3
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2__multipart_messages.2~uulO03Jct94mKYXEd4oQs-8MLKQIPj4.2
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_yum/docker-ce.repo
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_yum/epel-testing.repo
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_yum/CentOS-Sources.repo
492ef619-1d17-4cdd-a691-c8c11cf14fde.24617.2_yum/epel.repo

#10.删除bucket中的文件
[root@ceph-3 ~]# s3cmd rm s3://test/messages
delete: 's3://test/messages'

#11.删除bucket中的目录
[root@ceph-3 ~]# s3cmd rm --recursive s3://test/yum/
delete: 's3://test/yum/CentOS-Base.repo'
delete: 's3://test/yum/CentOS-CR.repo'
delete: 's3://test/yum/CentOS-Debuginfo.repo'
delete: 's3://test/yum/CentOS-Media.repo'
delete: 's3://test/yum/CentOS-Sources.repo'
delete: 's3://test/yum/CentOS-Vault.repo'
delete: 's3://test/yum/CentOS-fasttrack.repo'
delete: 's3://test/yum/CentOS-x86_64-kernel.repo'
delete: 's3://test/yum/docker-ce.repo'
delete: 's3://test/yum/epel-testing.repo'
delete: 's3://test/yum/epel.repo'

#12.删除bucket
[root@ceph-3 ~]# s3cmd rb s3://test
Bucket 's3://test/' removed
</code></pre><h4 id="3211-部署nfs-ganesha">3.2.11 部署nfs-ganesha</h4>
<p>ceph推荐使用nfs-ganesha来提供nfs服务</p>
<p>创建nfs池</p>
<pre tabindex="0"><code># ceph osd pool create nfs-ganesha 64 64
</code></pre><p>创建nfs-ganesha</p>
<pre tabindex="0"><code>[root@ceph-1 ~]# ceph orch apply nfs nfs nfs-ganesha nfs-ns --placement=&quot;3 ceph-1 ceph-2 ceph-3&quot;
Scheduled nfs.nfs update...

#启动application，不然会有warning告警
[root@ceph-1 ~]# ceph osd pool application enable nfs-ganesha nfs
enabled application 'nfs' on pool 'nfs-ganesha'

[root@ceph-1 ~]# ceph dashboard set-ganesha-clusters-rados-pool-namespace nfs-ganesha/nfs-ns
Option GANESHA_CLUSTERS_RADOS_POOL_NAMESPACE updated
</code></pre><p>cephfs的nfs是基于cephfs提供，所以在cephfs中创建一个/nfs目录，作为nfs服务的根目录</p>
<pre tabindex="0"><code>#1. 确保已挂载cephfs至/mnt/cephfs目录前提下
[root@ceph-3 ~]# mkdir -p /mnt/cephfs/nfs
#2. 登录https://172.16.80.45:8443/#/nfs，界面上创建nfs export
#3. 挂载nfs
[root@ceph-5 ~]# mount -t nfs 172.16.80.45:/nfs /mnt/
</code></pre><h2 id="云管">云管</h2>
<ul>
<li><a href="https://github.com/platform9/openstack-omni">https://github.com/platform9/openstack-omni</a> 提供一套标准的API管理混合和多云环境</li>
</ul>
<h2 id="5-参考链接">5. 参考链接</h2>
<ul>
<li><a href="https://docs.openstack.org/install-guide/">OpenStack Installation Guide</a></li>
<li><a href="https://docs.ceph.com/en/latest/install/">INSTALLING CEPH</a></li>
<li><a href="https://blog.csdn.net/networken/article/details/106870859">cephadm部署ceph集群</a></li>
<li><a href="https://blog.csdn.net/get_set/article/details/108092248">CentOS8使用cephadm部署和配置Ceph Octopus</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/77897908">跳出云管看云管</a></li>
</ul>


                
                
<div class="entry-shang text-center">
    
	    <p>「真诚赞赏，手留余香」</p>
	
	<button class="zs show-zs btn btn-bred">赞赏支持</button>
</div>
<div class="zs-modal-bg"></div>
<div class="zs-modal-box">
	<div class="zs-modal-head">
		<button type="button" class="close">×</button>
		<span class="author"><a href="https://www.iceyao.com.cn"><img src="/img/favicon.png" />爱折腾的工程师</a></span>
        
	        <p class="tip"><i></i><span>真诚赞赏，手留余香</span></p>
		
 
	</div>
	<div class="zs-modal-body">
		<div class="zs-modal-btns">
			<button class="btn btn-blink" data-num="2">2元</button>
			<button class="btn btn-blink" data-num="5">5元</button>
			<button class="btn btn-blink" data-num="10">10元</button>
			<button class="btn btn-blink" data-num="50">50元</button>
			<button class="btn btn-blink" data-num="100">100元</button>
			<button class="btn btn-blink" data-num="1">任意金额</button>
		</div>
		<div class="zs-modal-pay">
			<button class="btn btn-bred" id="pay-text">2元</button>
			<p>使用<span id="pay-type">微信</span>扫描二维码完成支付</p>
			<img src="/img/reward/wechat-2.png"  id="pay-image"/>
		</div>
	</div>
	<div class="zs-modal-footer">
		<label><input type="radio" name="zs-type" value="wechat" class="zs-type" checked="checked"><span ><span class="zs-wechat"><img src="/img/reward/wechat-btn.png"/></span></label>
		<label><input type="radio" name="zs-type" value="alipay" class="zs-type" class="zs-alipay"><img src="/img/reward/alipay-btn.png"/></span></label>
	</div>
</div>
<script type="text/javascript" src="/js/reward.js"></script>

                

                
                <hr>
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/post/2021-04-02-common_pass/" data-toggle="tooltip" data-placement="top" title="常用中间件服务部署方案">&larr;
                            Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/post/2021-07-19-container_network_development/" data-toggle="tooltip" data-placement="top" title="容器网络发展">Next
                            Post &rarr;</a>
                    </li>
                    
                </ul>
                

                


<script src="https://giscus.app/client.js"
        data-repo="yaoice/yaoice.github.io"
        data-repo-id="R_kgDOJnxqVg"
        data-category="General"
        data-category-id="DIC_kwDOJnxqVs4CWwUs"
        data-mapping="pathname"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>


            </div>

            
            
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
            

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                        
                        
                        
                        
                        
                        <a href="/tags/devops" title="devops">
                            devops
                        </a>
                        
                        
                        
                        <a href="/tags/go" title="go">
                            go
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/k8s" title="k8s">
                            k8s
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/llm" title="llm">
                            llm
                        </a>
                        
                        
                        
                        <a href="/tags/openstack" title="openstack">
                            openstack
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/tkestack" title="tkestack">
                            tkestack
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E7%BB%83%E8%BD%A6" title="练车">
                            练车
                        </a>
                        
                        
                    </div>
                </section>
                

                
                
            </div>
        </div>
    </div>
</article>




<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">                  
                    
                    <li>
                        <a href="mailto:yao3690093@gmail.com">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		           
                    
                    
                    
                    

		            
                    
                    <li>
                        <a target="_blank" href="/img/wechat.jpeg">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-weixin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		            
                    
                    <li>
                        <a target="_blank" href="https://github.com/yaoice">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		            
                    
                    
                    
                    
                    
                    
                    
                    
                    
            
            
            
           
                   <li>
                       <a href='' rel="alternate" type="application/rss+xml" title="爱折腾的工程师" >
                           <span class="fa-stack fa-lg">
                               <i class="fas fa-circle fa-stack-2x"></i>
                               <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                           </span>
                       </a>
                   </li>
            
             </ul>
		<p class="copyright text-muted">
                    Copyright &copy; 爱折腾的工程师 2024
                </p>
            </div>
        </div>
    </div>
</footer>




<script>
    function loadAsync(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>






<script>
    
    if($('#tag_cloud').length !== 0){
        loadAsync("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>


<script>
    loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>



<script>
    (function(){
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https'){
       bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else{
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>


<script>
    
    var _baId = '92c175994ded75a3cd2074bc1123e2be';

    
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>




<script type="text/javascript">
    function generateCatalog(selector) {

        
        
        
        
            _containerSelector = 'div.post-container'
        

        
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        
        $(selector).html('')

        
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    


    loadAsync("\/js\/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>






</body>
</html>
