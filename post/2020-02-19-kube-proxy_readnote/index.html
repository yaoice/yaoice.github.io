<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="爱折腾的工程师"><meta property="og:type" content="article"><meta property="og:image" content="https://www.iceyao.com.cn//img/post-bg-unix-linux.jpg"><meta property="twitter:image" content="https://www.iceyao.com.cn//img/post-bg-unix-linux.jpg"><meta name=title content="kube-proxy源码阅读笔记"><meta property="og:title" content="kube-proxy源码阅读笔记"><meta property="twitter:title" content="kube-proxy源码阅读笔记"><meta name=description content="iceyao，程序员, 开源爱好者，生活探险家 | 这里是iceyao的博客，与你一起发现更大的世界。"><meta property="og:description" content="iceyao，程序员, 开源爱好者，生活探险家 | 这里是iceyao的博客，与你一起发现更大的世界。"><meta property="twitter:description" content="iceyao，程序员, 开源爱好者，生活探险家 | 这里是iceyao的博客，与你一起发现更大的世界。"><meta property="twitter:card" content="summary"><meta name=keyword content="iceyao, IceYao's Blog, 博客, 个人网站, 互联网, Web, 云原生, PaaS, Istio, Kubernetes, 微服务, Microservice"><link rel="shortcut icon" href=/img/favicon.ico><title>kube-proxy源码阅读笔记 | 爱折腾的工程师 | IceYao's Blog</title>
<link rel=canonical href=/post/2020-02-19-kube-proxy_readnote/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-9J7CKFVPPM"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-9J7CKFVPPM")}</script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/>爱折腾的工程师</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/notes//>NOTES</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(/img/post-bg-unix-linux.jpg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/k8s title=k8s>k8s</a></div><h1>kube-proxy源码阅读笔记</h1><h2 class=subheading></h2><span class=meta>Posted by
爱折腾的工程师
on
Wednesday, February 19, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h3 id=环境>环境</h3><ul><li>Kubernetes v1.14.6</li><li>Etcd 3.3.12</li><li>Docker 18.09.9</li></ul><h3 id=k8s-service>K8s service</h3><p>service是一堆相同label pod集合的抽象，通过selector标签来匹配pod;
服务间访问也通过service(域名)，用service可以避免pod ip迁移发生变化；service对象也会对应多个
endpoint对象，endpoint用于服务发现；无论是service、endpoint都是逻辑概念，真正路由转发是
kube-proxy，通常以daemonset形式运行于集群中，kube-proxy会watch service、endpoint变化，
生成/改变宿主机上的iptables/ipvs转发规则</p><p>service类型：</p><ul><li>ClusterIP(默认)</li><li>NodePort</li><li>LoadBalancer</li><li>ExternelName</li></ul><p>headless service: 没有clusterIP的service, 给应用集群的每个pod分配一个dns域名，pod之间
用该域名直接访问，mysql/redis等集群都是这种方式</p><p>提下ingress, ingress不是service类型，作用于7层，是外部访问内部应用的入口；
不同的域名/路径来匹配不同的service</p><h3 id=kube-proxy模式>kube-proxy模式</h3><p>kube-proxy模式：</p><ul><li>userspace(这里不介绍，有了iptables)</li><li>iptables(userspace的改进)</li><li>ipvs</li><li>kernelspace(windows下使用)</li></ul><p>IPVS vs. IPTABLES:</p><p>IPVS模式在Kubernetes v1.8中引入，在v1.9中处于beta版本，在v1.11中处于GA版本。
IPTABLES模式已在v1.1中添加，并成为v1.2之后的默认操作模式。 IPVS和IPTABLES均基于netfilter。</p><p>IPVS模式和IPTABLES模式之间的差异如下：</p><ol><li>IPVS为大型群集提供了更好的可伸缩性和性能。</li><li>与IPTABLES相比，IPVS支持更复杂的负载平衡算法（最低负载，最少连接，局部性，加权等）。</li><li>IPVS支持服务器运行状况检查和连接重试等。</li><li>底层数据结构：IPTABLES使用链表，IPVS使用哈希表</li></ol><h4 id=iptables模式>iptables模式</h4><p>iptables 5张表5条链</p><p>5张表:</p><ul><li>filter: 过滤某个链上的数据包</li><li>nat: 网络地址转换，转换数据包的源地址和目的地址</li><li>mangle: 修改数据包的IP头部信息</li><li>raw: iptables本身是有状态的，对数据包有链接追踪(/proc/net/nf_conntrack可看到)，raw
可以用来消除这种追踪机制</li><li>security: 数据包上使用selinux</li></ul><p>5条链:</p><ul><li>PREROUTING: 数据包进入内核网络模块之后, 获得路由之前(可进行DNAT)</li><li>INPUT: 数据包被决定路由到本机之后, 一般处理本地进程的数据包, 目的地址是本机</li><li>FORWARD: 数据包被决定路由到其他主机之后, 一般处理转发到其它主机/网络命名空间的数据包</li><li>OUTPUT: 离开本机的数据包进入内核网络模块之后, 一般处理本地进程的输出数据包, 源地址是本机</li><li>POSTROUTING: 对于离开本机或者FORWARD的数据包, 当数据包被发送到网卡之前(可进行SNAT)</li></ul><p>iptables的表和链是有顺序的，工作流程如下:</p><p><img src=/img/posts/2020-02-19/1.png alt></p><p>kube-proxy iptables模式使用了filter和nat表，并定义了很多自定义链</p><p>filter表自定义链:</p><ul><li>KUBE-SERVICES</li><li>KUBE-EXTERNAL-SERVICES</li><li>KUBE-FIREWALL</li></ul><p>nat表自定义链:</p><ul><li>KUBE-HOSTPORTS</li><li>KUBE-SERVICES</li><li>KUBE-POSTROUTING</li><li>KUBE-MARK-DROP</li><li>KUBE-MARK-MASQ</li><li>KUBE-NODEPORTS</li><li>KUBE-SVC-XXX</li><li>KUBE-SEP-XXX</li></ul><p>具体来看kube-proxy生成的iptables规则, 以一个httpbin服务为例</p><p>httpbin的k8s manifest</p><pre tabindex=0><code># vim httpbin.yaml 
---
apiVersion: v1
kind: Service
metadata:
  name: httpbin
  labels:
    app: httpbin
spec:
  ports:
  - name: http
    port: 80
    targetPort: 80
  selector:
    app: httpbin
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpbin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: httpbin
  template:
    metadata:
      labels:
        app: httpbin
    spec:
      containers:
      - image: docker.io/kennethreitz/httpbin
        name: httpbin
        ports:
        - containerPort: 80
</code></pre><pre tabindex=0><code># kubectl apply -f httpbin.yaml 
service/httpbin created
deployment.apps/httpbin created
</code></pre><pre tabindex=0><code># kubectl get svc -l app=httpbin
NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
httpbin   ClusterIP   172.20.255.90   &lt;none&gt;        80/TCP    2m38s

# kubectl get pod -l app=httpbin
NAME                     READY   STATUS    RESTARTS   AGE
httpbin-896cb847-vwpg2   1/1     Running   0          2m45s
</code></pre><p>clusterIP为172.20.255.90，pod单副本</p><h5 id=clusterip-service类型>clusterIP service类型</h5><p>pod间访问clusterIP的iptables规则链顺序为：(对照上面的iptables工作流程图)</p><pre tabindex=0><code>PREROUTING -&gt; KUBE-SERVICES -&gt; KUBE-SVC-XXX -&gt; KUBE-SEP-XXX
</code></pre><pre tabindex=0><code>1. 进入PREROUTING的规则都转发到KUBE-SERVICES, 不符合KUBE-HOSTPORTS的条件
-A PREROUTING -m comment --comment &#34;kube hostport portals&#34; -m addrtype --dst-type LOCAL -j KUBE-HOSTPORTS
-A PREROUTING -m comment --comment &#34;kubernetes service portals&#34; -j KUBE-SERVICES

2. 访问clusterIP为172.20.255.90的都转发至KUBE-SVC-FREKB6WNWYJLKTHC
-A KUBE-SERVICES -d 172.20.255.90/32 -p tcp -m comment --comment &#34;default/httpbin:http cluster IP&#34; -m tcp --dport 80 -j KUBE-SVC-FREKB6WNWYJLKTHC

3. 进入KUBE-SVC-FREKB6WNWYJLKTHC的都转发至KUBE-SEP-PEA6WHECIZEOX47B
-A KUBE-SVC-FREKB6WNWYJLKTHC -j KUBE-SEP-PEA6WHECIZEOX47B

4. KUBE-SEP-PEA6WHECIZEOX47B对应endpoint中的172.20.0.40:80
-A KUBE-SEP-PEA6WHECIZEOX47B -s 172.20.0.40/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-PEA6WHECIZEOX47B -p tcp -m tcp -j DNAT --to-destination 172.20.0.40:80

# 0x4000是kube-proxy的数据包放行标记, 0x8000是数据包丢弃标记
-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
</code></pre><p>httpbin pod副本数扩到2，观察iptables KUBE-SVC-，KUBE-SEP-链的变化</p><pre tabindex=0><code># kubectl scale deployment httpbin --replicas=2 
deployment.extensions/httpbin scaled

-A KUBE-SVC-FREKB6WNWYJLKTHC -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-PEA6WHECIZEOX47B
-A KUBE-SVC-FREKB6WNWYJLKTHC -j KUBE-SEP-UHAR347MOFCEOPWZ

-A KUBE-SEP-UHAR347MOFCEOPWZ -s 172.20.1.183/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-UHAR347MOFCEOPWZ -p tcp -m tcp -j DNAT --to-destination 172.20.1.183:80
</code></pre><p>httpbin pod副本数扩到3，观察iptables KUBE-SVC-，KUBE-SEP-链的变化</p><pre tabindex=0><code># 3副本, 每个都是1/3概率
-A KUBE-SVC-FREKB6WNWYJLKTHC -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-PEA6WHECIZEOX47B
# probability=(1-0.33332999982)/2=0.50000000000
-A KUBE-SVC-FREKB6WNWYJLKTHC -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-JXNDCT5ED2555YYJ
-A KUBE-SVC-FREKB6WNWYJLKTHC -j KUBE-SEP-UHAR347MOFCEOPWZ

-A KUBE-SEP-UHAR347MOFCEOPWZ -s 172.20.1.183/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-UHAR347MOFCEOPWZ -p tcp -m tcp -j DNAT --to-destination 172.20.1.183:80
</code></pre><h5 id=nodeport-service类型>nodePort service类型</h5><p>httpbin svc修改为nodePort类型</p><pre tabindex=0><code>[root@ice ~]# kubectl get svc 
NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGE
httpbin                    NodePort    172.20.255.90    &lt;none&gt;        80:11387/TCP   
</code></pre><p>非本地访问nodePort的iptables规则链顺序为:</p><pre tabindex=0><code>PREROUTING -&gt; KUBE-SERVICES -&gt; KUBE-NODEPORTS -&gt; KUBE-SVC-XXX -&gt; KUBE-SEP-XXX
</code></pre><pre tabindex=0><code>1. 进入PREROUTING的规则都转发到KUBE-SERVICES, 不符合KUBE-HOSTPORTS的条件
-A PREROUTING -m comment --comment &#34;kube hostport portals&#34; -m addrtype --dst-type LOCAL -j KUBE-HOSTPORTS
-A PREROUTING -m comment --comment &#34;kubernetes service portals&#34; -j KUBE-SERVICES

2. 优先处理clusterIP
-A KUBE-SERVICES -d 172.20.255.90/32 -p tcp -m comment --comment &#34;default/httpbin:http cluster IP&#34; -m tcp --dport 80 -j KUBE-SVC-FREKB6WNWYJLKTHC

3. 进入KUBE-SERVICES的规则, 优先处理clusterIP，最后都转发到KUBE-NODEPORTS
-A KUBE-SERVICES -m comment --comment &#34;kubernetes service nodeports; NOTE: this must be the last rule in this chain&#34; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS

4. 接下来就跟上述clusterIP类型的流程一样了
-A KUBE-NODEPORTS -p tcp -m comment --comment &#34;default/httpbin:http&#34; -m tcp --dport 11387 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment &#34;default/httpbin:http&#34; -m tcp --dport 11387 -j KUBE-SVC-FREKB6WNWYJLKTHC

5.
-A KUBE-SVC-FREKB6WNWYJLKTHC -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-PEA6WHECIZEOX47B
-A KUBE-SVC-FREKB6WNWYJLKTHC -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-JXNDCT5ED2555YYJ
-A KUBE-SVC-FREKB6WNWYJLKTHC -j KUBE-SEP-UHAR347MOFCEOPWZ

6. 
-A KUBE-SEP-PEA6WHECIZEOX47B -s 172.20.0.40/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-PEA6WHECIZEOX47B -p tcp -m tcp -j DNAT --to-destination 172.20.0.40:80
-A KUBE-SEP-JXNDCT5ED2555YYJ -s 172.20.0.41/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-JXNDCT5ED2555YYJ -p tcp -m tcp -j DNAT --to-destination 172.20.0.41:80
-A KUBE-SEP-UHAR347MOFCEOPWZ -s 172.20.1.183/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-UHAR347MOFCEOPWZ -p tcp -m tcp -j DNAT --to-destination 172.20.1.183:80
</code></pre><p>本地访问nodePort的iptables规则链顺序为:</p><pre tabindex=0><code>OUTPUT -&gt; KUBE-SERVICES -&gt; KUBE-NODEPORTS -&gt; KUBE-SVC-XXX -&gt; KUBE-SEP-XXX
</code></pre><pre tabindex=0><code>1. 进入PREROUTING的规则都转发到KUBE-SERVICES都转发至KUBE-SERVICES
-A OUTPUT -m comment --comment &#34;kube hostport portals&#34; -m addrtype --dst-type LOCAL -j KUBE-HOSTPORTS
-A OUTPUT -m comment --comment &#34;kubernetes service portals&#34; -j KUBE-SERVICES

2. 优先处理clusterIP
-A KUBE-SERVICES -d 172.20.255.90/32 -p tcp -m comment --comment &#34;default/httpbin:http cluster IP&#34; -m tcp --dport 80 -j KUBE-SVC-FREKB6WNWYJLKTHC

3. 进入KUBE-SERVICES的规则, 优先处理clusterIP，最后都转发到KUBE-NODEPORTS
-A KUBE-SERVICES -m comment --comment &#34;kubernetes service nodeports; NOTE: this must be the last rule in this chain&#34; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS

4. 接下来就跟上述clusterIP类型的流程一样了
-A KUBE-NODEPORTS -p tcp -m comment --comment &#34;default/httpbin:http&#34; -m tcp --dport 11387 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment &#34;default/httpbin:http&#34; -m tcp --dport 11387 -j KUBE-SVC-FREKB6WNWYJLKTHC

5.
-A KUBE-SVC-FREKB6WNWYJLKTHC -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-PEA6WHECIZEOX47B
-A KUBE-SVC-FREKB6WNWYJLKTHC -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-JXNDCT5ED2555YYJ
-A KUBE-SVC-FREKB6WNWYJLKTHC -j KUBE-SEP-UHAR347MOFCEOPWZ

6. 
-A KUBE-SEP-PEA6WHECIZEOX47B -s 172.20.0.40/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-PEA6WHECIZEOX47B -p tcp -m tcp -j DNAT --to-destination 172.20.0.40:80
-A KUBE-SEP-JXNDCT5ED2555YYJ -s 172.20.0.41/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-JXNDCT5ED2555YYJ -p tcp -m tcp -j DNAT --to-destination 172.20.0.41:80
-A KUBE-SEP-UHAR347MOFCEOPWZ -s 172.20.1.183/32 -j KUBE-MARK-MASQ
-A KUBE-SEP-UHAR347MOFCEOPWZ -p tcp -m tcp -j DNAT --to-destination 172.20.1.183:80
</code></pre><h4 id=ipvs模式>ipvs模式</h4><p>ipvs有三种负载均衡模式：</p><ul><li>NAT(k8s用这种模式)</li><li>DR</li><li>TUN</li></ul><p>linux内核中的ipvs只有DNAT，没有SNAT; ipvs会使用iptables进行包过滤、SNAT、MASQUERADE。
具体来说，ipvs使用ipset来存储需要NAT或masquared时的ip和端口列表。ipset是iptables的扩展，
允许你创建匹配整个地址sets（地址集合)的规则。而不像普通的iptables链是线性的存储和过滤，
ip集合存储在带索引的数据结构中,这种集合比较大也可以进行高效的查找。</p><p>SNAT vs MASQUERADE区别：MASQUERADE不需要指定SNAT的目标ip，自动获取</p><p>ipvs工作流程图</p><p><img src=/img/posts/2020-02-19/2.png alt></p><p>启用ipvs模式</p><pre tabindex=0><code># yum install -y ipset ipvsadm

# cat &lt;&lt; &#39;EOF&#39; &gt; ipvs.modules
#!/bin/bash
ipvs_modules=(ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4)
for kernel_module in ${ipvs_modules[*]}; do
/sbin/modinfo -F filename ${kernel_module} &gt; /dev/null 2&gt;&amp;1
if [ $? -eq 0 ]; then
   /sbin/modprobe ${kernel_module}
fi
done
EOF

# bash ipvs.modules
# cut -f1 -d &#34; &#34;  /proc/modules | grep -E &#34;ip_vs|nf_conntrack_ipv4&#34;

# kubectl -n kube-system edit cm kube-proxy
mode: &#34;ipvs&#34;

# kubectl -n kube-system get pod -l k8s-app=kube-proxy | grep -v &#39;NAME&#39; | awk &#39;{print $1}&#39; | xargs kubectl -n kube-system delete pod

# iptables -t filter -F; iptables -t filter -X; iptables -t nat -F; iptables -t nat -X;
</code></pre><p>kube-proxy ipvs模式使用了nat表和filter表，也定义自定义链</p><p>filter表：</p><ul><li>KUBE-FIREWALL</li><li>KUBE-FORWARD</li></ul><p>nat表：</p><ul><li>KUBE-FIREWALL</li><li>KUBE-HOSTPORTS</li><li>KUBE-LOAD-BALANCER</li><li>KUBE-MARK-DROP</li><li>KUBE-MARK-MASQ</li><li>KUBE-NODE-PORT</li><li>KUBE-POSTROUTING</li><li>KUBE-SERVICES</li></ul><p>ipvs模式下的clusterIP，是可以ping通的，它在宿主机上是真实存在的ip; 因为ipvs的DNAT用到的netfilter hook函数是
作用在INPUT链上，clusterIP会绑定在本机dummy网卡，默认名为kube-ipvs0，内核网络栈识别
到这是本机的IP，从而进入INPUT链；发送的包时候再从POSTROUTING出.</p><p>还是以上面httpbin的例子</p><pre tabindex=0><code># kubectl get svc httpbin 
NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
httpbin   ClusterIP   172.20.255.90   &lt;none&gt;        80/TCP    9d
</code></pre><p>httpbin clusterIP为172.20.255.90(3副本)</p><h5 id=clusterip-service类型-1>clusterIP service类型</h5><p>访问clusterIP iptables规则链顺序为: (结合上面ipvs工作流程图)</p><pre tabindex=0><code>PREROUTING -&gt; KUBE-SERVICES -&gt; KUBE-CLUSTER-IP(ipset) -&gt; INPUT -&gt; KUBE-FIREWALL -&gt; POSTROUTING
</code></pre><pre tabindex=0><code>1. 进入PREROUTING链
-A PREROUTING -m comment --comment &#34;kubernetes service portals&#34; -j KUBE-SERVICES

2. 跳转至KUBE-SERVICES
-A KUBE-SERVICES -m comment --comment &#34;Kubernetes service cluster ip + port for masquerade purpose&#34; -m set --match-set KUBE-CLUSTER-IP src,dst -j KUBE-MARK-MASQ

3. 进入KUBE-CLUSTER-IP ipset
[root@ice ~]# ipset list |grep -A 30 KUBE-CLUSTER-IP
Name: KUBE-CLUSTER-IP
Type: hash:ip,port
Revision: 2
Header: family inet hashsize 1024 maxelem 65536
Size in memory: 17392
References: 2
Members:
172.20.255.218,tcp:8001
172.20.253.100,tcp:8080
172.20.254.248,tcp:25672
172.20.255.183,tcp:443
172.20.254.248,tcp:15672
172.20.252.10,udp:53
172.20.253.103,tcp:8080
172.20.253.243,tcp:44134
172.20.254.248,tcp:5672
172.20.253.137,tcp:9100
172.20.252.136,tcp:9090
172.20.255.125,tcp:80
172.20.254.248,tcp:4369
172.20.255.100,tcp:80
172.20.252.130,tcp:56790
172.20.255.183,tcp:80
172.20.253.196,tcp:5432
172.20.252.130,tcp:80
172.20.255.57,tcp:443
172.20.253.20,tcp:80
172.20.252.130,tcp:443
172.20.252.10,tcp:53
172.20.254.170,tcp:8080
172.20.255.90,tcp:80     # httpbin clusterIP

4. 再跳转至KUBE-MARK-MASQ，打上标记
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000

5. 又进入KUBE-CLUSTER-IP ipset, 进行DNAT
-A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT

6. 查看ipvs规则，clusterIP真正存在于宿主机的kube-ipvs0网卡上
# ipvsadm  -L -n |grep -A 3 172.20.255.90
TCP  172.20.255.90:80 rr
  -&gt; 172.20.0.40:80               Masq    1      0          0         
  -&gt; 172.20.0.41:80               Masq    1      0          0         
  -&gt; 172.20.1.183:80              Masq    1      0          0 

# ip a show kube-ipvs0 |grep -A 2 172.20.255.90
    inet 172.20.255.90/32 brd 172.20.255.90 scope global kube-ipvs0
       valid_lft forever preferred_lft forever

7. 进入INPUT链
-A INPUT -j KUBE-FIREWALL

8. 跳转至KUBE-FIREWALL，有0x8000标记，即丢弃
-A KUBE-FIREWALL -m comment --comment &#34;kubernetes firewall for dropping marked packets&#34; -m mark --mark 0x8000/0x8000 -j DROP

9. 进入INPUT链后，ipvs的ip_vs_in hook函数根据ipvs规则直接转发至POSTROUTING链
-A POSTROUTING -m comment --comment &#34;kubernetes postrouting rules&#34; -j KUBE-POSTROUTING

10. 标记0x4000的包，正常MASQUERADE(类SNAT)
-A KUBE-POSTROUTING -m comment --comment &#34;kubernetes service traffic requiring SNAT&#34; -m mark --mark 0x4000/0x4000 -j MASQUERADE
</code></pre><h5 id=nodeport-service类型-1>nodePort service类型</h5><p>非本地访问nodePort的iptables规则链顺序为:</p><pre tabindex=0><code>PREROUTING -&gt; KUBE-SERVICES -&gt; KUBE-NODE-PORT -&gt; INPUT --&gt; KUBE-FIREWALL --&gt; POSTROUTING
</code></pre><p>本地访问nodePort的iptables规则链顺序为:</p><pre tabindex=0><code>OUTPUT -&gt; KUBE-SERVICES -&gt; KUBE-NODE-PORT -&gt; INPUT --&gt; KUBE-FIREWALL --&gt; POSTROUTING
</code></pre><p>分析过程与上述类似.</p><h3 id=kube-proxy代码>kube-proxy代码</h3><p>启动函数</p><pre tabindex=0><code>k8s.io/kubernetes/cmd/kube-proxy/proxy.go

func main() {
    rand.Seed(time.Now().UnixNano())

    command := app.NewProxyCommand()

    // TODO: once we switch everything over to Cobra commands, we can go back to calling
    // utilflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the
    // normalize func and add the go flag set by hand.
    pflag.CommandLine.SetNormalizeFunc(cliflag.WordSepNormalizeFunc)
    pflag.CommandLine.AddGoFlagSet(goflag.CommandLine)
    // utilflag.InitFlags()
    logs.InitLogs()
    defer logs.FlushLogs()

    if err := command.Execute(); err != nil {
        fmt.Fprintf(os.Stderr, &#34;error: %v\n&#34;, err)
        os.Exit(1)
    }
}
</code></pre><p>进入到Run函数</p><pre tabindex=0><code>k8s.io/kubernetes/cmd/kube-proxy/app/server.go

func (o *Options) Run() error {
    defer close(o.errCh)   
    // 配置写到文件
    if len(o.WriteConfigTo) &gt; 0 {
        return o.writeConfigFile()
    }
    // 初始化proxyServer对象
    proxyServer, err := NewProxyServer(o)
    if err != nil {
        return err
    }
    // 如果为true，执行
    if o.CleanupAndExit {
        return proxyServer.CleanupAndExit()
    }

    o.proxyServer = proxyServer
    return o.runLoop()
}
</code></pre><p>NewProxyServer函数 - 初始化proxyServer对象</p><pre tabindex=0><code>func newProxyServer(
    config *proxyconfigapi.KubeProxyConfiguration,
    cleanupAndExit bool,
    scheme *runtime.Scheme,
    master string) (*ProxyServer, error) {
    // config必须有值
    if config == nil {
        return nil, errors.New(&#34;config is required&#34;)
    }
    // 初始化configz.Config对象
    if c, err := configz.New(proxyconfigapi.GroupName); err == nil {
        c.Set(config)
    } else {
        return nil, fmt.Errorf(&#34;unable to register configz: %s&#34;, err)
    }
    // 支持ipv4,ipv6
    protocol := utiliptables.ProtocolIpv4
    if net.ParseIP(config.BindAddress).To4() == nil {
        klog.V(0).Infof(&#34;IPv6 bind address (%s), assume IPv6 operation&#34;, config.BindAddress)
        protocol = utiliptables.ProtocolIpv6
    }
    // 初始化变量
    var iptInterface utiliptables.Interface
    var ipvsInterface utilipvs.Interface
    var kernelHandler ipvs.KernelHandler
    var ipsetInterface utilipset.Interface
    var dbus utildbus.Interface

    // 封装了os/exec
    // Create a iptables utils.
    execer := exec.New()

    // 封装了github.com/godbus/dbus，一个与D-Bus交互的golang库
    dbus = utildbus.New()
    // 初始化操作iptables的对象，针对iptables版本做了check
    iptInterface = utiliptables.New(execer, dbus, protocol)
    // 初始化操作ipvs模块的对象
    kernelHandler = ipvs.NewLinuxKernelHandler()
    // 初始化操作ipset的对象
    ipsetInterface = utilipset.New(execer)
    // 检查ipvs，ipset版本；会尝试在kube-proxy容器里自动加载ipvs所需模块
    canUseIPVS, _ := ipvs.CanUseIPVSProxier(kernelHandler, ipsetInterface)
    if canUseIPVS {
        // 初始化操作ipvs的对象
        ipvsInterface = utilipvs.New(execer)
    }

    // We omit creation of pretty much everything if we run in cleanup mode
    if cleanupAndExit {
        return &amp;ProxyServer{
            execer:         execer,
            IptInterface:   iptInterface,
            IpvsInterface:  ipvsInterface,
            IpsetInterface: ipsetInterface,
        }, nil
    }
    // 初始化clientset
    client, eventClient, err := createClients(config.ClientConnection, master)
    if err != nil {
        return nil, err
    }

    // Create event recorder
    hostname, err := utilnode.GetHostname(config.HostnameOverride)
    if err != nil {
        return nil, err
    }
    eventBroadcaster := record.NewBroadcaster()
    recorder := eventBroadcaster.NewRecorder(scheme, v1.EventSource{Component: &#34;kube-proxy&#34;, Host: hostname})

    nodeRef := &amp;v1.ObjectReference{
        Kind:      &#34;Node&#34;,
        Name:      hostname,
        UID:       types.UID(hostname),
        Namespace: &#34;&#34;,
    }

    var healthzServer *healthcheck.HealthzServer
    var healthzUpdater healthcheck.HealthzUpdater
    if len(config.HealthzBindAddress) &gt; 0 {
        healthzServer = healthcheck.NewDefaultHealthzServer(config.HealthzBindAddress, 2*config.IPTables.SyncPeriod.Duration, recorder, nodeRef)
        healthzUpdater = healthzServer
    }

    var proxier proxy.ProxyProvider
    var serviceEventHandler proxyconfig.ServiceHandler
    var endpointsEventHandler proxyconfig.EndpointsHandler

    // 获取kube-proxy模式，不指定的走iptables模式；检测该模式前置条件失败的话会尝试下一个模式
    // 顺序依次为ipvs -&gt; iptables -&gt; usernamespace
    proxyMode := getProxyMode(string(config.Mode), iptInterface, kernelHandler, ipsetInterface, iptables.LinuxKernelCompatTester{})
    nodeIP := net.ParseIP(config.BindAddress)
    if nodeIP.IsUnspecified() {
        nodeIP = utilnode.GetNodeIP(client, hostname)
    }
    if proxyMode == proxyModeIPTables {
        klog.V(0).Info(&#34;Using iptables Proxier.&#34;)
        if config.IPTables.MasqueradeBit == nil {
            // MasqueradeBit must be specified or defaulted.
            return nil, fmt.Errorf(&#34;unable to read IPTables MasqueradeBit from config&#34;)
        }
        // 初始化iptables proxier对象
        // TODO this has side effects that should only happen when Run() is invoked.
        proxierIPTables, err := iptables.NewProxier(
            iptInterface,
            utilsysctl.New(),
            execer,
            config.IPTables.SyncPeriod.Duration,
            config.IPTables.MinSyncPeriod.Duration,
            config.IPTables.MasqueradeAll,
            int(*config.IPTables.MasqueradeBit),
            config.ClusterCIDR,
            hostname,
            nodeIP,
            recorder,
            healthzUpdater,
            config.NodePortAddresses,
        )
        if err != nil {
            return nil, fmt.Errorf(&#34;unable to create proxier: %v&#34;, err)
        }
        metrics.RegisterMetrics()
        proxier = proxierIPTables
        serviceEventHandler = proxierIPTables
        endpointsEventHandler = proxierIPTables
    } else if proxyMode == proxyModeIPVS {
        klog.V(0).Info(&#34;Using ipvs Proxier.&#34;)
        // 初始化ipvs proxier对象
        proxierIPVS, err := ipvs.NewProxier(
            iptInterface,
            ipvsInterface,
            ipsetInterface,
            utilsysctl.New(),
            execer,
            config.IPVS.SyncPeriod.Duration,
            config.IPVS.MinSyncPeriod.Duration,
            config.IPVS.ExcludeCIDRs,
            config.IPVS.StrictARP,
            config.IPTables.MasqueradeAll,
            int(*config.IPTables.MasqueradeBit),
            config.ClusterCIDR,
            hostname,
            nodeIP,
            recorder,
            healthzServer,
            config.IPVS.Scheduler,
            config.NodePortAddresses,
        )
        if err != nil {
            return nil, fmt.Errorf(&#34;unable to create proxier: %v&#34;, err)
        }
        metrics.RegisterMetrics()
        proxier = proxierIPVS
        serviceEventHandler = proxierIPVS
        endpointsEventHandler = proxierIPVS
    } else {
        klog.V(0).Info(&#34;Using userspace Proxier.&#34;)
        // This is a proxy.LoadBalancer which NewProxier needs but has methods we don&#39;t need for
        // our config.EndpointsConfigHandler.
        loadBalancer := userspace.NewLoadBalancerRR()
        // set EndpointsConfigHandler to our loadBalancer
        endpointsEventHandler = loadBalancer

        // TODO this has side effects that should only happen when Run() is invoked.
        proxierUserspace, err := userspace.NewProxier(
            loadBalancer,
            net.ParseIP(config.BindAddress),
            iptInterface,
            execer,
            *utilnet.ParsePortRangeOrDie(config.PortRange),
            config.IPTables.SyncPeriod.Duration,
            config.IPTables.MinSyncPeriod.Duration,
            config.UDPIdleTimeout.Duration,
            config.NodePortAddresses,
        )
        if err != nil {
            return nil, fmt.Errorf(&#34;unable to create proxier: %v&#34;, err)
        }
        serviceEventHandler = proxierUserspace
        proxier = proxierUserspace
    }

    iptInterface.AddReloadFunc(proxier.Sync)

    return &amp;ProxyServer{
        Client:                 client,
        EventClient:            eventClient,
        IptInterface:           iptInterface,
        IpvsInterface:          ipvsInterface,
        IpsetInterface:         ipsetInterface,
        execer:                 execer,
        Proxier:                proxier,
        Broadcaster:            eventBroadcaster,
        Recorder:               recorder,
        ConntrackConfiguration: config.Conntrack,
        Conntracker:            &amp;realConntracker{},
        ProxyMode:              proxyMode,
        NodeRef:                nodeRef,
        MetricsBindAddress:     config.MetricsBindAddress,
        EnableProfiling:        config.EnableProfiling,
        OOMScoreAdj:            config.OOMScoreAdj,
        ResourceContainer:      config.ResourceContainer,
        ConfigSyncPeriod:       config.ConfigSyncPeriod.Duration,
        ServiceEventHandler:    serviceEventHandler,
        EndpointsEventHandler:  endpointsEventHandler,
        HealthzServer:          healthzServer,
    }, nil
}
</code></pre><p>接下来的调用流程：</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
A[o.runLoop] --&gt;B(o.proxyServer.Run) 
        B --&gt; C(s.Proxier.SyncLoop) 
      C --&gt; D(proxier.syncRunner.Loop)
      D --&gt; E(bfr.tryRun/bfr.fn)
      E --&gt; F{模式mode}
    F --&gt;|mode=iptables| G[proxier.syncProxyRules]
    F --&gt;|mode=ipvs| H[proxier.syncProxyRules]
    F --&gt;|mode=usernamespace| I[proxier.syncProxyRules]
    Z[调用流程]
</code></pre><p>最终会根据不同模式，调用到proxier.syncProxyRules；创建相应的iptables规则、ipvs规则，都是具体的命令；这个函数也比较长，可以一条一条规则对着分析.</p><h3 id=参考链接>参考链接</h3><ul><li><a href=https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md>iptables vs ipvs</a></li><li><a href=http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.filter_rules.html>http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.filter_rules.html</a></li><li><a href=https://blog.tianfeiyu.com/source-code-reading-notes/kubernetes/kube_proxy_ipvs.html>https://blog.tianfeiyu.com/source-code-reading-notes/kubernetes/kube_proxy_ipvs.html</a></li><li><a href=https://zhuanlan.zhihu.com/p/37230013>https://zhuanlan.zhihu.com/p/37230013</a></li></ul><div class="entry-shang text-center"><p>「真诚赞赏，手留余香」</p><button class="zs show-zs btn btn-bred">赞赏支持</button></div><div class=zs-modal-bg></div><div class=zs-modal-box><div class=zs-modal-head><button type=button class=close>×</button>
<span class=author><a href=https://www.iceyao.com.cn/><img src=/img/favicon.png>爱折腾的工程师</a></span><p class=tip><i></i><span>真诚赞赏，手留余香</span></p></div><div class=zs-modal-body><div class=zs-modal-btns><button class="btn btn-blink" data-num=2>2元</button>
<button class="btn btn-blink" data-num=5>5元</button>
<button class="btn btn-blink" data-num=10>10元</button>
<button class="btn btn-blink" data-num=50>50元</button>
<button class="btn btn-blink" data-num=100>100元</button>
<button class="btn btn-blink" data-num=1>任意金额</button></div><div class=zs-modal-pay><button class="btn btn-bred" id=pay-text>2元</button><p>使用<span id=pay-type>微信</span>扫描二维码完成支付</p><img src=/img/reward/wechat-2.png id=pay-image></div></div><div class=zs-modal-footer><label><input type=radio name=zs-type value=wechat class=zs-type checked><span><span class=zs-wechat><img src=/img/reward/wechat-btn.png></span></label>
<label><input type=radio name=zs-type value=alipay class=zs-type class=zs-alipay><img src=/img/reward/alipay-btn.png></span></label></div></div><script type=text/javascript src=/js/reward.js></script><hr><ul class=pager><li class=previous><a href=/post/2020-01-17-pod_preset%E8%87%AA%E5%8A%A8%E5%8C%96%E9%85%8D%E7%BD%AEk8s%E5%AE%B9%E5%99%A8%E6%97%B6%E5%8C%BA/ data-toggle=tooltip data-placement=top title="Pod preset自动化配置k8s容器时区">&larr;
Previous Post</a></li><li class=next><a href=/post/2020-04-10-kubelet_readnote/ data-toggle=tooltip data-placement=top title=kubelet源码阅读笔记>Next
Post &rarr;</a></li></ul><script src=https://giscus.app/client.js data-repo=yaoice/yaoice.github.io data-repo-id=R_kgDOJnxqVg data-category=General data-category-id=DIC_kwDOJnxqVs4CWwUs data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=en crossorigin=anonymous async></script></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/devops title=devops>devops
</a><a href=/tags/go title=go>go
</a><a href=/tags/k8s title=k8s>k8s
</a><a href=/tags/llm title=llm>llm
</a><a href=/tags/openstack title=openstack>openstack
</a><a href=/tags/tkestack title=tkestack>tkestack
</a><a href=/tags/%E7%BB%83%E8%BD%A6 title=练车>练车</a></div></section></div></div></div></article><script type=module>  
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; 
    mermaid.initialize({ startOnLoad: true });  
</script><script>Array.from(document.getElementsByClassName("language-mermaid")).forEach(e=>{e.parentElement.outerHTML=`<div class="mermaid">${e.innerHTML}</div>`})</script><style>.mermaid svg{display:block;margin:auto}</style><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:yao3690093@gmail.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat.jpeg><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/yaoice><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title=爱折腾的工程师><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; 爱折腾的工程师 2024</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script>(function(){var t,e=document.createElement("script"),n=window.location.protocol.split(":")[0];n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script><script>var _baId="92c175994ded75a3cd2074bc1123e2be",_hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="//hm.baidu.com/hm.js?"+_baId,e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){t=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),n=$(this).text(),i=$('<a href="'+o+'" rel="nofollow">'+n+"</a>"),s=$('<li class="'+t+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script></body></html>